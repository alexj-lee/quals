
@article{sunkin_allen_2013,
	title = {Allen Brain Atlas: an integrated spatio-temporal portal for exploring the central nervous system},
	volume = {41},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531093/},
	doi = {10.1093/nar/gks1042},
	shorttitle = {Allen Brain Atlas},
	abstract = {The Allen Brain Atlas (http://www.brain-map.org) provides a unique online public resource integrating extensive gene expression data, connectivity data and neuroanatomical information with powerful search and viewing tools for the adult and developing brain in mouse, human and non-human primate. Here, we review the resources available at the Allen Brain Atlas, describing each product and data type [such as in situ hybridization ({ISH}) and supporting histology, microarray, {RNA} sequencing, reference atlases, projection mapping and magnetic resonance imaging]. In addition, standardized and unique features in the web applications are described that enable users to search and mine the various data sets. Features include both simple and sophisticated methods for gene searches, colorimetric and fluorescent {ISH} image viewers, graphical displays of {ISH}, microarray and {RNA} sequencing data, Brain Explorer software for 3D navigation of anatomy and gene expression, and an interactive reference atlas viewer. In addition, cross data set searches enable users to query multiple Allen Brain Atlas data sets simultaneously. All of the Allen Brain Atlas resources can be accessed through the Allen Brain Atlas data portal.},
	pages = {D996--D1008},
	issue = {Database issue},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Res},
	author = {Sunkin, Susan M. and Ng, Lydia and Lau, Chris and Dolbeare, Tim and Gilbert, Terri L. and Thompson, Carol L. and Hawrylycz, Michael and Dang, Chinh},
	urldate = {2022-09-21},
	date = {2013-01},
	pmid = {23193282},
	pmcid = {PMC3531093},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/Y4VTCALF/Sunkin et al. - 2013 - Allen Brain Atlas an integrated spatio-temporal p.pdf:application/pdf},
}

@article{kuan_neuroinformatics_2015,
	title = {Neuroinformatics of the Allen Mouse Brain Connectivity Atlas},
	volume = {73},
	issn = {1046-2023},
	url = {https://www.sciencedirect.com/science/article/pii/S1046202314004277},
	doi = {10.1016/j.ymeth.2014.12.013},
	series = {Spatial mapping of multi-modal data in neuroscience},
	abstract = {The Allen Mouse Brain Connectivity Atlas is a mesoscale whole brain axonal projection atlas of the C57Bl/6J mouse brain. Anatomical trajectories throughout the brain were mapped into a common 3D space using a standardized platform to generate a comprehensive and quantitative database of inter-areal and cell-type-specific projections. This connectivity atlas has several desirable features, including brain-wide coverage, validated and versatile experimental techniques, a single standardized data format, a quantifiable and integrated neuroinformatics resource, and an open-access public online database (http://connectivity.brain-map.org/). Meaningful informatics data quantification and comparison is key to effective use and interpretation of connectome data. This relies on successful definition of a high fidelity atlas template and framework, mapping precision of raw data sets into the 3D reference framework, accurate signal detection and quantitative connection strength algorithms, and effective presentation in an integrated online application. Here we describe key informatics pipeline steps in the creation of the Allen Mouse Brain Connectivity Atlas and include basic application use cases.},
	pages = {4--17},
	journaltitle = {Methods},
	shortjournal = {Methods},
	author = {Kuan, Leonard and Li, Yang and Lau, Chris and Feng, David and Bernard, Amy and Sunkin, Susan M. and Zeng, Hongkui and Dang, Chinh and Hawrylycz, Michael and Ng, Lydia},
	urldate = {2022-09-21},
	date = {2015-02-01},
	langid = {english},
	keywords = {Digital atlas, Image registration, Mouse connectivity atlas, Neuronal projection, Signal detection},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/RGRK5G7K/Kuan et al. - 2015 - Neuroinformatics of the Allen Mouse Brain Connecti.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/987P3GR7/S1046202314004277.html:text/html},
}

@online{noauthor_learning_nodate,
	title = {Learning the parts of objects by non-negative matrix factorization {\textbar} Nature},
	url = {https://www.nature.com/articles/44565},
	urldate = {2022-09-21},
	file = {Learning the parts of objects by non-negative matrix factorization | Nature:/home/ajl/Zotero/storage/WFU6PJG6/44565.html:text/html},
}

@online{noauthor_greed_nodate,
	title = {Greed is good: algorithmic results for sparse approximation {\textbar} {IEEE} Journals \& Magazine {\textbar} {IEEE} Xplore},
	url = {https://ieeexplore.ieee.org/document/1337101},
	urldate = {2022-09-21},
	file = {Greed is good\: algorithmic results for sparse approximation | IEEE Journals & Magazine | IEEE Xplore:/home/ajl/Zotero/storage/XSIUKMPZ/1337101.html:text/html},
}

@article{wu_stability-driven_2016,
	title = {Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks},
	volume = {113},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1521171113},
	doi = {10.1073/pnas.1521171113},
	pages = {4290--4295},
	number = {16},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Wu, Siqi and Joseph, Antony and Hammonds, Ann S. and Celniker, Susan E. and Yu, Bin and Frise, Erwin},
	urldate = {2022-09-21},
	date = {2016-04-19},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:/home/ajl/Zotero/storage/XKJYGKIP/Wu et al. - 2016 - Stability-driven nonnegative matrix factorization .pdf:application/pdf},
}

@article{pasquini_dysfunctional_2022,
	title = {Dysfunctional cortical gradient topography in treatment resistant major depression},
	volume = {0},
	issn = {2451-9022},
	url = {https://www.biologicalpsychiatrycnni.org/article/S2451-9022(22)00270-1/fulltext},
	doi = {10.1016/j.bpsc.2022.10.009},
	number = {0},
	journaltitle = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	shortjournal = {{BPS}: {CNNI}},
	author = {Pasquini, Lorenzo and Fryer, Susanna L. and Eisendrath, Stuart J. and Segal, Zindel V. and Lee, Alex J. and Brown, Jesse A. and Saggar, Manish and Mathalon, Daniel H.},
	urldate = {2022-11-03},
	date = {2022-10-31},
	note = {Publisher: Elsevier},
	keywords = {Connectivity gradients, default mode network, functional connectivity, graph theory, treatment resistant major depression},
	file = {Full Text PDF:/home/ajl/Zotero/storage/4RDJ34TR/Pasquini et al. - 2022 - Dysfunctional cortical gradient topography in trea.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/MUUSUJBE/fulltext.html:text/html},
}

@article{mantyh_tau_2020,
	title = {Tau Positron Emission Tomographic Findings in a Former {US} Football Player With Pathologically Confirmed Chronic Traumatic Encephalopathy},
	volume = {77},
	issn = {2168-6149},
	url = {https://doi.org/10.1001/jamaneurol.2019.4509},
	doi = {10.1001/jamaneurol.2019.4509},
	abstract = {Biomarkers for chronic traumatic encephalopathy ({CTE}) are currently lacking. The radiotracer fluorine F 18–labeled (18F)–flortaucipir ({FTP}) detects tau pathology in Alzheimer disease, and positron emission tomography ({PET}) with {FTP} shows elevated binding in individuals at risk for {CTE}. No study, however, has assessed the correlation between in vivo {FTP} {PET} and postmortem tau in {CTE}.To assess the regional association between in vivo {FTP} binding and postmortem tau pathology in a patient with pathologically confirmed {CTE}.A white male former National Football League player with 17 years of {US} football exposure was clinically diagnosed with traumatic encephalopathy syndrome at a neurology tertiary referral center. 18F-Fludeoxyglucose, carbon 11–labeled Pittsburgh compound B, and {FTP} {PET} were performed 52 months prior to death, and magnetic resonance imaging, 50 months prior to death. Brain images were assessed qualitatively for abnormalities blinded to autopsy data. Autopsy was performed using a neurodegenerative research protocol. The {FTP} standardized uptake value ratios (inferior cerebellar gray reference region) and W-score (age-adjusted z-score) maps were compared with phosphorylated tau immunohistochemical analysis with monoclonal antibody {CP}13.Qualitative and quantitative comparisons between antemortem {FTP} {PET} and tau pathology at autopsy.Flortaucipir uptake was distributed in a patchy, frontotemporal-predominant pattern that overlapped with regions showing neurodegeneration on magnetic resonance imaging and hypometabolism on 18F-fludeoxyglucose {PET}. Pathological assessment revealed stage 4 {CTE}; limbic argyrophilic grain disease; stage 2 limbic-predominant, age-related transactive response {DNA}-binding protein 43 encephalopathy; and Braak neurofibrillary tangle stage 3. 18F-Flortaucipir W-maps matched areas of high postmortem tau burden in left fusiform and inferior temporal gyri and juxtacortical frontal white matter. High {FTP} W-scores with low tau burden were found in the basal ganglia, thalamus, motor cortex, and calcarine cortex. No regions with low {FTP} W-scores corresponded to areas with high pathological tau burden. A modest correlation, which did not reach statistical significance (ρ = 0.35, P = .17), was found between {FTP} standardized uptake value ratio and tau area fraction at the regional level.In this patient, {FTP} {PET} findings during life showed a modest correspondence with postmortem pathology in {CTE}. These findings suggest that {FTP} may have limited utility as a tau biomarker in {CTE}.},
	pages = {517--521},
	number = {4},
	journaltitle = {{JAMA} Neurology},
	shortjournal = {{JAMA} Neurology},
	author = {Mantyh, William G. and Spina, Salvatore and Lee, Alex and Iaccarino, Leonardo and Soleimani-Meigooni, David and Tsoy, Elena and Mellinger, Taylor J. and Grant, Harli and Vandevrede, Lawren and La Joie, Renaud and Lesman-Segev, Orit and Gaus, Stephanie and Possin, Katherine L. and Grinberg, Lea T. and Miller, Bruce L. and Seeley, William W. and Rabinovici, Gil D.},
	urldate = {2022-11-03},
	date = {2020-04-01},
	file = {Snapshot:/home/ajl/Zotero/storage/AN6CX5YE/2757865.html:text/html},
}

@article{brown_dynamic_2022,
	title = {A dynamic gradient architecture generates brain activity states},
	volume = {261},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922006413},
	doi = {10.1016/j.neuroimage.2022.119526},
	abstract = {The human brain exhibits a diverse yet constrained range of activity states. While these states can be faithfully represented in a low-dimensional latent space, our understanding of the constitutive functional anatomy is still evolving. Here we applied dimensionality reduction to task-free and task {fMRI} data to address whether latent dimensions reflect intrinsic systems and if so, how these systems may interact to generate different activity states. We find that each dimension represents a dynamic activity gradient, including a primary unipolar sensory-association gradient underlying the global signal. The gradients appear stable across individuals and cognitive states, while recapitulating key functional connectivity properties including anticorrelation, modularity, and regional hubness. We then use dynamical systems modeling to show that gradients causally interact via state-specific coupling parameters to create distinct brain activity patterns. Together, these findings indicate that a set of dynamic, intrinsic spatial gradients interact to determine the repertoire of possible brain activity states.},
	pages = {119526},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Brown, Jesse A. and Lee, Alex J. and Pasquini, Lorenzo and Seeley, William W.},
	urldate = {2022-11-03},
	date = {2022-11-01},
	langid = {english},
	keywords = {Dimensionality reduction, Dynamical systems, Functional connectivity, Global signal, Gradients},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/FLX8QRUT/Brown et al. - 2022 - A dynamic gradient architecture generates brain ac.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/MDW5UAJI/S1053811922006413.html:text/html},
}

@article{pasquini_state_2020,
	title = {State and trait characteristics of anterior insula time-varying functional connectivity},
	volume = {208},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S105381191931016X},
	doi = {10.1016/j.neuroimage.2019.116425},
	abstract = {The human anterior insula ({aINS}) is a topographically organized brain region, in which ventral portions contribute to socio-emotional function through limbic and autonomic connections, whereas the dorsal {aINS} contributes to cognitive processes through frontal and parietal connections. Open questions remain, however, regarding how {aINS} connectivity varies over time. We implemented a novel approach combining seed-to-whole-brain sliding-window functional connectivity {MRI} and k-means clustering to assess time-varying functional connectivity of {aINS} subregions. We studied three independent large samples of healthy participants and longitudinal datasets to assess inter- and intra-subject stability, and related {aINS} time-varying functional connectivity profiles to dispositional empathy. We identified four robust {aINS} time-varying functional connectivity modes that displayed both “state” and “trait” characteristics: while modes featuring connectivity to sensory regions were modulated by eye closure, modes featuring connectivity to higher cognitive and emotional processing regions were stable over time and related to empathy measures.},
	pages = {116425},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Pasquini, Lorenzo and Toller, Gianina and Staffaroni, Adam and Brown, Jesse A. and Deng, Jersey and Lee, Alex and Kurcyus, Katarzyna and Shdo, Suzanne M. and Allen, Isabel and Sturm, Virginia E. and Cobigo, Yann and Borghesani, Valentina and Battistella, Giovanni and Gorno-Tempini, Maria Luisa and Rankin, Katherine P. and Kramer, Joel and Rosen, Howard H. and Miller, Bruce L. and Seeley, William W.},
	urldate = {2022-11-03},
	date = {2020-03-01},
	langid = {english},
	keywords = {Anterior insula, Socio-emotional functioning, State, Time-varying functional connectivity, Trait},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/Q2V96IG8/Pasquini et al. - 2020 - State and trait characteristics of anterior insula.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/8Q3TB5Q7/S105381191931016X.html:text/html},
}

@article{pasquini_dynamic_nodate,
	title = {Dynamic autonomic nervous system states arise during emotions and manifest in basal physiology},
	volume = {n/a},
	issn = {1469-8986},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.14218},
	doi = {10.1111/psyp.14218},
	abstract = {The outflow of the autonomic nervous system ({ANS}) is continuous and dynamic, but its functional organization is not well understood. Whether {ANS} patterns accompany emotions, or arise in basal physiology, remain unsettled questions in the field. Here, we searched for brief {ANS} patterns amidst continuous, multichannel physiological recordings in 45 healthy older adults. Participants completed an emotional reactivity task in which they viewed video clips that elicited a target emotion (awe, sadness, amusement, disgust, or nurturant love); each video clip was preceded by a pre-trial baseline period and followed by a post-trial recovery period. Participants also sat quietly for a separate 2-min resting period to assess basal physiology. Using principal components analysis and unsupervised clustering algorithms to reduce the second-by-second physiological data during the emotional reactivity task, we uncovered five {ANS} states. Each {ANS} state was characterized by a unique constellation of patterned physiological changes that differentiated among the trials of the emotional reactivity task. These {ANS} states emerged and dissipated over time, with each instance lasting several seconds on average. {ANS} states with similar structures were also detectable in the resting period but were intermittent and of smaller magnitude. Our results offer new insights into the functional organization of the {ANS}. By assembling short-lived, patterned changes, the {ANS} is equipped to generate a wide range of physiological states that accompany emotions and that contribute to the architecture of basal physiology.},
	pages = {e14218},
	issue = {n/a},
	journaltitle = {Psychophysiology},
	author = {Pasquini, Lorenzo and Noohi, Fatemeh and Veziris, Christina R. and Kosik, Eena L. and Holley, Sarah R. and Lee, Alex and Brown, Jesse A. and Roy, Ashlin R. K. and Chow, Tiffany E. and Allen, Isabel and Rosen, Howard J. and Kramer, Joel H. and Miller, Bruce L. and Saggar, Manish and Seeley, William W. and Sturm, Virginia E.},
	urldate = {2022-12-01},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.14218},
	keywords = {autonomic nervous system, baseline, dynamic systems, emotions, physiology, resting state},
	file = {Full Text PDF:/home/ajl/Zotero/storage/LPZCF4M9/Pasquini et al. - Dynamic autonomic nervous system states arise duri.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/NYBNT2A5/psyp.html:text/html},
}

@misc{kolesnikov_accelerated_2018,
	title = {Accelerated deep learning R\&D},
	url = {https://github.com/catalyst-team/catalyst},
	publisher = {{GitHub}},
	author = {Kolesnikov, Sergey},
	date = {2018},
	note = {Publication Title: {GitHub} repository},
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	url = {https://openreview.net/forum?id=BJJsrmfCZ},
	abstract = {A summary of automatic differentiation techniques employed in {PyTorch} library, including novelties like support for in-place modification in presence of objects aliasing the same data, performance...},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and {DeVito}, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	urldate = {2021-08-18},
	date = {2017-10-28},
	langid = {english},
	file = {Full Text PDF:/home/ajl/Zotero/storage/KKHQLCSJ/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/4SKZ3VFJ/forum.html:text/html},
}

@article{yuan_ocnet_2021,
	title = {{OCNet}: Object Context Network for Scene Parsing},
	url = {http://arxiv.org/abs/1809.00916},
	shorttitle = {{OCNet}},
	abstract = {In this paper, we address the semantic segmentation task with a new context aggregation scheme named {\textbackslash}emph\{object context\}, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise. We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices. To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling{\textasciitilde}{\textbackslash}citep\{zhao2017pyramid\} and atrous spatial pyramid pooling{\textasciitilde}{\textbackslash}citep\{chen2018deeplab\}. We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, {ADE}20K, {LIP}, {PASCAL}-Context and {COCO}-Stuff},
	journaltitle = {{arXiv}:1809.00916 [cs]},
	author = {Yuan, Yuhui and Huang, Lang and Guo, Jianyuan and Zhang, Chao and Chen, Xilin and Wang, Jingdong},
	urldate = {2021-08-18},
	date = {2021-03-14},
	eprinttype = {arxiv},
	eprint = {1809.00916},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AG5AQUP6/Yuan et al. - 2021 - OCNet Object Context Network for Scene Parsing.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZF247VJ3/1809.html:text/html},
}

@article{yuan_segmentation_2021,
	title = {Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation},
	url = {http://arxiv.org/abs/1909.11065},
	shorttitle = {Segmentation Transformer},
	abstract = {In this paper, we address the semantic segmentation problem with a focus on the context aggregation strategy. Our motivation is that the label of a pixel is the category of the object that the pixel belongs to. We present a simple yet effective approach, object-contextual representations, characterizing a pixel by exploiting the representation of the corresponding object class. First, we learn object regions under the supervision of ground-truth segmentation. Second, we compute the object region representation by aggregating the representations of the pixels lying in the object region. Last, \% the representation similarity we compute the relation between each pixel and each object region and augment the representation of each pixel with the object-contextual representation which is a weighted aggregation of all the object region representations according to their relations with the pixel. We empirically demonstrate that the proposed approach achieves competitive performance on various challenging semantic segmentation benchmarks: Cityscapes, {ADE}20K, {LIP}, {PASCAL}-Context, and {COCO}-Stuff. Cityscapes, {ADE}20K, {LIP}, {PASCAL}-Context, and {COCO}-Stuff. Our submission "{HRNet} + {OCR} + {SegFix}" achieves 1-st place on the Cityscapes leaderboard by the time of submission. Code is available at: https://git.io/openseg and https://git.io/{HRNet}.{OCR}. We rephrase the object-contextual representation scheme using the Transformer encoder-decoder framework. The details are presented in{\textasciitilde}Section3.3.},
	journaltitle = {{arXiv}:1909.11065 [cs]},
	author = {Yuan, Yuhui and Chen, Xiaokang and Chen, Xilin and Wang, Jingdong},
	urldate = {2021-08-18},
	date = {2021-04-30},
	eprinttype = {arxiv},
	eprint = {1909.11065},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/HK3BXM5E/Yuan et al. - 2021 - Segmentation Transformer Object-Contextual Repres.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/IRVQK2RZ/1909.html:text/html},
}

@online{noauthor_170901507_nodate,
	title = {[1709.01507] Squeeze-and-Excitation Networks},
	url = {https://arxiv.org/abs/1709.01507},
	urldate = {2021-08-18},
	file = {[1709.01507] Squeeze-and-Excitation Networks:/home/ajl/Zotero/storage/2MAF8ULI/1709.html:text/html},
}

@online{noauthor_14126980_nodate,
	title = {[1412.6980] Adam: A Method for Stochastic Optimization},
	url = {https://arxiv.org/abs/1412.6980},
	urldate = {2021-08-18},
	file = {[1412.6980] Adam\: A Method for Stochastic Optimization:/home/ajl/Zotero/storage/TAHFNBM4/1412.html:text/html},
}

@online{biewald_experiment_2020,
	title = {Experiment Tracking with Weights and Biases},
	url = {wandb.com},
	titleaddon = {Experiment Tracking with Weights and Biases},
	type = {wandb.com},
	author = {Biewald, Lucas},
	urldate = {2021-08-08},
	date = {2020},
}

@article{caicedo_evaluation_2019,
	title = {Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images},
	volume = {95},
	issn = {1552-4930},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cyto.a.23863},
	doi = {10.1002/cyto.a.23863},
	abstract = {Identifying nuclei is often a critical first step in analyzing microscopy images of cells and classical image processing algorithms are most commonly used for this task. Recent developments in deep learning can yield superior accuracy, but typical evaluation metrics for nucleus segmentation do not satisfactorily capture error modes that are relevant in cellular images. We present an evaluation framework to measure accuracy, types of errors, and computational efficiency; and use it to compare deep learning strategies and classical approaches. We publicly release a set of 23,165 manually annotated nuclei and source code to reproduce experiments and run the proposed evaluation methodology. Our evaluation framework shows that deep learning improves accuracy and can reduce the number of biologically relevant errors by half. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.},
	pages = {952--965},
	number = {9},
	journaltitle = {Cytometry Part A},
	author = {Caicedo, Juan C. and Roth, Jonathan and Goodman, Allen and Becker, Tim and Karhohs, Kyle W. and Broisin, Matthieu and Molnar, Csaba and {McQuin}, Claire and Singh, Shantanu and Theis, Fabian J. and Carpenter, Anne E.},
	urldate = {2021-08-18},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cyto.a.23863},
	keywords = {chemical screen, deep learning, fluorescence imaging, image analysis, nuclear segmentation},
	file = {Full Text PDF:/home/ajl/Zotero/storage/NIA3GTUP/Caicedo et al. - 2019 - Evaluation of Deep Learning Strategies for Nucleus.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/22QH2R3X/cyto.a.html:text/html},
}

@online{noauthor_150504597_nodate,
	title = {[1505.04597] U-Net: Convolutional Networks for Biomedical Image Segmentation},
	url = {https://arxiv.org/abs/1505.04597},
	urldate = {2021-08-16},
}

@article{spina_neuropathological_2019,
	title = {Neuropathological correlates of structural and functional imaging biomarkers in 4-repeat tauopathies},
	volume = {142},
	issn = {0006-8950},
	url = {https://doi.org/10.1093/brain/awz122},
	doi = {10.1093/brain/awz122},
	abstract = {Neurodegenerative dementia syndromes are characterized by spreading of pathological protein deposition along syndrome-specific neural networks. Structural and functional {MRI} measures can assess the integrity of these networks and have been proposed as biomarkers of disease progression for clinical trials. The relationship between in vivo imaging measures and pathological features, at the single subject level, remains largely unknown. Patient-specific maps of atrophy and seed-based intrinsic connectivity disruption, as compared to normal controls, were obtained for 27 patients subsequently diagnosed with progressive supranuclear palsy (n = 16, seven males, age at death 68.9 ± 6.0 years, imaging-to-pathology interval = 670.2 ± 425.1 days) or corticobasal degeneration (n = 11, two males, age at death 66.7 ± 5.4 years, imaging-to-pathology interval = 696.2 ± 482.2 days). A linear mixed effect model with crossed random effects was used to test regional and single-subject level associations between post-mortem regional measures of neurodegeneration and tau inclusion burden, on the one hand, and regional volume loss and seed-based intrinsic connectivity reduction, on the other. A significant association was found between tau inclusion burden and in vivo volume loss, at the regional level and independent of neurodegeneration severity, in both progressive supranuclear palsy [n = 340 regions; beta 0.036; 95\% confidence interval ({CI}): 0.001, 0.072; P = 0.046] and corticobasal degeneration (n = 215 regions; beta 0.044; 95\% {CI}: 0.009, 0.079; P = 0.013). We also found a significant association between post-mortem neurodegeneration and in vivo volume loss in both progressive supranuclear palsy (n = 340 regions; beta 0.155; 95\% {CI}: 0.061, 0.248; P = 0.001) and corticobasal degeneration (n = 215 regions; beta 0.277; 95\% {CI}: 0.104, 0.450; P = 0.002). We found a significant association between regional neurodegeneration and intrinsic connectivity dysfunction in corticobasal degeneration (n = 215 regions; beta 0.074; 95\% {CI}: 0.005, 0.143; P = 0.035), but no other associations between post-mortem measures of tauopathy and intrinsic connectivity dysfunction reached statistical significance. Our data suggest that in vivo structural imaging measures reflect independent contributions from neurodegeneration and tau burden in progressive supranuclear palsy and corticobasal degeneration. Seed-based measures of intrinsic connectivity dysfunction showed less reliable predictive value when used as in vivo biomarkers of tauopathy. The findings provide important guidance for the use of imaging biomarkers as indirect in vivo assays of microscopic pathology.},
	pages = {2068--2081},
	number = {7},
	journaltitle = {Brain},
	shortjournal = {Brain},
	author = {Spina, Salvatore and Brown, Jesse A and Deng, Jersey and Gardner, Raquel C and Nana, Alissa L and Hwang, Ji-Hye L and Gaus, Stephanie E and Huang, Eric J and Kramer, Joel H and Rosen, Howie J and Kornak, John and Neuhaus, John and Miller, Bruce L and Grinberg, Lea T and Boxer, Adam L and Seeley, William W},
	urldate = {2021-08-16},
	date = {2019-07-01},
	file = {Full Text PDF:/home/ajl/Zotero/storage/R4X9HG2P/Spina et al. - 2019 - Neuropathological correlates of structural and fun.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/ERHJILRM/5488483.html:text/html},
}

@article{nana_neurons_2019,
	title = {Neurons selectively targeted in frontotemporal dementia reveal early stage {TDP}-43 pathobiology},
	volume = {137},
	issn = {1432-0533},
	doi = {10.1007/s00401-018-1942-8},
	abstract = {{TAR} {DNA}-binding protein 43 ({TDP}-43) aggregation is the most common pathological hallmark in frontotemporal dementia ({FTD}) and characterizes nearly all patients with motor neuron disease ({MND}). The earliest stages of {TDP}-43 pathobiology are not well-characterized, and whether neurodegeneration results from {TDP}-43 loss-of-function or aggregation remains unclear. In the behavioral variant of {FTD} ({bvFTD}), patients undergo selective dropout of von Economo neurons ({VENs}) and fork cells within the frontoinsular ({FI}) and anterior cingulate cortices. Here, we examined {TDP}-43 pathobiology within these vulnerable neurons in the {FI} across a clinical spectrum including 17 patients with sporadic {bvFTD}, {MND}, or both. In an exploratory analysis based on our initial observations, we further assessed ten patients with C9orf72-associated {bvFTD}/{MND}. {VENs} and fork cells showed early, disproportionate {TDP}-43 aggregation that correlated with anatomical and clinical severity, including loss of emotional empathy. The presence of a {TDP}-43 inclusion was associated with striking nuclear and somatodendritic atrophy. An intriguing minority of neurons lacked detectable nuclear {TDP}-43 despite the apparent absence of a cytoplasmic {TDP}-43 inclusion. These cells showed neuronal atrophy comparable to inclusion-bearing neurons, suggesting that the loss of nuclear {TDP}-43 function promotes neurodegeneration, even when {TDP}-43 aggregation is inconspicuous or absent.},
	pages = {27--46},
	number = {1},
	journaltitle = {Acta Neuropathologica},
	shortjournal = {Acta Neuropathol},
	author = {Nana, Alissa L. and Sidhu, Manu and Gaus, Stephanie E. and Hwang, Ji-Hye L. and Li, Libo and Park, Youngsoon and Kim, Eun-Joo and Pasquini, Lorenzo and Allen, Isabel E. and Rankin, Katherine P. and Toller, Gianina and Kramer, Joel H. and Geschwind, Daniel H. and Coppola, Giovanni and Huang, Eric J. and Grinberg, Lea T. and Miller, Bruce L. and Seeley, William W.},
	date = {2019-01},
	pmid = {30511086},
	pmcid = {PMC6339592},
	keywords = {Adult, Aged, Amyotrophic Lateral Sclerosis, Amyotrophic lateral sclerosis ({ALS}), C9orf72, C9orf72 Protein, {DNA} Repeat Expansion, {DNA}-Binding Proteins, Empathy, Female, Frontotemporal Dementia, Frontotemporal dementia ({FTD}), Humans, Inclusion Bodies, Male, Middle Aged, Motor Neuron Disease, Neurons, Pick Disease of the Brain, {TAR} {DNA}-binding protein 43 ({TDP}-43), Von Economo neuron ({VEN})},
	file = {Accepted Version:/home/ajl/Zotero/storage/9DTKW75V/Nana et al. - 2019 - Neurons selectively targeted in frontotemporal dem.pdf:application/pdf},
}

@article{theofilas_novel_2014,
	title = {A novel approach for integrative studies on neurodegenerative diseases in human brains},
	volume = {226},
	issn = {1872-678X},
	doi = {10.1016/j.jneumeth.2014.01.030},
	abstract = {Despite a massive research effort to elucidate Alzheimer's disease ({AD}) in recent decades, effective treatment remains elusive. This failure may relate to an oversimplification of the pathogenic processes underlying {AD} and also lack of understanding of {AD} progression during its long latent stages. Although evidence shows that the two specific neuropathological hallmarks in {AD} (neuronal loss and protein accumulation), which are opposite in nature, do not progress in parallel, the great majority of studies have focused on only one of these aspects. Furthermore, research focusing on single structures is likely to render an incomplete picture of {AD} pathogenesis because as {AD} involves complete brain networks, potential compensatory mechanisms within the network may ameliorate impairment of the system to a certain extent. Here, we describe an approach for enabling integrative analysis of the dual-nature lesions, simultaneously, in all components of one of the brain networks most vulnerable to {AD}. This approach is based on significant development of methods previously described mainly by our group that were optimized and complemented for this study. It combines unbiased stereology with immunohistochemistry and immunofluorescence, making use of advanced graphics computing for three-dimensional (3D) volume reconstructions. Although this study was performed in human brainstem and focused in {AD}, it may be applied to the study of any neurological disease characterized by dual-nature lesions, in humans and animal models. This approach does not require a high level of investment in new equipment and a significant number of specimens can be processed and analyzed within a funding cycle.},
	pages = {171--183},
	journaltitle = {Journal of Neuroscience Methods},
	shortjournal = {J Neurosci Methods},
	author = {Theofilas, Panos and Polichiso, Livia and Wang, Xuehua and Lima, Luzia C. and Alho, Ana T. L. and Leite, Renata E. P. and Suemoto, Claudia K. and Pasqualucci, Carlos A. and Jacob-Filho, Wilson and Heinsen, Helmut and {Brazilian Aging Brain Study Group} and Grinberg, Lea T.},
	date = {2014-04-15},
	pmid = {24503023},
	pmcid = {PMC4083099},
	keywords = {Aged, Humans, Male, Middle Aged, 3D reconstruction, Alzheimer Disease, Alzheimer's disease, Animals, Brain, Brain Diseases, Brain network, Brain Stem, Brainstem, Disease Models, Animal, Fluorescent Antibody Technique, Human, Imaging, Three-Dimensional, Immunohistochemistry, Locus Coeruleus, Neural Pathways, Neurodegenerative diseases, Neurodegenerative Diseases, Neuropathology, Raphe Nuclei, Stereology, Time Factors},
	file = {Full Text:/home/ajl/Zotero/storage/THMD5NGM/Theofilas et al. - 2014 - A novel approach for integrative studies on neurod.pdf:application/pdf},
}

@article{dapson_macromolecular_2007,
	title = {Macromolecular changes caused by formalin fixation and antigen retrieval},
	volume = {82},
	issn = {1052-0295},
	url = {https://doi.org/10.1080/10520290701567916},
	doi = {10.1080/10520290701567916},
	abstract = {Although the mechanics of formalin fixation and antigen retrieval have been studied extensively and reviewed periodically, little attention has been directed toward conformational changes in target molecules. Formaldehyde changes the shape of tissue molecules by appending small hydroxymethyl groups to them. These adducts, in turn, can react with other tissue molecules to form crosslinks, or they can participate in a variety of reactions during tissue processing, including formation of imines, ethoxymethyl adducts, and further crosslinks. Under the influence of alcohol dehydration, fixed {DNA} may fragment and form a variety of depurination products. The situation becomes even more complex with short fixation times because under these conditions, the dehydrating agent used for tissue processing denatures macromolecules in other ways, most notably through rearrangement of molecular shape to move hydrophobic realms outward and hydrophilic areas inward (hydrophobic inversions). How tissue molecules are modified affects the outcome of immunohistochemical staining and prospects for restoration of antigenicity. Immunoreacitivity may be compromised because epitopes are either sterically hidden, but otherwise unaffected, or they have been altered more directly. Enzyme-based retrieval methods are best suited for the former because they literally snip the molecule apart to reveal the portions of interest. Heat-induced retrieval with buffers can demodify affected epitopes by removing adducts and breaking crosslinks. The choice of temperature and {pH} is usually critical for optimal retrieval. Effective temperatures are directly related to the strength of bonds-higher temperatures are needed to break stronger bonds. The {pH} of the retrieval solution determines the charge on the tissue molecule; the goal is to create a charge that causes the demodified molecule to assume a near natural conformation. Rational use of these concepts should lead to better control of immunohistochemical reactions.},
	pages = {133--140},
	number = {3},
	journaltitle = {Biotechnic \& Histochemistry},
	author = {Dapson, {RW}},
	urldate = {2021-08-16},
	date = {2007-01-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10520290701567916},
	keywords = {antigen retrieval, fixation, formaldehyde, formalin, immunohistochemistry, molecular conformation},
	file = {Full Text PDF:/home/ajl/Zotero/storage/DDXV7I7X/Dapson - 2007 - Macromolecular changes caused by formalin fixation.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/P7WEBPPC/10520290701567916.html:text/html},
}

@book{dehghani_applications_2012,
	title = {Applications of Immunocytochemistry},
	isbn = {978-953-51-0229-8},
	abstract = {Immunocytochemistry is classically defined as a procedure to detect antigens in cellular contexts using antibodies. However, over the years many aspects of this procedure have evolved within a plethora of experimental setups. There are different ways to prepare a given specimen, different kinds of antibodies to apply, different techniques for imaging, and different methods of analyzing the data. In this book, various ways of performing each individual step of immunocytochemistry in different cellular contexts are exemplified and discussed. Applications of Immunocytochemistry offers technical and background information on different steps of immunocytochemistry and presents the application of this technique and its adaptations in cell lines, neural tissue, pancreatic tissue, sputum cells, sperm cells, preimplantation embryo, arabidopsis, fish gonads, and Leishmania.},
	pagetotal = {334},
	publisher = {{BoD} – Books on Demand},
	author = {Dehghani, Hesam},
	date = {2012-03-09},
	langid = {english},
	note = {Google-Books-{ID}: {fbqZDwAAQBAJ}},
	keywords = {Medical / General, Medical / Immunology},
}

@article{dowson_evaluation_1982,
	title = {The evaluation of autofluorescence emission spectra derived from neuronal lipopigment},
	volume = {128},
	issn = {0022-2720},
	doi = {10.1111/j.1365-2818.1982.tb04628.x},
	abstract = {A method for measuring the emission spectra from regions of neuronal lipopigment in tissue sections is described and illustrated. Each emission spectrum was derived from the means of six sets of readings, from either six regions of lipopigment from six neurones which were presumed to be from a homogeneous cell population, or from one region of one neurone. Characteristics of the emission spectra from lipopigment in various forms of neuronal ceroid-lipofusinoses ({NCLs}) and in brains without evidence of {NCL} are presented and discussed. The results indicate that the classification of lipopigments should not be restricted to the two categories of 'lipofuscin' and 'ceroid'. This method may aid the identification of various pathogenic mechanisms in neurones, and provide another means of investigating the effects of certain drugs on cerebral function.},
	pages = {261--270},
	issue = {Pt 3},
	journaltitle = {Journal of Microscopy},
	shortjournal = {J Microsc},
	author = {Dowson, J. H.},
	date = {1982-12},
	pmid = {7154059},
	keywords = {Aged, Humans, Middle Aged, Neurons, Alzheimer Disease, Brain Chemistry, Ceroid, Dementia, Lipofuscin, Medulla Oblongata, Neuronal Ceroid-Lipofuscinoses, Parietal Lobe, Pigments, Biological, Spectrometry, Fluorescence},
}

@article{clancy_reduction_1998,
	title = {Reduction of background autofluorescence in brain sections following immersion in sodium borohydride},
	volume = {83},
	issn = {0165-0270},
	doi = {10.1016/s0165-0270(98)00066-1},
	abstract = {Autofluorescence of aldehyde-fixed neural tissue often obscures perikaria and fine processes labeled with fluorescent anterograde or retrograde tracers. In particular, this autofluorescence hinders the detectability of fine axonal projections labeled with the convenient anterograde tracer, tetramethylrhodamine dextranamine. Background fluorescence was notably reduced by immersion of free-floating brain tissue sections in a solution of sodium borohydride ({NaBH}4, 0.1\%), a chemical which is known to neutralize Schiffs bases through reduction of amine-aldehyde compounds into non-fluorescent salts. The reversible and renewable immersion technique was most effective in paraformaldehyde-fixed tissue where the preservation quality was improved such that labeled axons remained detectable for more than 1 year after initial preparation.},
	pages = {97--102},
	number = {2},
	journaltitle = {Journal of Neuroscience Methods},
	shortjournal = {J Neurosci Methods},
	author = {Clancy, B. and Cauller, L. J.},
	date = {1998-09-01},
	pmid = {9765122},
	keywords = {Male, Animals, Brain, Amidines, Borohydrides, Coloring Agents, Dextrans, Fixatives, Fluorescent Dyes, Formaldehyde, Glutaral, Microscopy, Fluorescence, Polymers, Rats, Rats, Sprague-Dawley, Rhodamines, Schiff Bases, Tissue Fixation},
}

@article{theofilas_novel_2014-1,
	title = {A novel approach for integrative studies on neurodegenerative diseases in human brains},
	volume = {226},
	issn = {1872-678X},
	doi = {10.1016/j.jneumeth.2014.01.030},
	abstract = {Despite a massive research effort to elucidate Alzheimer's disease ({AD}) in recent decades, effective treatment remains elusive. This failure may relate to an oversimplification of the pathogenic processes underlying {AD} and also lack of understanding of {AD} progression during its long latent stages. Although evidence shows that the two specific neuropathological hallmarks in {AD} (neuronal loss and protein accumulation), which are opposite in nature, do not progress in parallel, the great majority of studies have focused on only one of these aspects. Furthermore, research focusing on single structures is likely to render an incomplete picture of {AD} pathogenesis because as {AD} involves complete brain networks, potential compensatory mechanisms within the network may ameliorate impairment of the system to a certain extent. Here, we describe an approach for enabling integrative analysis of the dual-nature lesions, simultaneously, in all components of one of the brain networks most vulnerable to {AD}. This approach is based on significant development of methods previously described mainly by our group that were optimized and complemented for this study. It combines unbiased stereology with immunohistochemistry and immunofluorescence, making use of advanced graphics computing for three-dimensional (3D) volume reconstructions. Although this study was performed in human brainstem and focused in {AD}, it may be applied to the study of any neurological disease characterized by dual-nature lesions, in humans and animal models. This approach does not require a high level of investment in new equipment and a significant number of specimens can be processed and analyzed within a funding cycle.},
	pages = {171--183},
	journaltitle = {Journal of Neuroscience Methods},
	shortjournal = {J Neurosci Methods},
	author = {Theofilas, Panos and Polichiso, Livia and Wang, Xuehua and Lima, Luzia C. and Alho, Ana T. L. and Leite, Renata E. P. and Suemoto, Claudia K. and Pasqualucci, Carlos A. and Jacob-Filho, Wilson and Heinsen, Helmut and {Brazilian Aging Brain Study Group} and Grinberg, Lea T.},
	date = {2014-04-15},
	pmid = {24503023},
	pmcid = {PMC4083099},
	keywords = {Aged, Humans, Male, Middle Aged, 3D reconstruction, Alzheimer Disease, Alzheimer's disease, Animals, Brain, Brain Diseases, Brain network, Brain Stem, Brainstem, Disease Models, Animal, Fluorescent Antibody Technique, Human, Imaging, Three-Dimensional, Immunohistochemistry, Locus Coeruleus, Neural Pathways, Neurodegenerative diseases, Neurodegenerative Diseases, Neuropathology, Raphe Nuclei, Stereology, Time Factors},
	file = {Full Text:/home/ajl/Zotero/storage/ABGWXJLZ/Theofilas et al. - 2014 - A novel approach for integrative studies on neurod.pdf:application/pdf},
}

@article{davis_characterizing_2014,
	title = {Characterizing and Diminishing Autofluorescence in Formalin-fixed Paraffin-embedded Human Respiratory Tissue},
	volume = {62},
	issn = {1551-5044},
	doi = {10.1369/0022155414531549},
	abstract = {Tissue autofluorescence frequently hampers visualization of immunofluorescent markers in formalin-fixed paraffin-embedded respiratory tissues. We assessed nine treatments reported to have efficacy in reducing autofluorescence in other tissue types. The three most efficacious were Eriochrome black T, Sudan black B and sodium borohydride, as measured using white light laser confocal Λ2 (multi-lambda) analysis. We also assessed the impact of steam antigen retrieval and serum application on human tracheal tissue autofluorescence. Functionally fitting this Λ2 data to 2-dimensional Gaussian surfaces revealed that steam antigen retrieval and serum application contribute minimally to autofluorescence and that the three treatments are disparately efficacious. Together, these studies provide a set of guidelines for diminishing autofluorescence in formalin-fixed paraffin-embedded human respiratory tissue. Additionally, these characterization techniques are transferable to similar questions in other tissue types, as demonstrated on frozen human liver tissue and paraffin-embedded mouse lung tissue fixed in different fixatives.},
	pages = {405--423},
	number = {6},
	journaltitle = {The Journal of Histochemistry and Cytochemistry: Official Journal of the Histochemistry Society},
	shortjournal = {J Histochem Cytochem},
	author = {Davis, A. Sally and Richter, Anke and Becker, Steven and Moyer, Jenna E. and Sandouk, Aline and Skinner, Jeff and Taubenberger, Jeffery K.},
	date = {2014-06},
	pmid = {24722432},
	pmcid = {PMC4174629},
	keywords = {immunohistochemistry, autofluorescence, confocal microscopy, formalin-fixation, human, immunofluorescence, paraffin-embedded tissue, respiratory},
	file = {Full Text:/home/ajl/Zotero/storage/A8TBWTF7/Davis et al. - 2014 - Characterizing and Diminishing Autofluorescence in.pdf:application/pdf},
}

@article{schindelin_fiji_2012,
	title = {Fiji: an open-source platform for biological-image analysis},
	volume = {9},
	rights = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2019},
	doi = {10.1038/nmeth.2019},
	shorttitle = {Fiji},
	abstract = {Presented is an overview of the image-analysis software platform Fiji, a distribution of {ImageJ} that updates the underlying {ImageJ} architecture and adds modern software design elements to expand the capabilities of the platform and facilitate collaboration between biologists and computer scientists.},
	pages = {676--682},
	number = {7},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Schindelin, Johannes and Arganda-Carreras, Ignacio and Frise, Erwin and Kaynig, Verena and Longair, Mark and Pietzsch, Tobias and Preibisch, Stephan and Rueden, Curtis and Saalfeld, Stephan and Schmid, Benjamin and Tinevez, Jean-Yves and White, Daniel James and Hartenstein, Volker and Eliceiri, Kevin and Tomancak, Pavel and Cardona, Albert},
	urldate = {2021-08-16},
	date = {2012-07},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Imaging;Software
Subject\_term\_id: imaging;software},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3N5FWHKQ/Schindelin et al. - 2012 - Fiji an open-source platform for biological-image.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/4ID8UZNU/nmeth.html:text/html},
}

@report{greenwald_whole-cell_2021,
	title = {Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.03.01.431313v2},
	abstract = {Understanding the spatial organization of tissues is of critical importance for both basic and translational research. While recent advances in tissue imaging are opening an exciting new window into the biology of human tissues, interpreting the data that they create is a significant computational challenge. Cell segmentation, the task of uniquely identifying each cell in an image, remains a substantial barrier for tissue imaging, as existing approaches are inaccurate or require a substantial amount of manual curation to yield useful results. Here, we addressed the problem of cell segmentation in tissue imaging data through large-scale data annotation and deep learning. We constructed {TissueNet}, an image dataset containing {\textgreater}1 million paired whole-cell and nuclear annotations for tissue images from nine organs and six imaging platforms. We created Mesmer, a deep learning-enabled segmentation algorithm trained on {TissueNet} that performs nuclear and whole-cell segmentation in tissue imaging data. We demonstrated that Mesmer has better speed and accuracy than previous methods, generalizes to the full diversity of tissue types and imaging platforms in {TissueNet}, and achieves human-level performance for whole-cell segmentation. Mesmer enabled the automated extraction of key cellular features, such as subcellular localization of protein signal, which was challenging with previous approaches. We further showed that Mesmer could be adapted to harness cell lineage information present in highly multiplexed datasets. We used this enhanced version to quantify cell morphology changes during human gestation. All underlying code and models are released with permissive licenses as a community resource.},
	pages = {2021.03.01.431313},
	author = {Greenwald, Noah F. and Miller, Geneva and Moen, Erick and Kong, Alex and Kagel, Adam and Fullaway, Christine Camacho and {McIntosh}, Brianna J. and Leow, Ke and Schwartz, Morgan Sarah and Dougherty, Thomas and Pavelchek, Cole and Cui, Sunny and Camplisson, Isabella and Bar-Tal, Omer and Singh, Jaiveer and Fong, Mara and Chaudhry, Gautam and Abraham, Zion and Moseley, Jackson and Warshawsky, Shiri and Soon, Erin and Greenbaum, Shirley and Risom, Tyler and Hollmann, Travis and Keren, Leeat and Graf, Will and Angelo, Michael and Valen, David Van},
	urldate = {2021-08-16},
	date = {2021-03-02},
	langid = {english},
	doi = {10.1101/2021.03.01.431313},
	note = {Company: Cold Spring Harbor Laboratory
Distributor: Cold Spring Harbor Laboratory
Label: Cold Spring Harbor Laboratory
Section: New Results
Type: article},
	file = {Full Text PDF:/home/ajl/Zotero/storage/Q5VMDPFX/Greenwald et al. - 2021 - Whole-cell segmentation of tissue images with huma.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/6IXGBYJJ/2021.03.01.html:text/html},
}

@article{schmidt_cell_2018,
	title = {Cell Detection with Star-convex Polygons},
	volume = {11071},
	url = {http://arxiv.org/abs/1806.03535},
	doi = {10.1007/978-3-030-00934-2_30},
	abstract = {Automatic detection and segmentation of cells and nuclei in microscopy images is important for many biological applications. Recent successful learning-based approaches include per-pixel cell segmentation with subsequent pixel grouping, or localization of bounding boxes with subsequent shape refinement. In situations of crowded cells, these can be prone to segmentation errors, such as falsely merging bordering cells or suppressing valid cell instances due to the poor approximation with bounding boxes. To overcome these issues, we propose to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. To that end, we train a convolutional neural network that predicts for every pixel a polygon for the cell instance at that position. We demonstrate the merits of our approach on two synthetic datasets and one challenging dataset of diverse fluorescence microscopy images.},
	pages = {265--273},
	journaltitle = {{arXiv}:1806.03535 [cs]},
	author = {Schmidt, Uwe and Weigert, Martin and Broaddus, Coleman and Myers, Gene},
	urldate = {2021-08-16},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1806.03535},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/V697MKKW/Schmidt et al. - 2018 - Cell Detection with Star-convex Polygons.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/B5B8XWWX/1806.html:text/html},
}

@article{black_codex_2021,
	title = {{CODEX} multiplexed tissue imaging with {DNA}-conjugated antibodies},
	volume = {16},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1750-2799},
	url = {https://www.nature.com/articles/s41596-021-00556-8},
	doi = {10.1038/s41596-021-00556-8},
	abstract = {Advances in multiplexed imaging technologies have drastically improved our ability to characterize healthy and diseased tissues at the single-cell level. Co-detection by indexing ({CODEX}) relies on {DNA}-conjugated antibodies and the cyclic addition and removal of complementary fluorescently labeled {DNA} probes and has been used so far to simultaneously visualize up to 60 markers in situ. {CODEX} enables a deep view into the single-cell spatial relationships in tissues and is intended to spur discovery in developmental biology, disease and therapeutic design. Herein, we provide optimized protocols for conjugating purified antibodies to {DNA} oligonucleotides, validating the conjugation by {CODEX} staining and executing the {CODEX} multicycle imaging procedure for both formalin-fixed, paraffin-embedded ({FFPE}) and fresh-frozen tissues. In addition, we describe basic image processing and data analysis procedures. We apply this approach to an {FFPE} human tonsil multicycle experiment. The hands-on experimental time for antibody conjugation is {\textasciitilde}4.5 h, validation of {DNA}-conjugated antibodies with {CODEX} staining takes {\textasciitilde}6.5 h and preparation for a {CODEX} multicycle experiment takes {\textasciitilde}8 h. The multicycle imaging and data analysis time depends on the tissue size, number of markers in the panel and computational complexity.},
	pages = {3802--3835},
	number = {8},
	journaltitle = {Nature Protocols},
	shortjournal = {Nat Protoc},
	author = {Black, Sarah and Phillips, Darci and Hickey, John W. and Kennedy-Darling, Julia and Venkataraaman, Vishal G. and Samusik, Nikolay and Goltsev, Yury and Schürch, Christian M. and Nolan, Garry P.},
	urldate = {2021-08-16},
	date = {2021-08},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Protocols
Publisher: Nature Publishing Group
Subject\_term: Biological techniques;Biotechnology;Cancer;Immunology;Pathogenesis
Subject\_term\_id: biological-techniques;biotechnology;cancer;immunology;pathogenesis},
	file = {Full Text PDF:/home/ajl/Zotero/storage/X2QXQE4T/Black et al. - 2021 - CODEX multiplexed tissue imaging with DNA-conjugat.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/4ZQWGRCE/s41596-021-00556-8.html:text/html},
}

@article{stringer_cellpose_2021,
	title = {Cellpose: a generalist algorithm for cellular segmentation},
	volume = {18},
	rights = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-020-01018-x},
	doi = {10.1038/s41592-020-01018-x},
	shorttitle = {Cellpose},
	abstract = {Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly.},
	pages = {100--106},
	number = {1},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Stringer, Carsen and Wang, Tim and Michaelos, Michalis and Pachitariu, Marius},
	urldate = {2021-08-16},
	date = {2021-01},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Cell biology;Computational biology and bioinformatics
Subject\_term\_id: cell-biology;computational-biology-and-bioinformatics},
	file = {Full Text PDF:/home/ajl/Zotero/storage/MKN5HI94/Stringer et al. - 2021 - Cellpose a generalist algorithm for cellular segm.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/CLWK6TYC/s41592-020-01018-x.html:text/html},
}

@article{brown_dynamic_2022-1,
	title = {A dynamic gradient architecture generates brain activity states},
	volume = {261},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922006413},
	doi = {10.1016/j.neuroimage.2022.119526},
	abstract = {The human brain exhibits a diverse yet constrained range of activity states. While these states can be faithfully represented in a low-dimensional latent space, our understanding of the constitutive functional anatomy is still evolving. Here we applied dimensionality reduction to task-free and task {fMRI} data to address whether latent dimensions reflect intrinsic systems and if so, how these systems may interact to generate different activity states. We find that each dimension represents a dynamic activity gradient, including a primary unipolar sensory-association gradient underlying the global signal. The gradients appear stable across individuals and cognitive states, while recapitulating key functional connectivity properties including anticorrelation, modularity, and regional hubness. We then use dynamical systems modeling to show that gradients causally interact via state-specific coupling parameters to create distinct brain activity patterns. Together, these findings indicate that a set of dynamic, intrinsic spatial gradients interact to determine the repertoire of possible brain activity states.},
	pages = {119526},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Brown, Jesse A. and Lee, Alex J. and Pasquini, Lorenzo and Seeley, William W.},
	urldate = {2022-08-04},
	date = {2022-11-01},
	langid = {english},
	keywords = {Dimensionality reduction, Dynamical systems, Functional connectivity, Global signal, Gradients},
	file = {ScienceDirect Snapshot:/home/ajl/Zotero/storage/RS2483FP/S1053811922006413.html:text/html},
}

@article{mantyh_tau_2020-1,
	title = {Tau Positron Emission Tomographic Findings in a Former {US} Football Player With Pathologically Confirmed Chronic Traumatic Encephalopathy},
	volume = {77},
	issn = {2168-6149},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6990867/},
	doi = {10.1001/jamaneurol.2019.4509},
	abstract = {This case report compares in vivo 18F-flortaucipir at positron emission tomography with regions of tau pathology found at postmortem analysis in a former {US} football player with pathologically confirmed chronic traumatic encephalopathy.},
	pages = {517--521},
	number = {4},
	journaltitle = {{JAMA} Neurology},
	shortjournal = {{JAMA} Neurol},
	author = {Mantyh, William G. and Spina, Salvatore and Lee, Alex and Iaccarino, Leonardo and Soleimani-Meigooni, David and Tsoy, Elena and Mellinger, Taylor J. and Grant, Harli and Vandevrede, Lawren and La Joie, Renaud and Lesman-Segev, Orit and Gaus, Stephanie and Possin, Katherine L. and Grinberg, Lea T. and Miller, Bruce L. and Seeley, William W. and Rabinovici, Gil D.},
	urldate = {2022-08-03},
	date = {2020-04},
	pmid = {31904765},
	pmcid = {PMC6990867},
}

@article{brown_local_2021,
	title = {Local neurodegeneration and global connectivity adaptation across the {FTD}-{AD} spectrum},
	volume = {17},
	issn = {1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.055308},
	doi = {10.1002/alz.055308},
	abstract = {Background Neurodegenerative diseases involve weakened functional connectivity in disease-targeted brain areas. Equally important but overlooked is the hyperconnectivity that appears in other brain areas (Hillary and Grafman, {TICS} 2017). Hyperconnectivity has been attributed to processes like disinhibition, imbalance, compensation, and reserve. It is critical to understand the neuroanatomical mechanism underlying hyperconnectivity because this functional process may accelerate subsequent disease progression. We performed structure-function mapping for patients across subtypes and stages of the frontotemporal dementia-Alzheimer’s disease atrophy spectrum. Our goal is to develop a comprehensive model relating diverse focal atrophy patterns to corresponding brain-wide functional connectivity reconfigurations. Method We studied patients with Alzheimer’s disease ({AD}), behavioral variant {FTD} ({bvFTD}), semantic and nonfluent variant primary progressive aphasia ({svPPA}/{nfvPPA}), cortical basal syndrome ({CBS}), and healthy controls ({HC}). We included subjects who received a clinical diagnosis at the {UCSF} Memory and Aging Center and had structural and task-free functional {MRI} scans (n=281). Each subject’s gray matter atrophy map was measured using voxel-based morphometry volume loss in 273 {ROIs}. Functional connectivity matrices (273x273) were derived for each subject. We combined data for all patients and controls into a structural data matrix (281x273) and a functional data matrix (281x37128). We then performed partial least squares regression to find components that maximized the covariance between structure and function. Result The first {PLS} component captured the relationship between global atrophy burden and a distributed pattern of functional connectivity loss in unimodal cortical areas and enhancement in subcortical-cortical pathways (r=0.64; Figure 1). The second {PLS} component showed {svPPA}-like anterior temporal atrophy corresponding to atrophy-proximal connectivity deficits, with enhancements in contralateral frontoparietal areas (r=0.67). The third {PLS} component revealed a spectrum from anterior ({bvFTD}) to posterior ({AD}), contrasting frontal-insular atrophy, connectivity deficits, and parietal connectivity enhancements versus parietal atrophy, connectivity deficits, and frontal connectivity enhancements (r=0.51). Conclusion Specific atrophy subtypes across the {FTD}-{AD} spectrum associate with proximal functional connectivity reductions. Intriguingly, these subtypes also exhibit concomitant functional connectivity enhancements in more distal areas. The enhancements are of the same magnitude as the deficits and may represent a general principle of functional “load-shifting” (Jones et al, Brain 2016) away from disease-targeted areas.},
	pages = {e055308},
	issue = {S6},
	journaltitle = {Alzheimer's \& Dementia},
	author = {Brown, Jesse A. and Lee, Alex Jihun and Pasquini, Lorenzo and Friedberg, Adit and Rabinovici, Gil D. and Kramer, Joel H and Tempini, Maria Luisa Gorno and Rosen, Howard J. and Miller, Bruce L. and Seeley, William W.},
	urldate = {2022-08-03},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.055308},
	file = {Full Text PDF:/home/ajl/Zotero/storage/VVTBFD3Z/Brown et al. - 2021 - Local neurodegeneration and global connectivity ad.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/2MIFQK6B/alz.html:text/html},
}

@article{pasquini_dysfunctional_2022-1,
	title = {Dysfunctional cortical gradient topography in treatment resistant major depression},
	url = {http://medrxiv.org/content/early/2022/06/17/2022.06.16.22276402.abstract},
	doi = {10.1101/2022.06.16.22276402},
	abstract = {Background Treatment-Resistant Depression ({TRD}) refers to patients with major depressive disorder who do not remit after two or more antidepressant trials. {TRD} is common and highly debilitating, but its neurobiological basis remains poorly understood. Recent neuroimaging studies have revealed cortical connectivity gradients that dissociate primary sensorimotor areas from higher-order associative cortices. This fundamental topography determines cortical information flow and is affected by psychiatric disorders. We examined how {TRD} impacts this hierarchical cortical organization.Methods We analyzed resting-state {fMRI} data from a mindfulness-based intervention study in 56 {TRD} patients and 28 healthy controls. Using novel gradient extraction tools, measures of cortical gradient dispersion within and between functional brain networks were derived, compared across groups, and associated with graph theoretical measures of network topology. Within {TRD}, baseline cortical gradient dispersion measures were correlated with baseline clinical measures (anxiety, depression, mindfulness), as well as with changes in these measures following treatment with either mindfulness-based therapy or a health enhancement program.Results Cortical gradient dispersion was reduced within major intrinsic brain networks in {TRD}. Reduced cortical gradient dispersion correlated with increased network modularity assessed through graph theory-based measures of network topology. Lower dispersion among Default Mode Network regions, a transmodal system linked to depression symptomatology, related to current levels of trait anxiety, depression, and mindfulness, but not to changes in these domains following treatment.Conclusions Our findings reveal widespread alterations in cortical gradient architecture in {TRD}, implicating a significant role for the Default Mode Network in mediating depression, anxiety, and lower mindfulness in patients.Competing Interest {StatementThe} authors have declared no competing interest.Clinical {TrialNCT}00871299Funding {StatementThis} work was supported by {NIH} grant K99-{AG}065457 to {LP}, {DP}2-{MH}119735 to {MS}, {NIH}/{NCCAM} grant R01-{AT}004572-02S1 to {SJE} and {DHM}.Author {DeclarationsI} confirm all relevant ethical guidelines have been followed, and any necessary {IRB} and/or ethics committee approvals have been obtained.{YesThe} details of the {IRB}/oversight body that provided approval or exemption for the research described are given below:All participants or their surrogates provided written informed consent prior to participation in accordance with the declaration of Helsinki. The University of California San Francisco Committee on Human Research approved the study.I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.{YesI} understand that all clinical trials and any other prospective interventional studies must be registered with an {ICMJE}-approved registry, such as {ClinicalTrials}.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration {ID} is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial {ID} field explaining why the study was not registered in advance).{YesI} have followed all appropriate research reporting guidelines and uploaded the relevant {EQUATOR} Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.{YesParticipants} data is not publicly shared due to privacy concerns but is available from the corresponding author after reasonable request. https://github.com/lollopasquini},
	pages = {2022.06.16.22276402},
	journaltitle = {{medRxiv}},
	author = {Pasquini, Lorenzo and Fryer, Susanna L. and Eisendrath, Stuart J. and Segal, Zindel V. and Lee, Alex J. and Brown, Jesse A. and Saggar, Manish and Mathalon, Daniel H.},
	date = {2022-01-01},
}

@article{pasquini_state_2020-1,
	title = {State and trait characteristics of anterior insula time-varying functional connectivity},
	volume = {208},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S105381191931016X},
	doi = {10.1016/j.neuroimage.2019.116425},
	abstract = {The human anterior insula ({aINS}) is a topographically organized brain region, in which ventral portions contribute to socio-emotional function through limbic and autonomic connections, whereas the dorsal {aINS} contributes to cognitive processes through frontal and parietal connections. Open questions remain, however, regarding how {aINS} connectivity varies over time. We implemented a novel approach combining seed-to-whole-brain sliding-window functional connectivity {MRI} and k-means clustering to assess time-varying functional connectivity of {aINS} subregions. We studied three independent large samples of healthy participants and longitudinal datasets to assess inter- and intra-subject stability, and related {aINS} time-varying functional connectivity profiles to dispositional empathy. We identified four robust {aINS} time-varying functional connectivity modes that displayed both “state” and “trait” characteristics: while modes featuring connectivity to sensory regions were modulated by eye closure, modes featuring connectivity to higher cognitive and emotional processing regions were stable over time and related to empathy measures.},
	pages = {116425},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Pasquini, Lorenzo and Toller, Gianina and Staffaroni, Adam and Brown, Jesse A. and Deng, Jersey and Lee, Alex and Kurcyus, Katarzyna and Shdo, Suzanne M. and Allen, Isabel and Sturm, Virginia E. and Cobigo, Yann and Borghesani, Valentina and Battistella, Giovanni and Gorno-Tempini, Maria Luisa and Rankin, Katherine P. and Kramer, Joel and Rosen, Howard H. and Miller, Bruce L. and Seeley, William W.},
	urldate = {2022-08-02},
	date = {2020-03-01},
	langid = {english},
	keywords = {Anterior insula, Socio-emotional functioning, State, Time-varying functional connectivity, Trait},
	file = {Full Text:/home/ajl/Zotero/storage/WUA3VDYS/Pasquini et al. - 2020 - State and trait characteristics of anterior insula.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/PZ8XCDXV/S105381191931016X.html:text/html},
}

@article{brown_dynamic_2021,
	title = {A dynamic gradient architecture generates brain activity states},
	url = {http://biorxiv.org/content/early/2021/03/03/2020.08.12.248112.abstract},
	doi = {10.1101/2020.08.12.248112},
	abstract = {A central goal of systems neuroscience is to determine the functional-anatomical basis of brain-wide activity dynamics. While brain activity patterns appear to be low-dimensional and guided by spatial gradients, the set of gradients remains provisional and their mode of interaction is unclear. Here we applied deep learning-based dimensionality reduction to task-free {fMRI} images to derive an intrinsic latent space of human brain activity. Each dimension represented a discrete, dynamically fluctuating spatial activity gradient. The principal dimension was a novel unipolar sensory-association gradient underlying the global signal. A small set of gradients appeared to underlie key functional connectomics phenomena. Different task activation patterns were generated by gradients adopting task-specific configurations. Dynamical systems modelling revealed that gradients interact via state-specific coupling parameters, allowing accurate forecasts and simulations of task-specific brain activity. Together, these findings indicate that a small set of dynamic, interacting gradients create the repertoire of possible brain activity states.Competing Interest {StatementThe} authors have declared no competing interest.},
	pages = {2020.08.12.248112},
	journaltitle = {{bioRxiv}},
	author = {Brown, Jesse A. and Lee, Alex J. and Pasquini, Lorenzo and Seeley, William W.},
	date = {2021-01-01},
}

@article{brown_intrinsic_nodate,
	title = {Intrinsic brain activity gradients dynamically coordinate functional connectivity states},
	abstract = {Brain areas are organized into functionally connected networks though the mechanism underlying this widespread coordination remains unclear. Here we apply deep learning-based dimensionality reduction to taskfree functional magnetic resonance images to discover the principal latent dimensions of human brain functional activity. We find that each dimension corresponds to a distinct and stable spatial activity gradient. Brain areas are distributed non-uniformly along each gradient, reflecting modular boundaries and hub properties. Gradients appear to dynamically steepen or flatten to produce task-specific activation patterns. Dynamical systems modelling reveals that gradients can interact via state-specific coupling parameters, allowing accurate forecasts and simulations of brain activity during different tasks. Together, these findings indicate that a small set of overlapping global activity gradients determine the repertoire of possible functional connectivity states.},
	pages = {22},
	author = {Brown, Jesse A and Lee, Alex J and Pasquini, Lorenzo and Seeley, William W},
	langid = {english},
	file = {Brown et al. - Intrinsic brain activity gradients dynamically coo.pdf:/home/ajl/Zotero/storage/5ZC9ZEPE/Brown et al. - Intrinsic brain activity gradients dynamically coo.pdf:application/pdf},
}

@article{zhang_presymptomatic_2021,
	title = {Presymptomatic and symptomatic {MAPT} mutation carriers feature functional connectivity alterations},
	volume = {17},
	issn = {1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.054128},
	doi = {10.1002/alz.054128},
	abstract = {Background Clinical trials for tauopathies require novel biomarkers for disease detection and monitoring. One previous study found functional connectivity ({FC}) alterations in presymptomatic ({preSx}) {MAPT} mutation carriers (Whitwell et al., 2011), yet studies have not examined {FC} networks along the {MAPT} disease continuum. We hypothesized that both symptomatic (Sx) and {preSx} {MAPT} mutation carriers would show {FC} alterations compared to healthy controls ({HC}). Method Leveraging task-free {fMRI} data from the {UCSF} Memory and Aging Center and the {ARTFL}/{LEFFTDS} Consortia (Boeve et al., 2019), we compared 14 Sx and 33 {preSx} to 80 {HC} to characterize their {FC} profiles. Using a seed-based approach, we studied {FC} within networks associated with different {MAPT} clinical syndromes (i.e., salience network [{SN}] for behavioral variant frontotemporal dementia, default mode network [{DMN}] for Alzheimer’s-like amnestic syndrome, corticobasal syndrome [{CBS}] and progressive supranuclear palsy [{PSP}] networks). Complementing the seed-based approach, we next calculated whole-brain intra-/inter-network {FC} matrices for 14 networks (Brown et al., 2019), and applied K-means clustering to assess whether {preSx} displayed heterogeneous connectivity profiles. {ComBat} was applied to harmonize multi-site imaging data (Fortin et al., 2017, 2018). Thresholding was set at a joint height and extent threshold of p{\textless}0.05 (uncorrected) with age, sex, education and handedness as nuisance covariates. Result Compared to {HC}, Sx featured disrupted {FC} within key hubs of all four networks, and regions of cerebellar and pontine hyperconnectivity within {CBS} and {PSP} networks. As seen in Sx vs. {HC}, {preSx} had similar anatomical patterns of {SN}/{CBS} network hypoconnectivity and {CBS}/{PSP} network hyperconnectivity vs. {HC}. In contrast to Sx, who had {DMN} disruption, {preSx} showed {DMN} hyperconnectivity vs. {HC}. Whole-brain analyses revealed that Sx had disrupted intra-/inter-network {FC} in networks involving the insula/anterior temporal lobe. Clustering analysis identified two {preSx} subgroups. Compared to {HC}, {preSx}1 principally had disrupted {FC} across networks including those disrupted in Sx, whereas {preSx}2 mainly demonstrated hyperconnectivity. Conclusion Sx and {preSx} both demonstrated robust {FC} alterations. Future studies will investigate whether the {preSx} subgroup whose whole-brain {FC} was similar to Sx in that it showed principally {FC} disruption may be at greater risk for imminent symptom conversion and/or neurodegeneration.},
	pages = {e054128},
	issue = {S4},
	journaltitle = {Alzheimer's \& Dementia},
	author = {Zhang, Liwen and Flagan, Taru M. and Chu, Stephanie A. and Häkkinen, Suvi and Brown, Jesse A. and Lee, Alex Jihun and Pasquini, Lorenzo and Mandelli, Maria Luisa and Tempini, Maria Luisa Gorno and Appleby, Brian and Dickerson, Brad C. and Domoto-Reilly, Kimiko and Geschwind, Daniel H. and Ghoshal, Nupur and Graff-Radford, Neill R. and Grossman, Murray and Hsiung, Ging-Yuek Robin and Huey, Edward D. and Kantarci, Kejal and Karydas, Anna M. and Kaufer, Daniel and Knopman, David S. and Litvan, Irene and Mackenzie, Ian R. and Mendez, Mario and Onyike, Chiadi U. and Ramos, Eliana Marisa and Roberson, Erik D. and Trataglia, Maria Carmela and Toga, Arthur W. and Weintraub, Sandra and Forsberg, Leah K. and Heuer, Hilary W. and Boeve, Bradley F. and Boxer, Adam L. and Rosen, Howard J. and Miller, Bruce L. and Seeley, William W. and Lee, Suzee E. and Consortia, Artfl/Lefftds},
	urldate = {2022-08-02},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.054128},
	file = {Full Text PDF:/home/ajl/Zotero/storage/KY33KGUL/Zhang et al. - 2021 - Presymptomatic and symptomatic MAPT mutation carri.pdf:application/pdf},
}

@article{flagan_complement_2021,
	title = {Complement and {NfL} associations with brain structure and functional connectivity alterations in presymptomatic and symptomatic {GRN} mutation carriers},
	volume = {17},
	issn = {1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.050737},
	doi = {10.1002/alz.050737},
	abstract = {Background Progranulin ({GRN}) mutations cause autosomal dominant frontotemporal lobar degeneration. Clinical trials for {GRN}-targeted therapies are underway, yet reliable biomarkers to predict onset and track disease progression remain elusive. Task-free functional {MRI} (tf-{fMRI}) connectivity may be more sensitive than structural measures for detecting presymptomatic changes (Dopper, 2014; Premi, 2016). Presymptomatic {GRN} feature tf-{fMRI} hyperconnectivity that is more pronounced in older carriers presumably closer to symptom onset, suggesting hyperconnectivity may be a harbinger of disease (Lee, 2019). To aid the interpretation of hyperconnectivity, we explored relationships of structural and tf-{fMRI} measures with candidate fluid biomarkers for {GRN}: {CSF} complement proteins C3b and C1q, which drive neurodegeneration in {GRN} -/- mice (Lui, 2016) and plasma neurofilament light chain ({NfL}), a marker of axonal injury. Method We studied 20 symptomatic {GRN} (Sx), 39 presymptomatic {GRN} ({preSx}), and 67 healthy-controls ({HC}). Cross-sectional {CSF} C3b, C1q, and plasma {NfL} concentrations were compared between groups. Correlations controlling for age assessed relationships between fluid biomarkers and symptom severity ({CDR}®+{NACC}-{FTLD} box score). Voxelwise multiple regression models examined interactions between fluid biomarkers and gene status on grey matter probability maps ({GM}) and voxelwise tf-{fMRI} whole-brain weighted degree ({WBD}) maps. Result Complement concentrations were similar between Sx, {preSx}, and {HC}. Across all {GRN}, C3b concentrations increased with symptom severity (r=0.34, p=0.04). In Sx, but not {preSx}, higher C3b was associated with increased {WBD} in medial-frontal regions, left insula, and thalamus compared to {HC}. Sx had higher {NfL} concentrations compared to {preSx} and {HC}. Across all {GRN} and in Sx, higher {NfL} was associated with greater symptom severity (r=0.60, p{\textless}0.001; r=0.72, p{\textless}0.001). In Sx, higher {NfL} was associated with lower {GM} in precuneus and bilateral frontoinsular cortices, but not {WBD}, compared to {HC}. In {preSx}, higher {NfL} was associated with increased {WBD} in thalamus and right temporal regions, but not {GM}, compared to {HC}. Conclusion C3b and {NfL} concentrations correlated with symptom severity. C3b concentrations were associated with tf-{fMRI} connectivity alterations in Sx while {NfL} concentrations were associated with connectivity alterations in {preSx}. Longitudinal studies are needed to determine temporal relationships between altered connectivity, atrophy, complement, {NfL} rise, and symptom onset.},
	pages = {e050737},
	issue = {S4},
	journaltitle = {Alzheimer's \& Dementia},
	author = {Flagan, Taru M. and Chu, Stephanie A. and Häkkinen, Suvi and {McFall}, David and Heller, Carolin and Rohrer, Jonathan D and Brown, Jesse A. and Lee, Alex Jihun and Pasquini, Lorenzo and Mandelli, Maria Luisa and Gorno-Tempini, Marilu and Appleby, Brian and Dickerson, Brad C. and Domoto-Reilly, Kimiko and Foroud, Tatiana M. and Geschwind, Daniel H. and Ghoshal, Nupur and Graff-Radford, Neill R. and Grossman, Murray and Hsiung, Ging-Yuek Robin and Huang, Eric J. and Huey, Edward D. and Kantarci, Kejal and Karydas, Anna M. and Kaufer, Daniel and Knopman, David S. and Litvan, Irene and Mackenzie, Ian R and Mendez, Mario F. and Onyike, Chiadi U and Petrucelli, Leonard and Ramos, Eliana Marisa and Roberson, Erik D. and Rojas, Julio C. and Tartaglia, Maria Carmela and Toga, Arthur W. and Weintraub, Sandra and Forsberg, Leah K. and Heuer, Hilary W. and Boeve, Bradley F. and Boxer, Adam L. and Rosen, Howard J. and Miller, Bruce L. and Moreno, Fermin and Seeley, William W. and Lee, Suzee E. and Consortia, Artfl/Lefftds},
	urldate = {2022-08-01},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.050737},
	file = {Full Text PDF:/home/ajl/Zotero/storage/6TLXZMXV/Flagan et al. - 2021 - Complement and NfL associations with brain structu.pdf:application/pdf},
}

@misc{pasquini_dynamic_2021,
	title = {Dynamic autonomic nervous system patterns differentiate human emotions and manifest in resting physiology},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.06.14.448456v1},
	doi = {10.1101/2021.06.14.448456},
	abstract = {Whether activity in the autonomic nervous system differs during distinct emotions remains controversial. We obtained continuous multichannel recordings of autonomic nervous system activity in healthy adults during a video-based emotional reactivity task. Dimensionality reduction revealed five principal components in the autonomic time series data, and these modes of covariation differentiated periods of baseline from those of video-viewing. Unsupervised clustering of the principal component time series data uncovered separable autonomic states that distinguished among the five emotion-inducing trials. These autonomic states were also detected in baseline physiology but were intermittent and of smaller magnitude. Our results suggest the autonomic nervous system assembles dynamic activity patterns during emotions that are similar across people and are present even during undirected moments of rest.
One Sentence Summary Dynamic autonomic patterns distinguish among emotions and are evident in resting physiology.},
	publisher = {{bioRxiv}},
	author = {Pasquini, Lorenzo and Noohi, Fatemeh and Veziris, Christina R. and Kosik, Eena L. and Holley, Sarah R. and Lee, Alex and Brown, Jesse A. and Roy, Ashlin R. K. and Chow, Tiffany E. and Allen, Isabel and Rosen, Howard J. and Kramer, Joel H. and Miller, Bruce L. and Saggar, Manish and Seeley, William W. and Sturm, Virginia E.},
	urldate = {2022-08-01},
	date = {2021-06-15},
	langid = {english},
	note = {Pages: 2021.06.14.448456
Section: New Results},
	file = {Full Text PDF:/home/ajl/Zotero/storage/6I7XBADX/Pasquini et al. - 2021 - Dynamic autonomic nervous system patterns differen.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/GFEN43PD/2021.06.14.html:text/html},
}

@article{mantyh_tau-pet_2020,
	title = {Tau-{PET} in a Former American Football Player with Pathologically Confirmed Chronic Traumatic Encephalopathy (2452)},
	volume = {94},
	rights = {© 2020},
	issn = {0028-3878, 1526-632X},
	url = {https://n.neurology.org/content/94/15_Supplement/2452},
	abstract = {Objective: Assess the regional relationship between in-vivo 18F-Flortaucipir ({FTP}) binding and postmortem tau pathology in a patient with pathologically-confirmed {CTE}.
Background: Biomarkers for Chronic Traumatic Encephalopathy ({CTE}) are currently lacking. {FTP} detects tau pathology in Alzheimer’s disease, and positron emission tomography ({PET}) with {FTP} shows elevated binding in individuals at risk for {CTE}. No study, however, has assessed the correlation between in-vivo {FTP} {PET} and postmortem tau in {CTE}.
Design/Methods: At a behavioral neurology tertiary referral center, a male former National Football League player with 17-years of American football exposure was clinically diagnosed with Traumatic Encephalopathy Syndrome. He underwent {MRI}, 18F-fluorodeoxyglucose, 11C-{PIB} and {FTP} {PET} 52 months prior to neuropathological examination. {FTP} Standardized Uptake Value Ratios ({SUVR}) (inferior cerebellar gray reference region) and W-score (age-adjusted Z-score) maps were qualitatively and quantitatively compared to phosphorylated tau ({CP}-13) immunostaining.
Results: {FTP} uptake was distributed in a patchy, frontotemporal predominant pattern, overlapping with regions showing neurodegeneration on {MRI} and hypometabolism on 18F-fluorodeoxyglucose {PET}. 11C-{PIB} {PET} was negative. Pathological assessment revealed stage {IV} {CTE}, limbic argyrophilic grain disease, Stage 2 limbic-predominant age-related {TDP}-43 Encephalopathy, and Braak neurofibrillary tangle stage 3. Amyloid β and alpha synuclein immunostaining were negative. {FTP} W-maps matched areas of high post-mortem tau burden in left fusiform and inferior temporal gyri and juxta-cortical frontal white matter. High {FTP} W-scores with low tau were found in basal ganglia, thalamus, motor cortex and calcarine cortex. No regions with low {FTP} W-scores corresponded to areas with high pathological tau burden. A modest correlation, which did not reach statistical significance (rho=0.35, p=0.17), was found between {FTP} {SUVR} and tau area fraction at the regional level.
Conclusions: {FTP} {PET} during life showed a modest correspondence with post-mortem pathology in {CTE}. Although based on a single case, {FTP} may have limited utility as a tau biomarker in {CTE}.Disclosure: Dr. Mantyh has nothing to disclose. Dr. Spina has nothing to disclose. Dr. Lee has nothing to disclose. Dr. Iaccarino has nothing to disclose. Dr. Soleimani-Meigooni has nothing to disclose. Dr. Tsoy has nothing to disclose. Dr. Mellinger has nothing to disclose. Dr. Grant has nothing to disclose. Dr. {VandeVrede} has nothing to disclose. Dr. La Joie has nothing to disclose. Dr. Lesman-Segev has nothing to disclose. Dr. Gaus has nothing to disclose. Dr. Possin has received research support from Quest Diagnostics. Dr. Grinberg has nothing to disclose. Dr. Miller has nothing to disclose. Dr. Seeley has received personal compensation for consulting, serving on a scientific advisory board, speaking, or other activities with Bristol Myers-Squibb, Merck Inc., and Biogen. Dr. Rabinovici has received personal compensation for consulting, serving on a scientific advisory board, speaking, or other activities with Eisai, Axon Neurosicience, and {GE} Healthcare. Dr. Rabinovici has received personal compensation in an editorial capacity for {JAMA} Neurology. Dr. Rabinovici has received research support from Avid Radiopharmaceuticals, Eli Lilly, {GE} Healthcare, Piramal, and Life Molecular Imaging.},
	number = {15},
	journaltitle = {Neurology},
	author = {Mantyh, William and Spina, Salvatore and Lee, Alex and Iaccarino, Leonardo and Soleimani-Meigooni, David and Tsoy, Elena and Mellinger, Taylor and Grant, Harli and {VandeVrede}, Lawren and Joie, Renaud La and Lesman-Segev, Orit and Gaus, Stephanie and Possin, Katherine and Grinberg, Lea and Miller, Bruce and Seeley, William and Rabinovici, Gil},
	urldate = {2022-07-29},
	date = {2020-04-14},
	langid = {english},
	note = {Publisher: Wolters Kluwer Health, Inc. on behalf of the American Academy of Neurology
Section: Monday, April 27},
}

@article{yuan_induction_2008,
	title = {Induction of {HIF}-1α Expression by Intermittent Hypoxia: Involvement of {NADPH} Oxidase, Ca2+ Signaling, Prolyl Hydroxylases, and {mTOR}},
	volume = {217},
	issn = {0021-9541},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2696817/},
	doi = {10.1002/jcp.21537},
	shorttitle = {Induction of {HIF}-1α Expression by Intermittent Hypoxia},
	abstract = {Sleep-disordered breathing with recurrent apnea (periodic cessation of breathing) results in chronic intermittent hypoxia ({IH}), which leads to cardiovascular and respiratory pathology. Molecular mechanisms underlying {IH}-evoked cardio-respiratory co-morbidities have not been delineated. Mice with heterozygous deficiency of hypoxia-inducible factor 1α ({HIF}-1α) do not develop cardio-respiratory responses to chronic {IH}. {HIF}-1α protein expression and {HIF}-1 transcriptional activity are induced by {IH} in {PC}12 cells. In the present study, we investigated the signaling pathways associated with {IH}-evoked {HIF}-1α accumulation. {PC}12 cells were exposed to aerobic conditions (20\% O2) or 60 cycles of {IH} (30 sec at 1.5\% O2 followed by 5 min at 20\% O2). Our results show that {IH}-induced {HIF}-1α accumulation is due to increased generation of {ROS} by {NADPH} oxidase. We further demonstrate that {ROS}-dependent Ca2+ signaling pathways involving phospholipase Cγ and protein kinase C activation are required for {IH}-evoked {HIF}-1α accumulation. {IH} leads to activation of {mTOR} and S6 kinase and rapamycin partially inhibited {IH}-induced {HIF}-1α accumulation. {IH} also decreased hydroxylation of {HIF}-1α protein and anti-oxidants as well as inhibitors of Ca+2 signaling prevented this response. Thus, both increased {mTOR}-dependent {HIF}-1α synthesis and decreased hydroxylase-dependent {HIF}-1α degradation contribute to {IH}-evoked {HIF}-1α accumulation. Following {IH}, {HIF}-1α and phosphorylated {mTOR} levels remained elevated during 90 min of re-oxygenation despite re-activation of prolyl hydroxylase. Rapamycin or cycloheximide, blocked increased {HIF}-1α levels during re-oxygenation indicating that {mTOR}-dependent protein synthesis is required for the persistent elevation of {HIF}-1α levels during re-oxygenation.},
	pages = {674--685},
	number = {3},
	journaltitle = {Journal of cellular physiology},
	shortjournal = {J Cell Physiol},
	author = {Yuan, Guoxiang and Nanduri, Jayasri and Khan, Shakil and Semenza, Gregg L. and Prabhakar, Nanduri R.},
	urldate = {2022-06-01},
	date = {2008-12},
	pmid = {18651560},
	pmcid = {PMC2696817},
	file = {Accepted Version:/home/ajl/Zotero/storage/578YXBTH/Yuan et al. - 2008 - Induction of HIF-1α Expression by Intermittent Hyp.pdf:application/pdf},
}

@article{taylor_mitochondria_2007,
	title = {Mitochondria and cellular oxygen sensing in the {HIF} pathway},
	volume = {409},
	issn = {0264-6021},
	url = {https://doi.org/10.1042/BJ20071249},
	doi = {10.1042/BJ20071249},
	abstract = {Mitochondrial respiration is responsible for more than 90\% of oxygen consumption in humans. Cells utilize oxygen as the final electron acceptor in the aerobic metabolism of glucose to generate {ATP} which fuels most active cellular processes. Consequently, a drop in tissue oxygen levels to the point where oxygen demand exceeds supply (termed hypoxia) leads rapidly to metabolic crisis and represents a severe threat to ongoing physiological function and ultimately, viability. Because of the central role of oxygen in metabolism, it is perhaps not surprising that we have evolved an efficient and rapid molecular response system which senses hypoxia in cells, leading to the induction of an array of adaptive genes which facilitate increased oxygen supply and support anaerobic {ATP} generation. This response is governed by {HIF} (hypoxia-inducible factor). The oxygen sensitivity of this pathway is conferred by a family of hydroxylases which repress {HIF} activity in normoxia allowing its rapid activation in hypoxia. Because of its importance in a diverse range of disease states, the mechanism by which cells sense hypoxia and transduce a signal to the {HIF} pathway is an area of intense investigation. Inhibition of mitochondrial function reverses hypoxia-induced {HIF} leading to speculation of a role for mitochondria in cellular oxygen sensing. However, the nature of the signal between mitochondria and oxygen-sensing hydroxylase enzymes has remained controversial. In the present review, two models of the role for mitochondria in oxygen sensing will be discussed and recent evidence will be presented which raises the possibility that these two models which implicate {ROS} (reactive oxygen species) and oxygen redistribution respectively may complement each other and facilitate rapid and dynamic activation of the {HIF} pathway in hypoxia.},
	pages = {19--26},
	number = {1},
	journaltitle = {Biochemical Journal},
	shortjournal = {Biochemical Journal},
	author = {Taylor, Cormac T.},
	urldate = {2022-06-01},
	date = {2007-12-11},
	file = {Full Text PDF:/home/ajl/Zotero/storage/HWBD3PNX/Taylor - 2007 - Mitochondria and cellular oxygen sensing in the HI.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/BYUSNCEH/Mitochondria-and-cellular-oxygen-sensing-in-the.html:text/html},
}

@article{grist_long-term_2019,
	title = {Long-term monitoring in a microfluidic system to study tumour spheroid response to chronic and cycling hypoxia},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-54001-8},
	doi = {10.1038/s41598-019-54001-8},
	abstract = {We demonstrate the application of a microfluidic platform combining spatiotemporal oxygen control and long-term microscopy monitoring to observe tumour spheroid response to hypoxia. The platform is capable of recreating physiologically-relevant low and cycling oxygen levels not attainable in traditional cell culture environments, while image-based monitoring visualizes cell response to these physiologically-relevant conditions. Monitoring spheroid cultures during hypoxic exposure allows us to observe, for the first time, that spheroids swell and shrink in response to time-varying oxygen profiles switching between 0\% and 10\% O2; this swelling-shrinkage behaviour appears to be driven by swelling of individual cells within the spheroids. We also apply the system to monitoring tumour models during anticancer treatment under varying oxygen conditions. We observe higher uptake of the anticancer agent doxorubicin under a cycling hypoxia profile than under either chronic hypoxia or in vitro normoxia, and the two-photon microscopy monitoring facilitated by our system also allows us to observe heterogeneity in doxorubicin uptake within spheroids at the single-cell level. Combining optical sectioning microscopy with precise spatiotemporal oxygen control and 3D culture opens the door for a wide range of future studies on microenvironmental mechanisms driving cancer progression and resistance to anticancer therapy. These types of studies could facilitate future improvements in cancer diagnostics and treatment.},
	pages = {17782},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Grist, Samantha M. and Nasseri, S. Soroush and Laplatine, Loïc and Schmok, Jonathan C. and Yao, Dickson and Hua, Jessica and Chrostowski, Lukas and Cheung, Karen C.},
	urldate = {2022-06-01},
	date = {2019-11-28},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Assay systems, Biomedical engineering},
	file = {Full Text PDF:/home/ajl/Zotero/storage/UIWRGGQD/Grist et al. - 2019 - Long-term monitoring in a microfluidic system to s.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/PSDAUWTW/s41598-019-54001-8.html:text/html},
}

@online{noauthor_activation_nodate,
	title = {Activation of tyrosine hydroxylase by intermittent hypoxia: involvement of serine phosphorylation {\textbar} Journal of Applied Physiology},
	url = {https://journals.physiology.org/doi/full/10.1152/japplphysiol.00186.2003?rfr_dat=cr_pub++0pubmed&url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org},
	urldate = {2022-06-01},
	file = {Activation of tyrosine hydroxylase by intermittent hypoxia\: involvement of serine phosphorylation | Journal of Applied Physiology:/home/ajl/Zotero/storage/526VRBRX/japplphysiol.00186.html:text/html},
}

@article{yuan_role_2004,
	title = {Role of oxidative stress in intermittent hypoxia-induced immediate early gene activation in rat {PC}12 cells},
	volume = {557},
	issn = {0022-3751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1665161/},
	doi = {10.1113/jphysiol.2003.058503},
	abstract = {Intermittent hypoxia ({IH}) occurs in many pathophysiological conditions. The molecular mechanisms associated with {IH}, however, have received little attention. Previous studies have reported that the c-fos gene via formation of activator protein-1 ({AP}-1) transcription factor contributes to adaptive responses to continuous hypoxia. In the present study, using a cell culture model we examined whether {IH} activates c-fos and {AP}-1 and if so, by what mechanisms. Experiments were performed on rat phaeochromocytoma cells exposed to 21\% O2 (normoxia) or 60 and 120 cycles of {IH}, each cycle consisting 15 s of hypoxia followed by 4 min of normoxia. {IH} resulted in a significant elevation of c-fos {mRNA} as well as transcriptional activation. {IH} was more potent and induced a longer lasting activation of c-fos than comparable cumulative duration of continuous hypoxia. {IH} increased {AP}-1 activity and tyrosine hydroxylase ({TH}) {mRNA}, an {AP}-1-regulated downstream gene, and these effects were prevented by antisense c-fos. Superoxide dismutase mimetic, a potent scavenger of superoxide anions, prevented {IH}-induced c-fos, {AP}-1 and {TH} activations. {IH} increased superoxide anion levels in mitochondria as evidenced by decreased aconitase enzyme activity and increased levels of hydrogen peroxide, a stable dismutated product of superoxide anions. Complex I of the mitochondrial electron transport chain was markedly inhibited in {IH} exposed cells. Pharmacological inhibitors of complex I mimicked the effects of {IH} during normoxia and occluded the effects of {IH} on c-fos activation, suggesting the involvement of the mitochondrial electron transport chain in the generation of superoxide anions during {IH}. These results suggest {IH}-induced c-fos-mediated transcriptional activation involves oxidative stress.},
	pages = {773--783},
	issue = {Pt 3},
	journaltitle = {The Journal of Physiology},
	shortjournal = {J Physiol},
	author = {Yuan, Guoxiang and Adhikary, Gautam and {McCormick}, Andrew A and Holcroft, John J and Kumar, Ganesh K and Prabhakar, Nanduri R},
	urldate = {2022-06-01},
	date = {2004-06-15},
	pmid = {15107478},
	pmcid = {PMC1665161},
	file = {Full Text:/home/ajl/Zotero/storage/KAZU4IDG/Yuan et al. - 2004 - Role of oxidative stress in intermittent hypoxia-i.pdf:application/pdf},
}

@online{noauthor_hif-1dependent_nodate,
	title = {{HIF}-1–Dependent Respiratory, Cardiovascular, and Redox Responses to Chronic Intermittent Hypoxia},
	url = {https://www.liebertpub.com/doi/epdf/10.1089/ars.2007.1691},
	urldate = {2022-06-01},
	langid = {english},
	doi = {10.1089/ars.2007.1691},
	file = {Snapshot:/home/ajl/Zotero/storage/6V6TVNYC/ars.2007.html:text/html},
}

@article{yeo_hypoxia_2019,
	title = {Hypoxia and aging},
	volume = {51},
	rights = {2019 The Author(s)},
	issn = {2092-6413},
	url = {https://www.nature.com/articles/s12276-019-0233-3},
	doi = {10.1038/s12276-019-0233-3},
	abstract = {Eukaryotic cells require sufficient oxygen (O2) for biological activity and survival. When the oxygen demand exceeds its supply, the oxygen levels in local tissues or the whole body decrease (termed hypoxia), leading to a metabolic crisis, threatening physiological functions and viability. Therefore, eukaryotes have developed an efficient and rapid oxygen sensing system: hypoxia-inducible factors ({HIFs}). The hypoxic responses are controlled by {HIFs}, which induce the expression of several adaptive genes to increase the oxygen supply and support anaerobic {ATP} generation in eukaryotic cells. Hypoxia also contributes to a functional decline during the aging process. In this review, we focus on the molecular mechanisms regulating {HIF}-1α and aging-associated signaling proteins, such as sirtuins, {AMP}-activated protein kinase, mechanistic target of rapamycin complex 1, {UNC}-51-like kinase 1, and nuclear factor κB, and their roles in aging and aging-related diseases. In addition, the effects of prenatal hypoxia and obstructive sleep apnea ({OSA})-induced intermittent hypoxia have been reviewed due to their involvement in the progression and severity of many diseases, including cancer and other aging-related diseases. The pathophysiological consequences and clinical manifestations of prenatal hypoxia and {OSA}-induced chronic intermittent hypoxia are discussed in detail.},
	pages = {1--15},
	number = {6},
	journaltitle = {Experimental \& Molecular Medicine},
	shortjournal = {Exp Mol Med},
	author = {Yeo, Eui-Ju},
	urldate = {2022-06-01},
	date = {2019-06},
	langid = {english},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {Experimental models of disease, Senescence},
	file = {Full Text PDF:/home/ajl/Zotero/storage/7I34MDYW/Yeo - 2019 - Hypoxia and aging.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/TCSHEHNE/s12276-019-0233-3.html:text/html},
}

@online{noauthor_programmable_nodate,
	title = {Programmable eukaryotic protein expression with {RNA} sensors {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2022.01.26.477951v1},
	urldate = {2022-06-01},
	file = {Programmable eukaryotic protein expression with RNA sensors | bioRxiv:/home/ajl/Zotero/storage/89KAGWWW/2022.01.26.html:text/html},
}

@article{martinez_cell_2019,
	title = {A Cell Culture Model that Mimics Physiological Tissue Oxygenation Using Oxygen-permeable Membranes},
	volume = {9},
	issn = {2331-8325},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7854217/},
	doi = {10.21769/BioProtoc.3371},
	abstract = {Dissolved oxygen and its availability to cells in culture is an overlooked variable which can have significant consequences on experimental research outcomes, including reproducibility. Oxygen sensing pathways play key roles in cell growth and behavior and pericellular oxygen levels should be controlled when establishing in vitro models. Standard cell culture techniques do not have adequate control over pericellular oxygen levels. Slow diffusion through culture media limits the precision of oxygen delivery to cells, making it difficult to accurately reproduce in vivo-like oxygen conditions. Furthermore, different types of cells consume oxygen at varying rates and this can be affected by the density of growing cells. Here, we describe a novel in vitro system that utilizes hypoxic chambers and oxygen-permeable culture dishes to control pericellular oxygen levels and provide rapid oxygen delivery to adherent cells. This procedure is particularly relevant for protocols studying effects of rapid oxygen changes or intermittent hypoxia on cellular behavior. The system is inexpensive and easily assembled without highly specialized equipment.},
	pages = {e3371},
	number = {18},
	journaltitle = {Bio-protocol},
	shortjournal = {Bio Protoc},
	author = {Martinez, Chloe-Anne and Cistulli, Peter A. and Cook, Kristina M.},
	urldate = {2022-06-01},
	date = {2019-09-20},
	pmid = {33654867},
	pmcid = {PMC7854217},
	file = {Full Text:/home/ajl/Zotero/storage/Y2E5PRUT/Martinez et al. - 2019 - A Cell Culture Model that Mimics Physiological Tis.pdf:application/pdf},
}

@report{martinez_intermittent_2021,
	title = {Intermittent hypoxia enhances the expression of {HIF}1A by increasing the quantity and catalytic activity of {KDM}4A-C and demethylating H3K9me3 at the {HIF}1A locus},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2021.07.25.453726v1},
	abstract = {Cellular oxygen-sensing pathways are primarily regulated by hypoxia inducible factor-1 ({HIF}-1) in chronic hypoxia and are well studied. Intermittent hypoxia also occurs in many pathological conditions, yet little is known about its biological effects. In this study, we investigated how two proposed cellular oxygen sensing systems, {HIF}-1 and {KDM}4A-C, respond to cells exposed to intermittent hypoxia and compared to chronic hypoxia. We found that intermittent hypoxia increases {HIF}-1 activity through a pathway distinct from chronic hypoxia, involving the {KDM}4A, -B and -C histone lysine demethylases. Intermittent hypoxia increases the quantity and activity of {KDM}4A-C resulting in a decrease in H3K9 methylation. This contrasts with chronic hypoxia, which decreases {KDM}4A-C activity, leading to hypermethylation of H3K9. Demethylation of histones bound to the {HIF}1A gene in intermittent hypoxia increases {HIF}1A {mRNA} expression, which has the downstream effect of increasing overall {HIF}-1 activity and expression of {HIF} target genes. This study highlights how multiple oxygen-sensing pathways can interact to regulate and fine tune the cellular hypoxic response depending on the period and length of hypoxia.},
	pages = {2021.07.25.453726},
	institution = {{bioRxiv}},
	author = {Martinez, Chloe-Anne and Bal, Neha and Cistulli, Peter A. and Cook, Kristina M.},
	urldate = {2022-06-01},
	date = {2021-07-26},
	langid = {english},
	doi = {10.1101/2021.07.25.453726},
	note = {Section: New Results
Type: article},
	file = {Full Text PDF:/home/ajl/Zotero/storage/J8VPDNNZ/Martinez et al. - 2021 - Intermittent hypoxia enhances the expression of HI.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/PDBD254U/2021.07.25.453726v1.html:text/html},
}

@article{cavadas_rest_2016,
	title = {{REST} is a hypoxia-responsive transcriptional repressor},
	volume = {6},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4987654/},
	doi = {10.1038/srep31355},
	abstract = {Cellular exposure to hypoxia results in altered gene expression in a range of physiologic and pathophysiologic states. Discrete cohorts of genes can be either up- or down-regulated in response to hypoxia. While the Hypoxia-Inducible Factor ({HIF}) is the primary driver of hypoxia-induced adaptive gene expression, less is known about the signalling mechanisms regulating hypoxia-dependent gene repression. Using {RNA}-seq, we demonstrate that equivalent numbers of genes are induced and repressed in human embryonic kidney ({HEK}293) cells. We demonstrate that nuclear localization of the Repressor Element 1-Silencing Transcription factor ({REST}) is induced in hypoxia and that {REST} is responsible for regulating approximately 20\% of the hypoxia-repressed genes. Using chromatin immunoprecipitation assays we demonstrate that {REST}-dependent gene repression is at least in part mediated by direct binding to the promoters of target genes. Based on these data, we propose that {REST} is a key mediator of gene repression in hypoxia.},
	pages = {31355},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Cavadas, Miguel A. S. and Mesnieres, Marion and Crifo, Bianca and Manresa, Mario C. and Selfridge, Andrew C. and Keogh, Ciara E. and Fabian, Zsolt and Scholz, Carsten C. and Nolan, Karen A. and Rocha, Liliane M. A. and Tambuwala, Murtaza M. and Brown, Stuart and Wdowicz, Anita and Corbett, Danielle and Murphy, Keith J. and Godson, Catherine and Cummins, Eoin P. and Taylor, Cormac T. and Cheong, Alex},
	urldate = {2022-06-01},
	date = {2016-08-17},
	pmid = {27531581},
	pmcid = {PMC4987654},
	file = {Full Text:/home/ajl/Zotero/storage/LLGDJ6WS/Cavadas et al. - 2016 - REST is a hypoxia-responsive transcriptional repre.pdf:application/pdf},
}

@article{cavadas_rest_2016-1,
	title = {{REST} is a hypoxia-responsive transcriptional repressor},
	volume = {6},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4987654/},
	doi = {10.1038/srep31355},
	abstract = {Cellular exposure to hypoxia results in altered gene expression in a range of physiologic and pathophysiologic states. Discrete cohorts of genes can be either up- or down-regulated in response to hypoxia. While the Hypoxia-Inducible Factor ({HIF}) is the primary driver of hypoxia-induced adaptive gene expression, less is known about the signalling mechanisms regulating hypoxia-dependent gene repression. Using {RNA}-seq, we demonstrate that equivalent numbers of genes are induced and repressed in human embryonic kidney ({HEK}293) cells. We demonstrate that nuclear localization of the Repressor Element 1-Silencing Transcription factor ({REST}) is induced in hypoxia and that {REST} is responsible for regulating approximately 20\% of the hypoxia-repressed genes. Using chromatin immunoprecipitation assays we demonstrate that {REST}-dependent gene repression is at least in part mediated by direct binding to the promoters of target genes. Based on these data, we propose that {REST} is a key mediator of gene repression in hypoxia.},
	pages = {31355},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Cavadas, Miguel A. S. and Mesnieres, Marion and Crifo, Bianca and Manresa, Mario C. and Selfridge, Andrew C. and Keogh, Ciara E. and Fabian, Zsolt and Scholz, Carsten C. and Nolan, Karen A. and Rocha, Liliane M. A. and Tambuwala, Murtaza M. and Brown, Stuart and Wdowicz, Anita and Corbett, Danielle and Murphy, Keith J. and Godson, Catherine and Cummins, Eoin P. and Taylor, Cormac T. and Cheong, Alex},
	urldate = {2022-06-01},
	date = {2016-08-17},
	pmid = {27531581},
	pmcid = {PMC4987654},
	file = {Full Text:/home/ajl/Zotero/storage/V6ARRFEX/Cavadas et al. - 2016 - REST is a hypoxia-responsive transcriptional repre.pdf:application/pdf},
}

@article{cavadas_hypoxia-inducible_2013,
	title = {Hypoxia-inducible factor ({HIF}) network: insights from mathematical models},
	volume = {11},
	issn = {1478-811X},
	url = {https://doi.org/10.1186/1478-811X-11-42},
	doi = {10.1186/1478-811X-11-42},
	shorttitle = {Hypoxia-inducible factor ({HIF}) network},
	abstract = {Oxygen is a crucial molecule for cellular function. When oxygen demand exceeds supply, the oxygen sensing pathway centred on the hypoxia inducible factor ({HIF}) is switched on and promotes adaptation to hypoxia by up-regulating genes involved in angiogenesis, erythropoiesis and glycolysis. The regulation of {HIF} is tightly modulated through intricate regulatory mechanisms. Notably, its protein stability is controlled by the oxygen sensing prolyl hydroxylase domain ({PHD}) enzymes and its transcriptional activity is controlled by the asparaginyl hydroxylase {FIH} (factor inhibiting {HIF}-1).},
	pages = {42},
	number = {1},
	journaltitle = {Cell Communication and Signaling},
	shortjournal = {Cell Communication and Signaling},
	author = {Cavadas, Miguel {AS} and Nguyen, Lan K. and Cheong, Alex},
	urldate = {2022-06-01},
	date = {2013-06-10},
	keywords = {{FIH}, {HIF}, Hypoxia, Mathematical model, {PHD}},
	file = {Full Text PDF:/home/ajl/Zotero/storage/TDZVWYYB/Cavadas et al. - 2013 - Hypoxia-inducible factor (HIF) network insights f.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/KAU535PY/1478-811X-11-42.html:text/html},
}

@article{dobrynin_kdm4a_2017,
	title = {{KDM}4A regulates {HIF}-1 levels through H3K9me3},
	volume = {7},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5593970/},
	doi = {10.1038/s41598-017-11658-3},
	abstract = {Regions of hypoxia (low oxygen) occur in most solid tumours and cells in these areas are the most aggressive and therapy resistant. In response to decreased oxygen, extensive changes in gene expression mediated by Hypoxia-Inducible Factors ({HIFs}) contribute significantly to the aggressive hypoxic tumour phenotype. In addition to {HIFs}, multiple histone demethylases are altered in their expression and activity, providing a secondary mechanism to extend the hypoxic signalling response. In this study, we demonstrate that the levels of {HIF}-1α are directly controlled by the repressive chromatin mark, H3K9me3. In conditions where the histone demethylase {KDM}4A is depleted or inactive, H3K9me3 accumulates at the {HIF}-1α locus, leading to a decrease in {HIF}-1α {mRNA} and a reduction in {HIF}-1α stabilisation. Loss of {KDM}4A in hypoxic conditions leads to a decreased {HIF}-1α mediated transcriptional response and correlates with a reduction in the characteristics associated with tumour aggressiveness, including invasion, migration, and oxygen consumption. The contribution of {KDM}4A to the regulation of {HIF}-1α is most robust in conditions of mild hypoxia. This suggests that {KDM}4A can enhance the function of {HIF}-1α by increasing the total available protein to counteract any residual activity of prolyl hydroxylases.},
	pages = {11094},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Dobrynin, Grzegorz and {McAllister}, Tom E. and Leszczynska, Katarzyna B. and Ramachandran, Shaliny and Krieg, Adam J. and Kawamura, Akane and Hammond, Ester M.},
	urldate = {2022-06-01},
	date = {2017-09-11},
	pmid = {28894274},
	pmcid = {PMC5593970},
	file = {Full Text:/home/ajl/Zotero/storage/2TDXQYUB/Dobrynin et al. - 2017 - KDM4A regulates HIF-1 levels through H3K9me3.pdf:application/pdf},
}

@article{bagnall_tight_2014,
	title = {Tight Control of Hypoxia-inducible Factor-α Transient Dynamics Is Essential for Cell Survival in Hypoxia},
	volume = {289},
	issn = {0021-9258, 1083-351X},
	url = {https://www.jbc.org/article/S0021-9258(20)44027-X/abstract},
	doi = {10.1074/jbc.M113.500405},
	abstract = {{\textless}p{\textgreater}Intracellular signaling involving hypoxia-inducible factor ({HIF}) controls the adaptive responses to hypoxia. There is a growing body of evidence demonstrating that intracellular signals encode temporal information. Thus, the dynamics of protein levels, as well as protein quantity and/or localization, impacts on cell fate. We hypothesized that such temporal encoding has a role in {HIF} signaling and cell fate decisions triggered by hypoxic conditions. Using live cell imaging in a controlled oxygen environment, we observed transient 3-h pulses of {HIF}-1α and -2α expression under continuous hypoxia. We postulated that the well described prolyl hydroxylase ({PHD}) oxygen sensors and {HIF} negative feedback regulators could be the origin of the pulsatile {HIF} dynamics. We used iterative mathematical modeling and experimental analysis to scrutinize which parameter of the {PHD} feedback could control {HIF} timing and we probed for the functional redundancy between the three main {PHD} proteins. We identified {PHD}2 as the main {PHD} responsible for {HIF} peak duration. We then demonstrated that this has important consequences, because the transient nature of the {HIF} pulse prevents cell death by avoiding transcription of p53-dependent pro-apoptotic genes. We have further shown the importance of considering {HIF} dynamics for coupling mathematical models by using a described {HIF}-p53 mathematical model. Our results indicate that the tight control of {HIF} transient dynamics has important functional consequences on the cross-talk with key signaling pathways controlling cell survival, which is likely to impact on {HIF} targeting strategies for hypoxia-associated diseases such as tumor progression and ischemia.{\textless}/p{\textgreater}},
	pages = {5549--5564},
	number = {9},
	journaltitle = {Journal of Biological Chemistry},
	shortjournal = {Journal of Biological Chemistry},
	author = {Bagnall, James and Leedale, Joseph and Taylor, Sarah E. and Spiller, David G. and White, Michael R. H. and Sharkey, Kieran J. and Bearon, Rachel N. and Sée, Violaine},
	urldate = {2022-06-01},
	date = {2014-02-28},
	pmid = {24394419},
	note = {Publisher: Elsevier},
	file = {Full Text PDF:/home/ajl/Zotero/storage/BJMYBLRY/Bagnall et al. - 2014 - Tight Control of Hypoxia-inducible Factor-α Transi.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/XIBZQJPS/fulltext.html:text/html},
}

@article{zhang_tiparp_2020,
	title = {{TiPARP} forms nuclear condensates to degrade {HIF}-1α and suppress tumorigenesis},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1921815117},
	doi = {10.1073/pnas.1921815117},
	pages = {13447--13456},
	number = {24},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Zhang, Lu and Cao, Ji and Dong, Longying and Lin, Hening},
	urldate = {2022-06-01},
	date = {2020-06-16},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:/home/ajl/Zotero/storage/QRPRUZ7Z/Zhang et al. - 2020 - TiPARP forms nuclear condensates to degrade HIF-1α.pdf:application/pdf},
}

@article{bagnall_tight_2014-1,
	title = {Tight Control of Hypoxia-inducible Factor-α Transient Dynamics Is Essential for Cell Survival in Hypoxia},
	volume = {289},
	issn = {0021-9258},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3937633/},
	doi = {10.1074/jbc.M113.500405},
	abstract = {Background: Hypoxia inducible factor-α ({HIF}-α) is the main transcription factor activated in low oxygen conditions., Results: Single cell imaging reveals pulses in nuclear levels of {HIF}-α., Conclusion: The transient nature of the {HIF}-α nuclear accumulation is required to avoid cell death., Significance: The duration of {HIF}-α response depends on cellular oxygenation, and can encode information and dictate cell fate., Intracellular signaling involving hypoxia-inducible factor ({HIF}) controls the adaptive responses to hypoxia. There is a growing body of evidence demonstrating that intracellular signals encode temporal information. Thus, the dynamics of protein levels, as well as protein quantity and/or localization, impacts on cell fate. We hypothesized that such temporal encoding has a role in {HIF} signaling and cell fate decisions triggered by hypoxic conditions. Using live cell imaging in a controlled oxygen environment, we observed transient 3-h pulses of {HIF}-1α and -2α expression under continuous hypoxia. We postulated that the well described prolyl hydroxylase ({PHD}) oxygen sensors and {HIF} negative feedback regulators could be the origin of the pulsatile {HIF} dynamics. We used iterative mathematical modeling and experimental analysis to scrutinize which parameter of the {PHD} feedback could control {HIF} timing and we probed for the functional redundancy between the three main {PHD} proteins. We identified {PHD}2 as the main {PHD} responsible for {HIF} peak duration. We then demonstrated that this has important consequences, because the transient nature of the {HIF} pulse prevents cell death by avoiding transcription of p53-dependent pro-apoptotic genes. We have further shown the importance of considering {HIF} dynamics for coupling mathematical models by using a described {HIF}-p53 mathematical model. Our results indicate that the tight control of {HIF} transient dynamics has important functional consequences on the cross-talk with key signaling pathways controlling cell survival, which is likely to impact on {HIF} targeting strategies for hypoxia-associated diseases such as tumor progression and ischemia.},
	pages = {5549--5564},
	number = {9},
	journaltitle = {The Journal of Biological Chemistry},
	shortjournal = {J Biol Chem},
	author = {Bagnall, James and Leedale, Joseph and Taylor, Sarah E. and Spiller, David G. and White, Michael R. H. and Sharkey, Kieran J. and Bearon, Rachel N. and Sée, Violaine},
	urldate = {2022-06-01},
	date = {2014-02-28},
	pmid = {24394419},
	pmcid = {PMC3937633},
	file = {Full Text:/home/ajl/Zotero/storage/N74WZ4SX/Bagnall et al. - 2014 - Tight Control of Hypoxia-inducible Factor-α Transi.pdf:application/pdf},
}

@article{nguyen_dynamic_2013,
	title = {A dynamic model of the hypoxia-inducible factor 1α ({HIF}-1α) network},
	volume = {126},
	issn = {0021-9533},
	url = {https://doi.org/10.1242/jcs.119974},
	doi = {10.1242/jcs.119974},
	abstract = {Activation of the hypoxia-inducible factor ({HIF}) pathway is a critical step in the transcriptional response to hypoxia. Although many of the key proteins involved have been characterised, the dynamics of their interactions in generating this response remain unclear. In the present study, we have generated a comprehensive mathematical model of the {HIF}-1α pathway based on core validated components and dynamic experimental data, and confirm the previously described connections within the predicted network topology. Our model confirms previous work demonstrating that the steps leading to optimal {HIF}-1α transcriptional activity require sequential inhibition of both prolyl- and asparaginyl-hydroxylases. We predict from our model (and confirm experimentally) that there is residual activity of the asparaginyl-hydroxylase {FIH} (factor inhibiting {HIF}) at low oxygen tension. Furthermore, silencing {FIH} under conditions where prolyl-hydroxylases are inhibited results in increased {HIF}-1α transcriptional activity, but paradoxically decreases {HIF}-1α stability. Using a core module of the {HIF} network and mathematical proof supported by experimental data, we propose that asparaginyl hydroxylation confers a degree of resistance upon {HIF}-1α to proteosomal degradation. Thus, through in vitro experimental data and in silico predictions, we provide a comprehensive model of the dynamic regulation of {HIF}-1α transcriptional activity by hydroxylases and use its predictive and adaptive properties to explain counter-intuitive biological observations.},
	pages = {1454--1463},
	number = {6},
	journaltitle = {Journal of Cell Science},
	shortjournal = {Journal of Cell Science},
	author = {Nguyen, Lan K. and Cavadas, Miguel A. S. and Scholz, Carsten C. and Fitzpatrick, Susan F. and Bruning, Ulrike and Cummins, Eoin P. and Tambuwala, Murtaza M. and Manresa, Mario C. and Kholodenko, Boris N. and Taylor, Cormac T. and Cheong, Alex},
	urldate = {2022-06-01},
	date = {2013-03-15},
	file = {Full Text PDF:/home/ajl/Zotero/storage/HNWZLX7Z/Nguyen et al. - 2013 - A dynamic model of the hypoxia-inducible factor 1α.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/YFT5JV9J/A-dynamic-model-of-the-hypoxia-inducible-factor-1.html:text/html},
}

@article{brezis_hypoxia_1995,
	title = {Hypoxia of the Renal Medulla — Its Implications for Disease},
	volume = {332},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/10.1056/NEJM199503093321006},
	doi = {10.1056/NEJM199503093321006},
	pages = {647--655},
	number = {10},
	journaltitle = {New England Journal of Medicine},
	shortjournal = {N Engl J Med},
	author = {Brezis, Mayer and Rosen, Seymour},
	urldate = {2022-06-01},
	date = {1995-03-09},
	note = {Publisher: Massachusetts Medical Society},
	file = {Full Text PDF:/home/ajl/Zotero/storage/ITNRSIJ3/Brezis and Rosen - 1995 - Hypoxia of the Renal Medulla — Its Implications fo.pdf:application/pdf},
}

@online{noauthor_state_nodate,
	title = {State and trait characteristics of anterior insula time-varying functional connectivity - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S105381191931016X},
	urldate = {2022-03-16},
	file = {State and trait characteristics of anterior insula time-varying functional connectivity - ScienceDirect:/home/ajl/Zotero/storage/DZH853TI/S105381191931016X.html:text/html},
}

@article{mantyh_tau_2020-2,
	title = {Tau Positron Emission Tomographic Findings in a Former {US} Football Player With Pathologically Confirmed Chronic Traumatic Encephalopathy},
	volume = {77},
	issn = {2168-6149},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6990867/},
	doi = {10.1001/jamaneurol.2019.4509},
	abstract = {This case report compares in vivo 18F-flortaucipir at positron emission tomography with regions of tau pathology found at postmortem analysis in a former {US} football player with pathologically confirmed chronic traumatic encephalopathy.},
	pages = {517--521},
	number = {4},
	journaltitle = {{JAMA} Neurology},
	shortjournal = {{JAMA} Neurol},
	author = {Mantyh, William G. and Spina, Salvatore and Lee, Alex and Iaccarino, Leonardo and Soleimani-Meigooni, David and Tsoy, Elena and Mellinger, Taylor J. and Grant, Harli and Vandevrede, Lawren and La Joie, Renaud and Lesman-Segev, Orit and Gaus, Stephanie and Possin, Katherine L. and Grinberg, Lea T. and Miller, Bruce L. and Seeley, William W. and Rabinovici, Gil D.},
	urldate = {2022-03-16},
	date = {2020-04},
	pmid = {31904765},
	pmcid = {PMC6990867},
}

@article{flagan_complement_2021-1,
	title = {Complement and {NfL} associations with brain structure and functional connectivity alterations in presymptomatic and symptomatic {GRN} mutation carriers},
	volume = {17},
	issn = {1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.050737},
	doi = {10.1002/alz.050737},
	abstract = {Background Progranulin ({GRN}) mutations cause autosomal dominant frontotemporal lobar degeneration. Clinical trials for {GRN}-targeted therapies are underway, yet reliable biomarkers to predict onset and track disease progression remain elusive. Task-free functional {MRI} (tf-{fMRI}) connectivity may be more sensitive than structural measures for detecting presymptomatic changes (Dopper, 2014; Premi, 2016). Presymptomatic {GRN} feature tf-{fMRI} hyperconnectivity that is more pronounced in older carriers presumably closer to symptom onset, suggesting hyperconnectivity may be a harbinger of disease (Lee, 2019). To aid the interpretation of hyperconnectivity, we explored relationships of structural and tf-{fMRI} measures with candidate fluid biomarkers for {GRN}: {CSF} complement proteins C3b and C1q, which drive neurodegeneration in {GRN} -/- mice (Lui, 2016) and plasma neurofilament light chain ({NfL}), a marker of axonal injury. Method We studied 20 symptomatic {GRN} (Sx), 39 presymptomatic {GRN} ({preSx}), and 67 healthy-controls ({HC}). Cross-sectional {CSF} C3b, C1q, and plasma {NfL} concentrations were compared between groups. Correlations controlling for age assessed relationships between fluid biomarkers and symptom severity ({CDR}®+{NACC}-{FTLD} box score). Voxelwise multiple regression models examined interactions between fluid biomarkers and gene status on grey matter probability maps ({GM}) and voxelwise tf-{fMRI} whole-brain weighted degree ({WBD}) maps. Result Complement concentrations were similar between Sx, {preSx}, and {HC}. Across all {GRN}, C3b concentrations increased with symptom severity (r=0.34, p=0.04). In Sx, but not {preSx}, higher C3b was associated with increased {WBD} in medial-frontal regions, left insula, and thalamus compared to {HC}. Sx had higher {NfL} concentrations compared to {preSx} and {HC}. Across all {GRN} and in Sx, higher {NfL} was associated with greater symptom severity (r=0.60, p{\textless}0.001; r=0.72, p{\textless}0.001). In Sx, higher {NfL} was associated with lower {GM} in precuneus and bilateral frontoinsular cortices, but not {WBD}, compared to {HC}. In {preSx}, higher {NfL} was associated with increased {WBD} in thalamus and right temporal regions, but not {GM}, compared to {HC}. Conclusion C3b and {NfL} concentrations correlated with symptom severity. C3b concentrations were associated with tf-{fMRI} connectivity alterations in Sx while {NfL} concentrations were associated with connectivity alterations in {preSx}. Longitudinal studies are needed to determine temporal relationships between altered connectivity, atrophy, complement, {NfL} rise, and symptom onset.},
	pages = {e050737},
	issue = {S4},
	journaltitle = {Alzheimer's \& Dementia},
	author = {Flagan, Taru M. and Chu, Stephanie A. and Häkkinen, Suvi and {McFall}, David and Heller, Carolin and Rohrer, Jonathan D and Brown, Jesse A. and Lee, Alex Jihun and Pasquini, Lorenzo and Mandelli, Maria Luisa and Gorno-Tempini, Marilu and Appleby, Brian and Dickerson, Brad C. and Domoto-Reilly, Kimiko and Foroud, Tatiana M. and Geschwind, Daniel H. and Ghoshal, Nupur and Graff-Radford, Neill R. and Grossman, Murray and Hsiung, Ging-Yuek Robin and Huang, Eric J. and Huey, Edward D. and Kantarci, Kejal and Karydas, Anna M. and Kaufer, Daniel and Knopman, David S. and Litvan, Irene and Mackenzie, Ian R and Mendez, Mario F. and Onyike, Chiadi U and Petrucelli, Leonard and Ramos, Eliana Marisa and Roberson, Erik D. and Rojas, Julio C. and Tartaglia, Maria Carmela and Toga, Arthur W. and Weintraub, Sandra and Forsberg, Leah K. and Heuer, Hilary W. and Boeve, Bradley F. and Boxer, Adam L. and Rosen, Howard J. and Miller, Bruce L. and Moreno, Fermin and Seeley, William W. and Lee, Suzee E. and Consortia, Artfl/Lefftds},
	urldate = {2022-03-16},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.050737},
	file = {Full Text PDF:/home/ajl/Zotero/storage/KPL6H22U/Flagan et al. - 2021 - Complement and NfL associations with brain structu.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/HEFTM6ZB/alz.html:text/html},
}

@article{brown_local_2021-1,
	title = {Local neurodegeneration and global connectivity adaptation across the {FTD}-{AD} spectrum},
	volume = {17},
	issn = {1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/alz.055308},
	doi = {10.1002/alz.055308},
	abstract = {Background Neurodegenerative diseases involve weakened functional connectivity in disease-targeted brain areas. Equally important but overlooked is the hyperconnectivity that appears in other brain areas (Hillary and Grafman, {TICS} 2017). Hyperconnectivity has been attributed to processes like disinhibition, imbalance, compensation, and reserve. It is critical to understand the neuroanatomical mechanism underlying hyperconnectivity because this functional process may accelerate subsequent disease progression. We performed structure-function mapping for patients across subtypes and stages of the frontotemporal dementia-Alzheimer’s disease atrophy spectrum. Our goal is to develop a comprehensive model relating diverse focal atrophy patterns to corresponding brain-wide functional connectivity reconfigurations. Method We studied patients with Alzheimer’s disease ({AD}), behavioral variant {FTD} ({bvFTD}), semantic and nonfluent variant primary progressive aphasia ({svPPA}/{nfvPPA}), cortical basal syndrome ({CBS}), and healthy controls ({HC}). We included subjects who received a clinical diagnosis at the {UCSF} Memory and Aging Center and had structural and task-free functional {MRI} scans (n=281). Each subject’s gray matter atrophy map was measured using voxel-based morphometry volume loss in 273 {ROIs}. Functional connectivity matrices (273x273) were derived for each subject. We combined data for all patients and controls into a structural data matrix (281x273) and a functional data matrix (281x37128). We then performed partial least squares regression to find components that maximized the covariance between structure and function. Result The first {PLS} component captured the relationship between global atrophy burden and a distributed pattern of functional connectivity loss in unimodal cortical areas and enhancement in subcortical-cortical pathways (r=0.64; Figure 1). The second {PLS} component showed {svPPA}-like anterior temporal atrophy corresponding to atrophy-proximal connectivity deficits, with enhancements in contralateral frontoparietal areas (r=0.67). The third {PLS} component revealed a spectrum from anterior ({bvFTD}) to posterior ({AD}), contrasting frontal-insular atrophy, connectivity deficits, and parietal connectivity enhancements versus parietal atrophy, connectivity deficits, and frontal connectivity enhancements (r=0.51). Conclusion Specific atrophy subtypes across the {FTD}-{AD} spectrum associate with proximal functional connectivity reductions. Intriguingly, these subtypes also exhibit concomitant functional connectivity enhancements in more distal areas. The enhancements are of the same magnitude as the deficits and may represent a general principle of functional “load-shifting” (Jones et al, Brain 2016) away from disease-targeted areas.},
	pages = {e055308},
	issue = {S6},
	journaltitle = {Alzheimer's \& Dementia},
	author = {Brown, Jesse A. and Lee, Alex Jihun and Pasquini, Lorenzo and Friedberg, Adit and Rabinovici, Gil D. and Kramer, Joel H and Tempini, Maria Luisa Gorno and Rosen, Howard J. and Miller, Bruce L. and Seeley, William W.},
	urldate = {2022-03-16},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/alz.055308},
	file = {Full Text PDF:/home/ajl/Zotero/storage/JLGVRDHD/Brown et al. - 2021 - Local neurodegeneration and global connectivity ad.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/8N5BJKFN/alz.html:text/html},
}

@article{rabe_efficacy_2018,
	title = {Efficacy and Safety of Dupilumab in Glucocorticoid-Dependent Severe Asthma},
	volume = {378},
	issn = {0028-4793},
	url = {https://doi.org/10.1056/NEJMoa1804093},
	doi = {10.1056/NEJMoa1804093},
	abstract = {Dupilumab for Decrease in Oral Glucocorticoids for Asthma Patients who used oral glucocorticoids for asthma were able to reduce the dose of treatment more successfully when dupilumab, a monoclonal antibody targeting signaling through the interleukin-4 and interleukin-13 receptor, was added to their regimen than when placebo was added.},
	pages = {2475--2485},
	number = {26},
	journaltitle = {New England Journal of Medicine},
	author = {Rabe, Klaus F. and Nair, Parameswaran and Brusselle, Guy and Maspero, Jorge F. and Castro, Mario and Sher, Lawrence and Zhu, Hongjie and Hamilton, Jennifer D. and Swanson, Brian N. and Khan, Asif and Chao, Jingdong and Staudinger, Heribert and Pirozzi, Gianluca and Antoni, Christian and Amin, Nikhil and Ruddy, Marcella and Akinlade, Bolanle and Graham, Neil M.H. and Stahl, Neil and Yancopoulos, George D. and Teper, Ariel},
	urldate = {2022-03-08},
	date = {2018-06-28},
	pmid = {29782224},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://doi.org/10.1056/{NEJMoa}1804093},
	file = {Full Text PDF:/home/ajl/Zotero/storage/HUXK8UDR/Rabe et al. - 2018 - Efficacy and Safety of Dupilumab in Glucocorticoid.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/3PCVR7V4/NEJMoa1804093.html:text/html},
}

@article{castro_dupilumab_2018,
	title = {Dupilumab Efficacy and Safety in Moderate-to-Severe Uncontrolled Asthma},
	volume = {378},
	issn = {1533-4406},
	doi = {10.1056/NEJMoa1804092},
	abstract = {{BACKGROUND}: Dupilumab is a fully human anti-interleukin-4 receptor α monoclonal antibody that blocks both interleukin-4 and interleukin-13 signaling. We assessed its efficacy and safety in patients with uncontrolled asthma.
{METHODS}: We randomly assigned 1902 patients 12 years of age or older with uncontrolled asthma in a 2:2:1:1 ratio to receive add-on subcutaneous dupilumab at a dose of 200 or 300 mg every 2 weeks or matched-volume placebos for 52 weeks. The primary end points were the annualized rate of severe asthma exacerbations and the absolute change from baseline to week 12 in the forced expiratory volume in 1 second ({FEV}1) before bronchodilator use in the overall trial population. Secondary end points included the exacerbation rate and {FEV}1 in patients with a blood eosinophil count of 300 or more per cubic millimeter. Asthma control and dupilumab safety were also assessed.
{RESULTS}: The annualized rate of severe asthma exacerbations was 0.46 (95\% confidence interval [{CI}], 0.39 to 0.53) among patients assigned to 200 mg of dupilumab every 2 weeks and 0.87 (95\% {CI}, 0.72 to 1.05) among those assigned to a matched placebo, for a 47.7\% lower rate with dupilumab than with placebo (P{\textless}0.001); similar results were seen with the dupilumab dose of 300 mg every 2 weeks. At week 12, the {FEV}1 had increased by 0.32 liters in patients assigned to the lower dose of dupilumab (difference vs. matched placebo, 0.14 liters; P{\textless}0.001); similar results were seen with the higher dose. Among patients with a blood eosinophil count of 300 or more per cubic millimeter, the annualized rate of severe asthma exacerbations was 0.37 (95\% {CI}, 0.29 to 0.48) among those receiving lower-dose dupilumab and 1.08 (95\% {CI}, 0.85 to 1.38) among those receiving a matched placebo (65.8\% lower rate with dupilumab than with placebo; 95\% {CI}, 52.0 to 75.6); similar results were observed with the higher dose. Blood eosinophilia occurred after the start of the intervention in 52 patients (4.1\%) who received dupilumab as compared with 4 patients (0.6\%) who received placebo.
{CONCLUSIONS}: In this trial, patients who received dupilumab had significantly lower rates of severe asthma exacerbation than those who received placebo, as well as better lung function and asthma control. Greater benefits were seen in patients with higher baseline levels of eosinophils. Hypereosinophilia was observed in some patients. (Funded by Sanofi and Regeneron Pharmaceuticals; {LIBERTY} {ASTHMA} {QUEST} {ClinicalTrials}.gov number, {NCT}02414854 .).},
	pages = {2486--2496},
	number = {26},
	journaltitle = {The New England Journal of Medicine},
	shortjournal = {N Engl J Med},
	author = {Castro, Mario and Corren, Jonathan and Pavord, Ian D. and Maspero, Jorge and Wenzel, Sally and Rabe, Klaus F. and Busse, William W. and Ford, Linda and Sher, Lawrence and {FitzGerald}, J. Mark and Katelaris, Constance and Tohda, Yuji and Zhang, Bingzhi and Staudinger, Heribert and Pirozzi, Gianluca and Amin, Nikhil and Ruddy, Marcella and Akinlade, Bolanle and Khan, Asif and Chao, Jingdong and Martincova, Renata and Graham, Neil M. H. and Hamilton, Jennifer D. and Swanson, Brian N. and Stahl, Neil and Yancopoulos, George D. and Teper, Ariel},
	date = {2018-06-28},
	pmid = {29782217},
	keywords = {Adult, Female, Humans, Male, Middle Aged, Adolescent, Anti-Asthmatic Agents, Antibodies, Monoclonal, Antibodies, Monoclonal, Humanized, Asthma, Bronchodilator Agents, Child, Double-Blind Method, Drug Therapy, Combination, Eosinophilia, Forced Expiratory Volume, Injections, Subcutaneous, Intention to Treat Analysis, Interleukin-13, Receptors, Interleukin-4, Young Adult},
	file = {Full Text:/home/ajl/Zotero/storage/EYB762MY/Castro et al. - 2018 - Dupilumab Efficacy and Safety in Moderate-to-Sever.pdf:application/pdf},
}

@article{castro_dupilumab_2020,
	title = {Dupilumab improves lung function in patients with uncontrolled, moderate-to-severe asthma},
	volume = {6},
	rights = {Copyright ©{ERS} 2020. http://creativecommons.org/licenses/by-nc/4.0/This article is open access and distributed under the terms of the Creative Commons Attribution Non-Commercial Licence 4.0.},
	issn = {2312-0541},
	url = {https://openres.ersjournals.com/content/6/1/00204-2019},
	doi = {10.1183/23120541.00204-2019},
	abstract = {Background Dupilumab, a fully human monoclonal antibody, blocks the shared receptor component for interleukin-4 and interleukin-13, key drivers of type 2 inflammation. In the phase 3 {LIBERTY} {ASTHMA} {QUEST} trial ({NCT}02414854) in patients with uncontrolled, moderate-to-severe asthma, add-on dupilumab 200 mg or 300 mg every 2 weeks reduced exacerbations and improved forced expiratory volume in 1 s ({FEV}1) and quality of life over 52 weeks. This analysis evaluates dupilimab's effect on lung function in the overall population, and subgroups with baseline elevated type 2 inflammatory biomarkers.
Methods Patients were randomised to 52 weeks of subcutaneous dupilumab 200 mg every 2 weeks, 300 mg every 2 weeks, or matched-volume placebos. Lung function outcomes were analysed in the overall population, in patients with ≥150 eosinophils·µL−1, ≥300 eosinophils·µL−1, ≥25 ppb fractional exhaled nitric oxide ({FeNO}), and both ≥150 eosinophils·µL−1 and ≥25 ppb {FeNO}, at baseline.
Results Dupilumab treatment (200 mg and 300 mg every 2 weeks) resulted in significant improvements versus placebo after 52 weeks in pre-bronchodilator {FEV}1 (0.20 and 0.13 L, respectively, versus placebo) and post-bronchodilator {FEV}1 (0.19 and 0.13 L, respectively), forced vital capacity ({FVC}) (0.20 and 0.14 L, respectively), forced expiratory flow (0.19 and 0.13 L·s−1, respectively) and pre-bronchodilator {FEV}1/{FVC} ratio (1.75\% and 1.61\%, respectively) in the overall population (p{\textless}0.001). Difference versus placebo in post-bronchodilator {FEV}1 slope of change (weeks 4–52) was significant (0.04 L·year−1; p{\textless}0.05). Greater improvements were achieved in patients with elevated baseline blood eosinophil and/or {FeNO} levels for most outcomes.
Conclusions Dupilumab improves lung function outcomes, including large and small airway measurements and fixed airway obstruction, in patients with uncontrolled, moderate-to-severe asthma; particularly in patients with elevated biomarkers of type 2 inflammation.
Tweetable abstract @{ERSpublications}
click to {tweetDupilumab} is a fully human monoclonal antibody that blocks the shared receptor component for interleukin-4 and interleukin-13, key drivers of type 2 inflammation, improving lung function outcomes in patients with uncontrolled, moderate-to-severe asthma http://bit.ly/2OhKMpi},
	number = {1},
	journaltitle = {{ERJ} Open Research},
	author = {Castro, Mario and Rabe, Klaus F. and Corren, Jonathan and Pavord, Ian D. and Katelaris, Constance H. and Tohda, Yuji and Zhang, Bingzhi and Rice, Megan S. and Maroni, Jaman and Rowe, Paul and Pirozzi, Gianluca and Amin, Nikhil and Ruddy, Marcella and Akinlade, Bolanle and Graham, Neil M. H. and Teper, Ariel},
	urldate = {2022-03-08},
	date = {2020-01-01},
	langid = {english},
	note = {Publisher: European Respiratory Society
Section: Original articles},
	file = {Full Text PDF:/home/ajl/Zotero/storage/YU8LF2HA/Castro et al. - 2020 - Dupilumab improves lung function in patients with .pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/38VIQ7GV/00204-2019.html:text/html},
}

@article{vatrella_dupilumab_2014,
	title = {Dupilumab: a novel treatment for asthma},
	volume = {7},
	issn = {1178-6965},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4159398/},
	doi = {10.2147/JAA.S52387},
	shorttitle = {Dupilumab},
	abstract = {Simultaneously with the steady progress towards a better knowledge of the pathobiology of asthma, the potential usefulness of anticytokine therapies is emerging as one of the key concepts in the newly developing treatments of this widespread airway disease. In particular, given the key role played by interleukin ({IL})-4 and {IL}-13 in the pathophysiology of the most typical aspects of asthma, such as chronic airway inflammation, tissue remodeling, and bronchial hyperresponsiveness, these pleiotropic cytokines are now considered as suitable therapeutic targets. Among the recently developed antiasthma biologic drugs, the monoclonal antibody dupilumab is very promising because of its ability to inhibit the biological effects of both {IL}-4 and {IL}-13. Indeed, dupilumab prevents {IL}-4/13 interactions with the α-subunit of the {IL}-4 receptor complex. A recent trial showed that in patients with difficult-to-control asthma, dupilumab can markedly decrease asthma exacerbations and improve respiratory symptoms and lung function; these effects were paralleled by significant reductions in T-helper 2-associated inflammatory biomarkers. However, further larger and longer trials are required to extend and validate these preliminary results, and also to carefully study the safety and tolerability profile of dupilumab.},
	pages = {123--130},
	journaltitle = {Journal of Asthma and Allergy},
	shortjournal = {J Asthma Allergy},
	author = {Vatrella, Alessandro and Fabozzi, Immacolata and Calabrese, Cecilia and Maselli, Rosario and Pelaia, Girolamo},
	urldate = {2022-03-08},
	date = {2014-09-04},
	pmid = {25214796},
	pmcid = {PMC4159398},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/VF5CD6DK/Vatrella et al. - 2014 - Dupilumab a novel treatment for asthma.pdf:application/pdf},
}

@online{noauthor_pspg219-dupilumab_nodate,
	title = {pspg219-dupilumab},
	url = {https://docs.google.com/document/d/14tKRGAsSn7CN6qvPr3ah1_zc3I5l1seHNKQqSaN-NN0/edit?usp=embed_facebook},
	abstract = {{PSPG}219 final project Alex Lee Dupilumab, an antibody therapeutic for asthma  Abstract  	Asthma is a heterogeneous, chronic autoimmune disease that has high prevalence across the {US} and significantly disrupts quality of life. Patients with asthma are at increased risk for respiratory illness and ...},
	titleaddon = {Google Docs},
	urldate = {2022-03-08},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/UMVTXTU2/edit.html:text/html},
}

@article{corren_role_2013,
	title = {Role of Interleukin-13 in Asthma},
	volume = {13},
	issn = {1534-6315},
	url = {https://doi.org/10.1007/s11882-013-0373-9},
	doi = {10.1007/s11882-013-0373-9},
	abstract = {Interleukin-13 is a pleiotropic {TH}2 cytokine that has been shown to be central to the pathogenesis of asthma. Some of the most prominent of the effects of {IL}-13 include increases in goblet cell differentiation, activation of fibroblasts, elevation of bronchial hyperresponsiveness, and switching of B cell antibody production from {IgM} to {IgE}. The relevances of these effects to asthma have been carefully studied in both animal models and more recently in human studies. As the role of {IL}-13 in asthma has become more defined, a number of potential biomarkers for {TH}2 airway inflammation, and hence {IL}-13 activity, have been identified, including blood and sputum eosinophils, total serum {IgE}, proteins derived from the bronchial epithelium (e.g., serum periostin), and exhaled nitric oxide. Most importantly, many of these markers for {TH}2 inflammation are strong predictors for positive responses to inhaled corticosteroid treatment. These biomarkers may also be useful in identifying patients who are most likely to benefit from specific {IL}-13 antagonism, as was demonstrated in a recent clinical trial of anti-{IL}-13 antibody therapy (lebrikizumab) in patients with poorly controlled asthma despite using inhaled corticosteroids. In that study, significant improvements in {FEV}1 were observed in patients with elevations of serum periostin but not in patients with normal periostin levels. These data indicate that {IL}-13 antagonists may fulfill an important unmet need in patients with poorly controlled asthma and biologic evidence of persistent {IL}-13 activity.},
	pages = {415--420},
	number = {5},
	journaltitle = {Current Allergy and Asthma Reports},
	shortjournal = {Curr Allergy Asthma Rep},
	author = {Corren, Jonathan},
	urldate = {2022-03-08},
	date = {2013-10-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/ajl/Zotero/storage/PINLQ663/Corren - 2013 - Role of Interleukin-13 in Asthma.pdf:application/pdf},
}

@article{steinke_th2_2001,
	title = {Th2 cytokines and asthma — Interleukin-4: its role in the pathogenesis of asthma, and targeting it for asthma treatment with interleukin-4 receptor antagonists},
	volume = {2},
	issn = {1465-9921},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC59570/},
	doi = {10.1186/rr40},
	shorttitle = {Th2 cytokines and asthma — Interleukin-4},
	abstract = {Interleukin-4 ({IL}-4) mediates important pro-inflammatory functions in asthma including induction of the {IgE} isotype switch, expression of vascular cell adhesion molecule-1 ({VCAM}-1), promotion of eosinophil transmigration across endothelium, mucus secretion, and differentiation of T helper type 2 lymphocytes leading to cytokine release. Asthma is a complex genetic disorder that has been linked to polymorphisms in the {IL}-4 gene promoter and proteins involved in {IL}-4 signaling. Soluble recombinant {IL}-4 receptor lacks transmembrane and cytoplasmic activating domains and can therefore sequester {IL}-4 without mediating cellular activation. We report the results of initial clinical trials, which demonstrate clinical efficacy of this naturally occurring {IL}-4 antagonist as a therapeutic agent in asthma.},
	pages = {66--70},
	number = {2},
	journaltitle = {Respiratory Research},
	shortjournal = {Respir Res},
	author = {Steinke, John W and Borish, Larry},
	urldate = {2022-03-08},
	date = {2001},
	pmid = {11686867},
	pmcid = {PMC59570},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/P4DDBEWG/Steinke and Borish - 2001 - Th2 cytokines and asthma — Interleukin-4 its role.pdf:application/pdf},
}

@online{noauthor_role_nodate,
	title = {Role of Interleukin-13 in Asthma {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s11882-013-0373-9},
	urldate = {2022-03-08},
	file = {Role of Interleukin-13 in Asthma | SpringerLink:/home/ajl/Zotero/storage/BETUHF6T/s11882-013-0373-9.html:text/html},
}

@article{nijkamp_nitric_1993,
	title = {Nitric oxide synthesis inhibitors induce airway hyperresponsiveness in the guinea pig in vivo and in vitro. Role of the epithelium},
	volume = {148},
	issn = {0003-0805},
	doi = {10.1164/ajrccm/148.3.727},
	abstract = {The administration by aerosol of the nitric oxide ({NO}) synthesis inhibitors, N omega-nitro-L-arginine methyl ester (L-{NAME}) or Ng-monomethyl-L-arginine (L-{NMMA}), to spontaneously breathing anesthetized guinea pigs resulted in a significant enhancement of lung resistance ({RL}) after increasing intravenous doses of histamine. The maximal response was increased (p {\textless} 0.01) by 126\% (L-{NAME}) and 282\% (L-{NMMA}) compared with the control groups. This effect was inhibited by giving an aerosol of the {NO} precursor L-arginine (L-Arg) but not by its inactive enantiomer D-arginine (D-Arg). Perfusion through the lumen of guinea pig tracheal tubes in vitro with nitric oxide synthesis inhibitors (120 {microM}) resulted in a significant increase in basal tone, suggesting a role for {NO} in the maintenance of basal tone. In addition, the histamine concentration-response curve was significantly shifted upward: the maximal response was increased (p {\textless} 0.01) by 335\% (L-{NAME}) and 250\% (L-{NMMA}) compared with the control group. This effect was concentration dependently inhibited by coincubation with L-Arg (120, 200, and 400 {microM}), but not with D-Arg (200 {microM}). Furthermore, removal of the epithelium resulted in an upward shift in the histamine concentration-response curve: the maximal response was increased by 185\%. However, incubation with L-{NAME} did not further increase tracheal responsiveness to histamine, but addition of L-Arg (360 {microM}), when a plateau was reached, relaxed the tissues to control values. Nitric oxide synthesis inhibition did not change the responsiveness of intact tissues in vitro after intraluminal stimulation with leukotriene D4, serotonin, or the cholinergic agonist arecoline.({ABSTRACT} {TRUNCATED} {AT} 250 {WORDS})},
	pages = {727--734},
	number = {3},
	journaltitle = {The American Review of Respiratory Disease},
	shortjournal = {Am Rev Respir Dis},
	author = {Nijkamp, F. P. and van der Linde, H. J. and Folkerts, G.},
	date = {1993-09},
	pmid = {8368646},
	keywords = {Male, Animals, Analysis of Variance, Arginine, Bronchi, Bronchial Hyperreactivity, Dose-Response Relationship, Drug, Drug Interactions, Epithelium, Guinea Pigs, Histamine, In Vitro Techniques, Muscle, Smooth, {NG}-Nitroarginine Methyl Ester, Nitric Oxide, omega-N-Methylarginine},
}

@article{taylor_allergen-induced_1998,
	title = {Allergen-induced early and late asthmatic responses are not affected by inhibition of endogenous nitric oxide},
	volume = {158},
	issn = {1073-449X},
	doi = {10.1164/ajrccm.158.1.9709091},
	abstract = {Endogenous exhaled nitric oxide ({NO}) is increased during the late response to inhaled allergen in patients with asthma and may be bronchoprotective in asthma or have a deleterious effect when generated in excess under inflammatory conditions. To investigate this, we evaluated the effect of inhibiting endogenous {NO} production with nebulized {NG}-nitro-L-arginine methyl ester (L-{NAME}), a nonselective {NO} synthase ({NOS}) inhibitor, on early and late asthmatic responses to inhaled allergen in patients with mild allergic asthma. After a screening allergen challenge ({AC}), 22 male patients attended two visits conducted in a double-blind, randomized, placebo-controlled, crossover manner. Twelve patients demonstrating an early asthmatic response only (single responders) inhaled either L-{NAME} 170 mg or 0.9\% saline 20 min before {AC}, with exhaled {NO} and {FEV}1 measured for 3 h. Ten patients demonstrating both early and late asthmatic responses (dual responders) were studied in a similar fashion but inhaled two further doses of L-{NAME} or placebo 3.5 and 7 h after the initial dose, with exhaled {NO} and {FEV}1 measured for 10 h. L-{NAME} reduced exhaled {NO} levels by 77 +/- 5\% (p {\textless} 0.01) and 71 +/- 7\% (p {\textless} 0.01) in single and dual responders, respectively, but had no significant effect on early or late asthmatic responses. Following {AC} in single responders, the mean (+/- {SEM}) maximum fall in {FEV}1 after L-{NAME} and saline was 21.2 +/- 2.9\% and 23.8 +/- 3.0\%, respectively, and in dual responders, 31.2 +/- 4.5\% and 31.8 +/- 5. 8\% during the early asthmatic responses, and 27.4 +/- 3.9\% and 30.6 +/- 4.5\% during the late asthmatic responses, respectively. Area under the curve ({AUC}) did not significantly differ. {AUC}0-2 h in single responders after L-{NAME} and saline was 20.2 +/- 3.9 and 24.9 +/- 4.4 Delta\% {FEV}1/h, and in dual responders, 37.6 +/- 8.4 and 36.7 +/- 8.4 Delta\% {FEV}1/h, respectively, and 106.2 +/- 18.9 and 117.1 +/- 22.4 Delta\% {FEV}1/h, respectively, for the {AUC}4-10 h. This study suggests that in mild allergic asthma, endogenous {NO} neither protects against nor contributes to the processes underlying airway responses to inhaled allergen.},
	pages = {99--106},
	number = {1},
	journaltitle = {American Journal of Respiratory and Critical Care Medicine},
	shortjournal = {Am J Respir Crit Care Med},
	author = {Taylor, D. A. and {McGrath}, J. L. and O'Connor, B. J. and Barnes, P. J.},
	date = {1998-07},
	pmid = {9655713},
	keywords = {Adult, Humans, Male, Middle Aged, Time Factors, Asthma, Double-Blind Method, Bronchi, {NG}-Nitroarginine Methyl Ester, Nitric Oxide, Bronchial Provocation Tests, Bronchoconstriction, Cross-Over Studies, Enzyme Inhibitors},
	file = {Submitted Version:/home/ajl/Zotero/storage/9EVUK4LI/Taylor et al. - 1998 - Allergen-induced early and late asthmatic response.pdf:application/pdf},
}

@article{prado_nitric_2011,
	title = {Nitric Oxide in Asthma Physiopathology},
	volume = {2011},
	issn = {2090-5521},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3658695/},
	doi = {10.5402/2011/832560},
	abstract = {Asthma is a chronic inflammatory airway disease characterized by allergen-induced airway hyperresponsiveness, airway inflammation, and remodeling. Nitric oxide ({NO}) derived from constitutive and inducible enzymes affects many aspects of asthma physiopathology. Animal in vivo studies have indicated that inhibition of {iNOS} may play a central role in the modulation of these features, particularly extracellular matrix remodeling. Additionally, increases in {iNOS}-derived {NO}, observed in asthmatic patients, may lead to an increase in peroxynitrite and an imbalance of oxidant and antioxidant pathways. In addition, endogenous nitric oxide produced by constitutive enzymes may protect against the remodeling of the lung. Therefore, nitric oxide donors and/or {iNOS} inhibitors may have therapeutic potential in asthma treatment and can also be used with corticosteroids to counteract airway remodeling. This paper focuses on the pathophysiological role of nitric oxide, mainly derived from inducible isoforms, in the various pathologic mechanisms of allergic asthma and the importance of nitric oxide and/or arginase inhibitors in asthma treatment.},
	pages = {832560},
	journaltitle = {{ISRN} Allergy},
	shortjournal = {{ISRN} Allergy},
	author = {Prado, Carla M. and Martins, Mílton A. and Tibério, Iolanda F. L. C.},
	urldate = {2022-03-08},
	date = {2011-04-19},
	pmid = {23724233},
	pmcid = {PMC3658695},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/T3ZKWFKA/Prado et al. - 2011 - Nitric Oxide in Asthma Physiopathology.pdf:application/pdf},
}

@online{noauthor_regeneron_nodate,
	title = {Regeneron Technology: R\&D Excellence in the Biotech Industry},
	url = {https://www.regeneron.com/science/technology},
	shorttitle = {Regeneron Technology},
	abstract = {Learn about the many Regeneron-invented technologies accelerating drug discovery and development such as {VelociGene}®, {VelociMouse}®, {VelocImmune}® and more.},
	urldate = {2022-03-08},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/4C56YE55/technology.html:text/html},
}

@online{noauthor_astrazeneca_nodate,
	title = {{AstraZeneca} Licenses Regeneron's {VelocImmune}® Technology for Discovering Human Monoclonal Antibodies {\textbar} Regeneron Pharmaceuticals Inc.},
	url = {https://investor.regeneron.com/news-releases/news-release-details/astrazeneca-licenses-regenerons-velocimmuner-technology/},
	urldate = {2022-03-08},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/RQUW3ZN8/astrazeneca-licenses-regenerons-velocimmuner-technology.html:text/html},
}

@article{scott_mice_2007,
	title = {Mice with a human touch},
	volume = {25},
	issn = {1087-0156},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7096984/},
	doi = {10.1038/nbt1007-1075},
	abstract = {Years of tinkering with the mouse immune system genes has finally produced a winner, with the approval of the first fully human monoclonal antibody made in a mouse. A real breakthrough or an incremental improvement? Christopher Thomas Scott investigates.},
	pages = {1075--1077},
	number = {10},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotechnol},
	author = {Scott, Christopher Thomas},
	urldate = {2022-03-08},
	date = {2007},
	pmid = {17921981},
	pmcid = {PMC7096984},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/RPF8KX2C/Scott - 2007 - Mice with a human touch.pdf:application/pdf},
}

@article{harding_immunogenicity_2010,
	title = {The immunogenicity of humanized and fully human antibodies},
	volume = {2},
	issn = {1942-0862},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881252/},
	abstract = {Monoclonal antibodies represent an attractive therapeutic tool as they are highly specific for their targets, convey effector functions and enjoy robust manufacturing procedures. Humanization of murine monoclonal antibodies has vastly improved their in vivo tolerability. Humanization, the replacement of mouse constant regions and V framework regions for human sequences, results in a significantly less immunogenic product. However, some humanized and even fully human sequence-derived antibody molecules still carry immunological risk. to more fully understand the immunologic potential of humanized and human antibodies, we analyzed {CD}4+ helper T cell epitopes in a set of eight humanized antibodies. the antibodies studied represented a number of different {VH} and {VL} family members carrying unique {CDR} regions. In spite of these differences, {CD}4+ T cell epitopes were found only in {CDR}-sequence containing regions. We were able to incorporate up to two amino acid modifications in a single epitope that reduced the immunogenic potential while retaining full biologic function. We propose that immunogenicity will always be present in some antibody molecules due to the nature of the antigen-specific combining sites. A consequence of this result is modifications to reduce immunogenicity will be centered on the affinity-determining regions. Modifications to {CDR} regions can be designed that reduce the immunogenic potential while maintaining the bioactivity of the antibody molecule.},
	pages = {256--265},
	number = {3},
	journaltitle = {{mAbs}},
	shortjournal = {{MAbs}},
	author = {Harding, Fiona A and Stickler, Marcia M and Razo, Jennifer and {DuBridge}, Robert B},
	urldate = {2022-03-08},
	date = {2010},
	pmid = {20400861},
	pmcid = {PMC2881252},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/GKXGMAWY/Harding et al. - 2010 - The immunogenicity of humanized and fully human an.pdf:application/pdf},
}

@online{noauthor_immunogenicity_nodate,
	title = {Immunogenicity of engineered antibodies - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S1046202305000113?via%3Dihub},
	urldate = {2022-03-08},
	file = {Immunogenicity of engineered antibodies - ScienceDirect:/home/ajl/Zotero/storage/JRRNEXYV/S1046202305000113.html:text/html},
}

@article{sgro_side-effects_1995,
	title = {Side-effects of a monoclonal antibody, muromonab {CD}3/orthoclone {OKT}3: bibliographic review},
	volume = {105},
	issn = {0300-483X},
	doi = {10.1016/0300-483x(95)03123-w},
	shorttitle = {Side-effects of a monoclonal antibody, muromonab {CD}3/orthoclone {OKT}3},
	abstract = {Orthoclone {OKT}3 is a very powerful immunosuppressive drug marketed by Laboratory Cilag and the first monoclonal murine antibody to become available for therapy in humans. It is indicated in acute allograft rejection treatment and its side-effects are strongly linked with its mechanism of action, {ORT}/{OKT}3 is an Ig2a immunoglobulin produced by hybridoma technique that recognizes, binds and blocks the {CD}3 complex of the T-cell receptor. Two types of side-effects may occur that are either the consequences of overimmunosuppression or activation of immune system, since {ORT}/{OKT}3 behaves as a foreign antigen. This report is a bibliographic review of the suspected side-effects of this product.},
	pages = {23--29},
	number = {1},
	journaltitle = {Toxicology},
	shortjournal = {Toxicology},
	author = {Sgro, C.},
	date = {1995-12-20},
	pmid = {8638282},
	keywords = {Humans, Animals, Binding, Competitive, Blood Coagulation Disorders, Communicable Diseases, Cytokines, Graft Rejection, Immune System, Immunosuppressive Agents, Kidney Transplantation, Muromonab-{CD}3, Neoplasms, Receptors, Antigen, T-Cell, T-Lymphocytes},
}

@article{tsurushita_design_2005,
	title = {Design of humanized antibodies: from anti-Tac to Zenapax},
	volume = {36},
	issn = {1046-2023},
	doi = {10.1016/j.ymeth.2005.01.007},
	shorttitle = {Design of humanized antibodies},
	abstract = {Since the introduction of hybridoma technology, monoclonal antibodies have become one of the most important tools in the biosciences, finding diverse applications including their use in the therapy of human disease. Initial attempts to use monoclonal antibodies as therapeutics were hampered, however, by the potent immunogenicity of mouse (and other rodent) antibodies in humans. Humanization technology has made it possible to remove the immunogenicity associated with the use of rodent antibodies, or at least to reduce it to an acceptable level for clinical use in humans, thus facilitating the application of monoclonal antibodies to the treatment of human disease. To date, nine humanized monoclonal antibodies have been approved for use as human therapeutics in the United States. In this paper, we describe procedures for antibody humanization with an emphasis on strategies for designing humanized antibodies with the aid of computer-guided modeling of antibody variable domains, using as an example the humanized anti-{CD}25 monoclonal antibody, Zenapax.},
	pages = {69--83},
	number = {1},
	journaltitle = {Methods (San Diego, Calif.)},
	shortjournal = {Methods},
	author = {Tsurushita, Naoya and Hinton, Paul R. and Kumar, Shankar},
	date = {2005-05},
	pmid = {15848076},
	keywords = {Humans, Animals, Antibodies, Monoclonal, Antibodies, Monoclonal, Humanized, Amino Acid Sequence, Base Sequence, Daclizumab, Immunoglobulin G, Immunoglobulin Variable Region, Mice, Molecular Sequence Data, Protein Engineering, Protein Structure, Tertiary, Sequence Alignment},
}

@online{noauthor_precise_nodate,
	title = {Precise and in situ genetic humanization of 6 Mb of mouse immunoglobulin genes},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1323896111},
	abstract = {This paper describes a major advance in genomic engineering, describing by far the
largest genetic humanization of the mouse ever attempted. Six megabases of mouse immune
genes were replaced in a precise manner and “in situ” (in the ...Genetic humanization, which involves replacing mouse genes with their human counterparts,
can create powerful animal models for the study of human genes and diseases. One important
example of genetic humanization involves mice humanized for their Ig genes,...},
	titleaddon = {{PNAS}},
	urldate = {2022-03-04},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/53JQE9U8/pnas.html:text/html},
}

@article{bleecker_systematic_2020,
	title = {Systematic Literature Review of Systemic Corticosteroid Use for Asthma Management},
	volume = {201},
	issn = {1073-449X},
	url = {https://www.atsjournals.org/doi/10.1164/rccm.201904-0903SO},
	doi = {10.1164/rccm.201904-0903SO},
	abstract = {Systemic corticosteroid use to manage uncontrolled asthma and its associated healthcare burden may account for important health-related adverse effects. We conducted a systematic literature review to investigate the real-world extent and burden of systemic corticosteroid use in asthma. We searched {MEDLINE} and Embase databases to identify English-language articles published in 2010–2017, using search terms for asthma with keywords for oral corticosteroids and systemic corticosteroids. Observational studies, prescription database analyses, economic analyses, and surveys on oral/systemic corticosteroid use in children ({\textgreater}5 yr old), adolescents (12–17 yr old), and adults with asthma were included. We identified and reviewed 387 full-text articles, and our review included data from 139 studies. The included studies were conducted in Europe, North America, and Asia. Overall, oral/systemic corticosteroids were commonly used for asthma management and were more frequently used in patients with severe asthma than in those with milder disease. Long-term oral/systemic corticosteroid use was, in general, less frequent than short-term use. Compared with no use, long-term and repeated short-term oral/systemic corticosteroid use were associated with an increased risk of acute and chronic adverse events, even when doses were comparatively low. Greater oral/systemic corticosteroid exposure was also associated with increased costs and healthcare resource use. This review provides a comprehensive overview of oral/systemic corticosteroid use and associated adverse events for patients with all degrees of asthma severity and exposure duration. We report that oral/systemic corticosteroid use is prevalent in asthma management, and the risks of acute and chronic complications increase with the cumulative oral corticosteroid dosage.},
	pages = {276--293},
	number = {3},
	journaltitle = {American Journal of Respiratory and Critical Care Medicine},
	shortjournal = {Am J Respir Crit Care Med},
	author = {Bleecker, Eugene R. and Menzies-Gow, Andrew N. and Price, David B. and Bourdin, Arnaud and Sweet, Stephen and Martin, Amber L. and Alacqua, Marianna and Tran, Trung N.},
	urldate = {2022-03-04},
	date = {2020-02},
	note = {Publisher: American Thoracic Society - {AJRCCM}},
	keywords = {asthma, oral corticosteroids, severe asthma, systematic literature review, systemic corticosteroids},
	file = {Full Text PDF:/home/ajl/Zotero/storage/Y27VYBEY/Bleecker et al. - 2020 - Systematic Literature Review of Systemic Corticost.pdf:application/pdf},
}

@online{noauthor_efficacy_nodate,
	title = {Efficacy and safety of benralizumab for patients with severe asthma uncontrolled with high-dosage inhaled corticosteroids and long-acting β2-agonists ({SIROCCO}): a randomised, multicentre, placebo-controlled phase 3 trial - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0140673616313241?via%3Dihub},
	urldate = {2022-03-04},
	file = {Efficacy and safety of benralizumab for patients with severe asthma uncontrolled with high-dosage inhaled corticosteroids and long-acting β2-agonists (SIROCCO)\: a randomised, multicentre, placebo-controlled phase 3 trial - ScienceDirect:/home/ajl/Zotero/storage/I3KHPHAA/S0140673616313241.html:text/html},
}

@article{pavord_mepolizumab_2012,
	title = {Mepolizumab for severe eosinophilic asthma ({DREAM}): a multicentre, double-blind, placebo-controlled trial},
	volume = {380},
	issn = {0140-6736},
	url = {https://www.sciencedirect.com/science/article/pii/S014067361260988X},
	doi = {10.1016/S0140-6736(12)60988-X},
	shorttitle = {Mepolizumab for severe eosinophilic asthma ({DREAM})},
	abstract = {Background
Some patients with severe asthma have recurrent asthma exacerbations associated with eosinophilic airway inflammation. Early studies suggest that inhibition of eosinophilic airway inflammation with mepolizumab—a monoclonal antibody against interleukin 5—is associated with a reduced risk of exacerbations. We aimed to establish efficacy, safety, and patient characteristics associated with the response to mepolizumab.
Methods
We undertook a multicentre, double-blind, placebo-controlled trial at 81 centres in 13 countries between Nov 9, 2009, and Dec 5, 2011. Eligible patients were aged 12–74 years, had a history of recurrent severe asthma exacerbations, and had signs of eosinophilic inflammation. They were randomly assigned (in a 1:1:1:1 ratio) to receive one of three doses of intravenous mepolizumab (75 mg, 250 mg, or 750 mg) or matched placebo (100 {mL} 0·9\% {NaCl}) with a central telephone-based system and computer-generated randomly permuted block schedule stratified by whether treatment with oral corticosteroids was required. Patients received 13 infusions at 4-week intervals. The primary outcome was the rate of clinically significant asthma exacerbations, which were defined as validated episodes of acute asthma requiring treatment with oral corticosteroids, admission, or a visit to an emergency department. Patients, clinicians, and data analysts were masked to treatment assignment. Analyses were by intention to treat. This trial is registered with {ClinicalTrials}.gov, number {NCT}01000506.
Findings
621 patients were randomised: 159 were assigned to placebo, 154 to 75 mg mepolizumab, 152 to 250 mg mepolizumab, and 156 to 750 mg mepolizumab. 776 exacerbations were deemed to be clinically significant. The rate of clinically significant exacerbations was 2·40 per patient per year in the placebo group, 1·24 in the 75 mg mepolizumab group (48\% reduction, 95\% {CI} 31–61\%; p{\textless}0·0001), 1·46 in the 250 mg mepolizumab group (39\% reduction, 19–54\%; p=0·0005), and 1·15 in the 750 mg mepolizumab group (52\% reduction, 36–64\%; p{\textless}0·0001). Three patients died during the study, but the deaths were not deemed to be related to treatment.
Interpretation
Mepolizumab is an effective and well tolerated treatment that reduces the risk of asthma exacerbations in patients with severe eosinophilic asthma.
Funding
{GlaxoSmithKline}.},
	pages = {651--659},
	number = {9842},
	journaltitle = {The Lancet},
	shortjournal = {The Lancet},
	author = {Pavord, Ian D and Korn, Stephanie and Howarth, Peter and Bleecker, Eugene R and Buhl, Roland and Keene, Oliver N and Ortega, Hector and Chanez, Pascal},
	urldate = {2022-03-04},
	date = {2012-08-18},
	langid = {english},
	file = {ScienceDirect Snapshot:/home/ajl/Zotero/storage/TQC32RH7/S014067361260988X.html:text/html},
}

@online{noauthor_dupilumab_nodate,
	title = {Dupilumab Efficacy and Safety in Moderate-to-Severe Uncontrolled Asthma {\textbar} {NEJM}},
	url = {https://www.nejm.org/doi/10.1056/NEJMoa1804092},
	urldate = {2022-03-04},
	file = {Dupilumab Efficacy and Safety in Moderate-to-Severe Uncontrolled Asthma | NEJM:/home/ajl/Zotero/storage/GZCVNRIG/NEJMoa1804092.html:text/html},
}

@article{domingo_as-needed_2019,
	title = {As-needed {ICS}-{LABA} in Mild Asthma: What Does the Evidence Say?},
	volume = {79},
	issn = {1179-1950},
	url = {https://doi.org/10.1007/s40265-019-01202-0},
	doi = {10.1007/s40265-019-01202-0},
	shorttitle = {As-needed {ICS}-{LABA} in Mild Asthma},
	abstract = {For the last three decades, the guidelines for asthma management have supported a stepwise therapeutic approach, based on the administration of controller medications (especially inhaled corticosteroids) complemented by on-demand use of rescue medication. Classically, the rescue medication recommended comprised short-acting β agonists ({SABA}). Some years ago, the use of Symbicort Maintenance and Reliever Therapy ({SMART}) demonstrated the benefits of a combination of budesonide-formoterol, an inhaled corticosteroid, and a long-acting β agonist ({ICS}-{LABA}) as rescue medication in moderate and severe asthma. The results were enthusiastically received, and this therapeutic option was adopted in the guidelines for moderate to severe asthma patients. Recently, four trials (two randomised placebo control trials under the auspices of the {SYGMA} project and two real-life studies, Novel {START}, and the {PRACTICAL} trial) have explored the potential benefits of substituting {SABA} with budesonide-formoterol as rescue medication in mild asthma patients. The {SYGMA} 1 and 2 studies showed that the combination with formoterol-budesonide as rescue medication provides better asthma control than short-acting β-agonists alone in {GINA} step 2 patients, although the superiority was slight. Compared to budesonide maintenance therapy, the fixed combination of {ICS}-{LABA} on demand provides poorer asthma control. Regarding exacerbations, the fixed dose {ICS}-{LABA} combination on demand showed the same benefits for the prevention of exacerbations as chronic {ICS} treatment in mild asthma patients. The Novel {START} study, which assessed a population with milder symptoms, concluded that the fixed dose {ICS}-{LABA} combination used as needed was superior to {SABA} (albuterol) as needed for the prevention of asthma exacerbations. These results in fact show that, in undertreated {GINA} step 2 with only {SABA} as needed, {ICS}-{LABA} is more effective than {SABA}. The authors of {PRACTICAL} concluded that the study provided modest evidence that the {ICS}-{LABA} combination used as-needed for symptom relief reduces the rate of severe exacerbations compared with maintenance low-dose budesonide plus terbutaline as needed, although the study was not limited to mild asthma since according to the treatment consumed, it was evident that they had recruited some moderate asthma patients. Despite this poor evidence, and ignoring the clinical histological benefits of chronic inhaled corticosteroids (especially when administered promptly), {GINA} 2019 recently recommended daily low dose {ICS} or {ICS}-{LABA} as needed as a first option for step 2 patients. For step 1, symptom-driven or as-needed treatment with {ICS}-{LABA} is recommended rather than {SABA} alone (the preferred option until the last {GINA} update). Finally, the {SIENA} study showed that 73\% of patients with mild asthma do not have an eosinophilic phenotype and that these patients have a similar clinical response to {ICS} (mometasone) and antimuscarinic drugs (tiotropium), results that challenge the indication of a drug combination that incorporates {ICS} as a first option. Overall, we believe there is insufficient evidence for the systematic recommendation of as-needed {ICS}-{LABA} instead of {SABA} on request for {GINA} step 1 or as a replacement for chronic {ICS} in {GINA} step 2.},
	pages = {1729--1737},
	number = {16},
	journaltitle = {Drugs},
	shortjournal = {Drugs},
	author = {Domingo, Christian and Rello, Jordi and Sogo, Ana},
	urldate = {2022-03-04},
	date = {2019-11-01},
	langid = {english},
	file = {Springer Full Text PDF:/home/ajl/Zotero/storage/KYSMD7WN/Domingo et al. - 2019 - As-needed ICS-LABA in Mild Asthma What Does the E.pdf:application/pdf},
}

@article{aalbers_achieving_2016,
	title = {Achieving asthma control with {ICS}/{LABA}: A review of strategies for asthma management and prevention},
	volume = {111},
	issn = {0954-6111},
	url = {https://www.sciencedirect.com/science/article/pii/S0954611115300834},
	doi = {10.1016/j.rmed.2015.11.002},
	shorttitle = {Achieving asthma control with {ICS}/{LABA}},
	abstract = {Maintenance treatment with an inhaled corticosteroid ({ICS}) and a long-acting β2-agonist ({LABA}) is recommended for patients whose asthma is not controlled with a low-to-moderate dose of {ICS} alone; a separate reliever medication is used on an as-needed basis. The Gaining Optimal Asthma {ControL} ({GOAL}) study demonstrated that salmeterol/fluticasone maintenance treatment can improve asthma control and reduce future risk compared with fluticasone alone, although the dose escalation design of this study meant that most patients treated with salmeterol/fluticasone were receiving the highest dose of {ICS} at the end of the study. Similarly, budesonide/formoterol maintenance therapy improved asthma control and reduced future risk compared with budesonide alone in the Formoterol and Corticosteroids Establishing Therapy ({FACET}) study. An alternative approach to asthma management is to use an {ICS}/{LABA} for both maintenance and reliever therapy. A large body of clinical evidence has shown that the use of budesonide/formoterol in this way improves both current control and reduces future risk compared with {ICS}/{LABA} plus as-needed short-acting β2-agonist ({SABA}), even when patients receive lower maintenance doses of {ICS} as part of the maintenance and reliever therapy regimen. In addition, one study has shown that beclometasone/formoterol maintenance and reliever therapy reduces exacerbations more effectively than beclometasone/formoterol plus as-needed {SABA}. The use of {ICS}/{LABA} as both maintenance and reliever therapy ensures that an increase in reliever use in response to worsening symptoms is automatically matched by an increase in {ICS}.},
	pages = {1--7},
	journaltitle = {Respiratory Medicine},
	shortjournal = {Respiratory Medicine},
	author = {Aalbers, René and Vogelmeier, Claus and Kuna, Piotr},
	urldate = {2022-03-04},
	date = {2016-02-01},
	langid = {english},
	keywords = {Asthma, Beclometasone, Budesonide, Formoterol, {ICS}/{LABA}, Maintenance and reliever therapy},
	file = {Full Text:/home/ajl/Zotero/storage/JU9BSRMW/Aalbers et al. - 2016 - Achieving asthma control with ICSLABA A review o.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/XUVD7JZH/S0954611115300834.html:text/html},
}

@article{nurmagambetov_economic_2018,
	title = {The Economic Burden of Asthma in the United States, 2008–2013},
	volume = {15},
	issn = {2329-6933},
	url = {https://www.atsjournals.org/doi/10.1513/AnnalsATS.201703-259OC},
	doi = {10.1513/AnnalsATS.201703-259OC},
	abstract = {Rationale: Asthma is a chronic disease that affects quality of life, productivity at work and school, and healthcare use; and it can result in death. Measuring the current economic burden of asthma provides important information on the impact of asthma on society. This information can be used to make informed decisions about allocation of limited public health resources.

Objectives: In this paper, we provide a comprehensive approach to estimating the current prevalence, medical costs, cost of absenteeism (missed work and school days), and mortality attributable to asthma from a national perspective. In addition, we estimate the association of the incremental medical cost of asthma with several important factors, including race/ethnicity, education, poverty, and insurance status.

Methods: The primary source of data was the 2008–2013 household component of the Medical Expenditure Panel Survey. We defined treated asthma as the presence of at least one medical or pharmaceutical encounter or claim associated with asthma. For the main analysis, we applied two-part regression models to estimate asthma-related annual per-person incremental medical costs and negative binomial models to estimate absenteeism associated with asthma.

Results: Of 213,994 people in the pooled sample, 10,237 persons had treated asthma (prevalence, 4.8\%). The annual per-person incremental medical cost of asthma was \$3,266 (in 2015 U.S. dollars), of which \$1,830 was attributable to prescription medication, \$640 to office visits, \$529 to hospitalizations, \$176 to hospital-based outpatient visits, and \$105 to emergency room visits. For certain groups, the per-person incremental medical cost of asthma differed from that of the population average, namely \$2,145 for uninsured persons and \$3,581 for those living below the poverty line. During 2008–2013, asthma was responsible for \$3 billion in losses due to missed work and school days, \$29 billion due to asthma-related mortality, and \$50.3 billion in medical costs. All combined, the total cost of asthma in the United States based on the pooled sample amounted to \$81.9 billion in 2013.

Conclusions: Asthma places a significant economic burden on the United States, with a total cost of asthma, including costs incurred by absenteeism and mortality, of \$81.9 billion in 2013.},
	pages = {348--356},
	number = {3},
	journaltitle = {Annals of the American Thoracic Society},
	shortjournal = {Annals {ATS}},
	author = {Nurmagambetov, Tursynbek and Kuwahara, Robin and Garbe, Paul},
	urldate = {2022-03-04},
	date = {2018-03},
	note = {Publisher: American Thoracic Society - {AJRCCM}},
	keywords = {cost of illness, health expenditures, healthcare costs, treatment costs},
	file = {Full Text PDF:/home/ajl/Zotero/storage/G3PTJBD7/Nurmagambetov et al. - 2018 - The Economic Burden of Asthma in the United States.pdf:application/pdf},
}

@online{noauthor_time_nodate,
	title = {Time Trends in Racial and Ethnic Disparities in Asthma Prevalence in the United States From the Behavioral Risk Factor Surveillance System ({BRFSS}) Study (1999–2011) {\textbar} {AJPH} {\textbar} Vol. 105 Issue 6},
	url = {https://ajph.aphapublications.org/doi/10.2105/AJPH.2014.302172},
	urldate = {2022-03-04},
	file = {Time Trends in Racial and Ethnic Disparities in Asthma Prevalence in the United States From the Behavioral Risk Factor Surveillance System (BRFSS) Study (1999–2011) | AJPH | Vol. 105 Issue 6:/home/ajl/Zotero/storage/23L37C36/AJPH.2014.html:text/html},
}

@article{qin_asthma-related_2021,
	title = {Asthma-related emergency department ({ED}) visits and post-{ED} visit hospital and critical care admissions, National Hospital Ambulatory Medical Care Survey, 2010–2015},
	volume = {58},
	issn = {0277-0903},
	url = {https://doi.org/10.1080/02770903.2020.1713149},
	doi = {10.1080/02770903.2020.1713149},
	abstract = {Background: Exacerbation of asthma symptoms increases the likelihood of emergency department ({ED}) visits and hospitalizations. Because the {ED} is an important healthcare resource for immediate asthma care with acute exacerbations, we identify those populations most likely to seek {ED} treatment for asthma and describe the asthma burden for post-{ED} visit hospitalizations and critical care units.Methods: We examined the characteristics of asthma-related {ED} visits and hospital admissions and assessed the association between them using multivariable logistic regression models by analyzing data from the National Hospital Ambulatory Medical Care Survey ({NHAMCS}) during 2010–2015.Results: Of all {ED} visits, 1.32\% were asthma-related; of all {ED} visits that resulted in hospitalization, 1.12\% were asthma-related and, of all {ED} visits that resulted in admission to a critical care unit, 1.20\% were asthma related. The percentages of asthma-related {ED} visits and post-{ED} hospitalizations (H) were greater among children (adjusted prevalence ratio: {ED}: 2.28 [1.96.29–2.65]; H: 8.75 [5.93–12.92]) than among adults and greater for blacks ({ED}: (2.26 [1.97–2.60]; H: 3.25 [2.07–5.12]) and Hispanics ({ED}: (1.74 [1.47–2.08]; H: 2.424 [1.46–4.00]) than for whites. The percentage of {ED} visits was also greater for those covered by Medicaid or the Children’s Health Insurance Program ({CHIP}) than by private insurance.Conclusions: Both asthma-related {ED} visits and post-{ED} hospitalizations were greater for children, blacks, and Hispanics. {ED} visits were also greater for Medicaid/{CHIP}. These findings might help prompt future studies on identifying additional potential risk factors for frequent {ED} visits among disproportionally affected subpopulations.},
	pages = {565--572},
	number = {5},
	journaltitle = {Journal of Asthma},
	author = {Qin, Xiaoting and Zahran, Hatice S. and Malilay, Josephine},
	urldate = {2022-03-04},
	date = {2021-05-04},
	pmid = {31922923},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/02770903.2020.1713149},
	keywords = {critical care unit, demographics, Healthcare use, hospitalization, source of payment},
}

@online{noauthor_full_nodate,
	title = {Full article: Asthma-related emergency department ({ED}) visits and post-{ED} visit hospital and critical care admissions, National Hospital Ambulatory Medical Care Survey, 2010–2015},
	url = {https://www.tandfonline.com/doi/full/10.1080/02770903.2020.1713149},
	urldate = {2022-03-04},
}

@online{noauthor_full_nodate-1,
	title = {Full article: Asthma-related emergency department ({ED}) visits and post-{ED} visit hospital and critical care admissions, National Hospital Ambulatory Medical Care Survey, 2010–2015},
	url = {https://www.tandfonline.com/doi/full/10.1080/02770903.2020.1713149},
	urldate = {2022-03-04},
	file = {Full article\: Asthma-related emergency department (ED) visits and post-ED visit hospital and critical care admissions, National Hospital Ambulatory Medical Care Survey, 2010–2015:/home/ajl/Zotero/storage/L5AY55UX/02770903.2020.html:text/html},
}

@online{noauthor_most_2021,
	title = {Most Recent National Asthma Data {\textbar} {CDC}},
	url = {https://www.cdc.gov/asthma/most_recent_national_asthma_data.htm},
	abstract = {Most Recent National Asthma Data},
	urldate = {2022-03-04},
	date = {2021-04-07},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/5PNDJHMS/most_recent_national_asthma_data.html:text/html},
}

@online{noauthor_age-specific_nodate,
	title = {Age-specific incidence of allergic and non-allergic asthma {\textbar} {BMC} Pulmonary Medicine {\textbar} Full Text},
	url = {https://bmcpulmmed.biomedcentral.com/articles/10.1186/s12890-019-1040-2},
	urldate = {2022-03-04},
	file = {Age-specific incidence of allergic and non-allergic asthma | BMC Pulmonary Medicine | Full Text:/home/ajl/Zotero/storage/ULF2NPYW/s12890-019-1040-2.html:text/html},
}

@article{hammad_basic_2021,
	title = {The basic immunology of asthma},
	volume = {184},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867421001665},
	doi = {10.1016/j.cell.2021.02.016},
	abstract = {In many asthmatics, chronic airway inflammation is driven by {IL}-4-, {IL}-5-, and {IL}-13-producing Th2 cells or {ILC}2s. Type 2 cytokines promote hallmark features of the disease such as eosinophilia, mucus hypersecretion, bronchial hyperresponsiveness ({BHR}), {IgE} production, and susceptibility to exacerbations. However, only half the asthmatics have this “type 2-high” signature, and “type 2-low” asthma is more associated with obesity, presence of neutrophils, and unresponsiveness to corticosteroids, the mainstay asthma therapy. Here, we review the underlying immunological basis of various asthma endotypes by discussing results obtained from animal studies as well as results generated in clinical studies targeting specific immune pathways.},
	pages = {1469--1485},
	number = {6},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Hammad, Hamida and Lambrecht, Bart N.},
	urldate = {2022-03-04},
	date = {2021-03-18},
	langid = {english},
	keywords = {asthma, biologics, endotypes, immune cells, lung},
	file = {ScienceDirect Snapshot:/home/ajl/Zotero/storage/U6MLH3R6/S0092867421001665.html:text/html},
}

@article{suleymanova_deep_2018,
	title = {A deep convolutional neural network approach for astrocyte detection},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-31284-x},
	doi = {10.1038/s41598-018-31284-x},
	abstract = {Astrocytes are involved in various brain pathologies including trauma, stroke, neurodegenerative disorders such as Alzheimer’s and Parkinson’s diseases, or chronic pain. Determining cell density in a complex tissue environment in microscopy images and elucidating the temporal characteristics of morphological and biochemical changes is essential to understand the role of astrocytes in physiological and pathological conditions. Nowadays, manual stereological cell counting or semi-automatic segmentation techniques are widely used for the quantitative analysis of microscopy images. Detecting astrocytes automatically is a highly challenging computational task, for which we currently lack efficient image analysis tools. We have developed a fast and fully automated software that assesses the number of astrocytes using Deep Convolutional Neural Networks ({DCNN}). The method highly outperforms state-of-the-art image analysis and machine learning methods and provides precision comparable to those of human experts. Additionally, the runtime of cell detection is significantly less than that of other three computational methods analysed, and it is faster than human observers by orders of magnitude. We applied our {DCNN}-based method to examine the number of astrocytes in different brain regions of rats with opioid-induced hyperalgesia/tolerance ({OIH}/{OIT}), as morphine tolerance is believed to activate glia. We have demonstrated a strong positive correlation between manual and {DCNN}-based quantification of astrocytes in rat brain.},
	pages = {12878},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Suleymanova, Ilida and Balassa, Tamas and Tripathi, Sushil and Molnar, Csaba and Saarma, Mart and Sidorova, Yulia and Horvath, Peter},
	urldate = {2022-02-14},
	date = {2018-08-27},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, Single-cell imaging},
	file = {Full Text PDF:/home/ajl/Zotero/storage/CGBZPMAB/Suleymanova et al. - 2018 - A deep convolutional neural network approach for a.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/C2GT25SI/s41598-018-31284-x.html:text/html},
}

@article{ljosa_annotated_2012,
	title = {Annotated high-throughput microscopy image sets for validation},
	volume = {9},
	rights = {2012 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.2083},
	doi = {10.1038/nmeth.2083},
	pages = {637--637},
	number = {7},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Ljosa, Vebjorn and Sokolnicki, Katherine L. and Carpenter, Anne E.},
	urldate = {2022-02-14},
	date = {2012-07},
	langid = {english},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {Bioinformatics, Microscopy},
	file = {Full Text PDF:/home/ajl/Zotero/storage/C5Q88YDW/Ljosa et al. - 2012 - Annotated high-throughput microscopy image sets fo.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/SQRRRGQQ/nmeth.html:text/html},
}

@article{kayasandik_multistep_2020,
	title = {A multistep deep learning framework for the automated detection and segmentation of astrocytes in fluorescent images of brain tissue},
	volume = {10},
	rights = {2020 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-020-61953-9},
	doi = {10.1038/s41598-020-61953-9},
	abstract = {While astrocytes have been traditionally described as passive supportive cells, studies during the last decade have shown they are active players in many aspects of {CNS} physiology and function both in normal and disease states. However, the precise mechanisms regulating astrocytes function and interactions within the {CNS} are still poorly understood. This knowledge gap is due in large part to the limitations of current image analysis tools that cannot process astrocyte images efficiently and to the lack of methods capable of quantifying their complex morphological characteristics. To provide an unbiased and accurate framework for the quantitative analysis of fluorescent images of astrocytes, we introduce a new automated image processing pipeline whose main novelties include an innovative module for cell detection based on multiscale directional filters and a segmentation routine that leverages deep learning and sparse representations to reduce the need of training data and improve performance. Extensive numerical tests show that our method performs very competitively with respect to state-of-the-art methods also in challenging images where astrocytes are clustered together. Our code is released open source and freely available to the scientific community.},
	pages = {5137},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Kayasandik, Cihan Bilge and Ru, Wenjuan and Labate, Demetrio},
	urldate = {2022-02-14},
	date = {2020-03-20},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Mathematics and computing, Neuroscience},
	file = {Full Text PDF:/home/ajl/Zotero/storage/B4XAAJAV/Kayasandik et al. - 2020 - A multistep deep learning framework for the automa.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/RDI7LLNZ/s41598-020-61953-9.html:text/html},
}

@inproceedings{seabold_statsmodels_2010,
	title = {statsmodels: Econometric and statistical modeling with python},
	booktitle = {9th Python in Science Conference},
	author = {Seabold, Skipper and Perktold, Josef},
	date = {2010},
}

@inproceedings{hagberg_exploring_2008,
	location = {Pasadena, {CA} {USA}},
	title = {Exploring Network Structure, Dynamics, and Function using {NetworkX}},
	pages = {11 -- 15},
	booktitle = {Proceedings of the 7th Python in Science Conference},
	author = {Hagberg, Aric A. and Schult, Daniel A. and Swart, Pieter J.},
	editor = {Varoquaux, Gaël and Vaught, Travis and Millman, Jarrod},
	date = {2008},
	keywords = {notion},
}

@article{king_bigg_2016,
	title = {{BiGG} Models: A platform for integrating, standardizing and sharing genome-scale models},
	volume = {44},
	issn = {0305-1048},
	url = {https://doi.org/10.1093/nar/gkv1049},
	doi = {10.1093/nar/gkv1049},
	shorttitle = {{BiGG} Models},
	abstract = {Genome-scale metabolic models are mathematically-structured knowledge bases that can be used to predict metabolic pathway usage and growth phenotypes. Furthermore, they can generate and test hypotheses when integrated with experimental data. To maximize the value of these models, centralized repositories of high-quality models must be established, models must adhere to established standards and model components must be linked to relevant databases. Tools for model visualization further enhance their utility. To meet these needs, we present {BiGG} Models (http://bigg.ucsd.edu), a completely redesigned Biochemical, Genetic and Genomic knowledge base. {BiGG} Models contains more than 75 high-quality, manually-curated genome-scale metabolic models. On the website, users can browse, search and visualize models. {BiGG} Models connects genome-scale models to genome annotations and external databases. Reaction and metabolite identifiers have been standardized across models to conform to community standards and enable rapid comparison across models. Furthermore, {BiGG} Models provides a comprehensive application programming interface for accessing {BiGG} Models with modeling and analysis tools. As a resource for highly curated, standardized and accessible models of metabolism, {BiGG} Models will facilitate diverse systems biology studies and support knowledge-based analysis of diverse experimental data.},
	pages = {D515--D522},
	issue = {D1},
	journaltitle = {Nucleic Acids Research},
	shortjournal = {Nucleic Acids Research},
	author = {King, Zachary A. and Lu, Justin and Dräger, Andreas and Miller, Philip and Federowicz, Stephen and Lerman, Joshua A. and Ebrahim, Ali and Palsson, Bernhard O. and Lewis, Nathan E.},
	urldate = {2021-12-13},
	date = {2016-01-04},
	file = {Full Text PDF:/home/ajl/Zotero/storage/LPAZD7G9/King et al. - 2016 - BiGG Models A platform for integrating, standardi.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/HMTY2FD3/2502593.html:text/html},
}

@online{noauthor_what_nodate,
	title = {What is flux balance analysis? {\textbar} Nature Biotechnology},
	url = {https://www.nature.com/articles/nbt.1614},
	urldate = {2021-12-13},
}

@online{noauthor_cobrapy_nodate,
	title = {{COBRApy}: {COnstraints}-Based Reconstruction and Analysis for Python {\textbar} {BMC} Systems Biology {\textbar} Full Text},
	url = {https://bmcsystbiol.biomedcentral.com/articles/10.1186/1752-0509-7-74},
	urldate = {2021-12-13},
}

@article{cplex_v12_2009,
	title = {V12. 1: User’s Manual for {CPLEX}},
	volume = {46},
	pages = {157},
	number = {53},
	journaltitle = {International Business Machines Corporation},
	author = {Cplex, {IBM} {ILOG}},
	date = {2009},
}

@article{kim_identification_2019,
	title = {Identification of critical connectors in the directed reaction-centric graphs of microbial metabolic networks},
	volume = {20},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-019-2897-z},
	doi = {10.1186/s12859-019-2897-z},
	abstract = {Detection of central nodes in asymmetrically directed biological networks depends on centrality metrics quantifying individual nodes’ importance in a network. In topological analyses on metabolic networks, various centrality metrics have been mostly applied to metabolite-centric graphs. However, centrality metrics including those not depending on high connections are largely unexplored for directed reaction-centric graphs.},
	pages = {328},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Kim, Eun-Youn and Ashlock, Daniel and Yoon, Sung Ho},
	urldate = {2021-12-13},
	date = {2019-06-13},
	keywords = {Cascade number, Centrality metric, Directed network, Information flow, Metabolic network, Reaction-centric graph},
	file = {Full Text PDF:/home/ajl/Zotero/storage/YTRJFUVH/Kim et al. - 2019 - Identification of critical connectors in the direc.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/CAWEADMV/s12859-019-2897-z.html:text/html},
}

@article{wolf_instance_2020,
	title = {Instance Separation Emerges from Inpainting},
	url = {http://arxiv.org/abs/2003.00891},
	abstract = {Deep neural networks trained to inpaint partially occluded images show a deep understanding of image composition and have even been shown to remove objects from images convincingly. In this work, we investigate how this implicit knowledge of image composition can be leveraged for fully self-supervised instance separation. We propose a measure for the independence of two image regions given a fully self-supervised inpainting network and separate objects by maximizing this independence. We evaluate our method on two microscopy image datasets and show that it reaches similar segmentation performance to fully supervised methods.},
	journaltitle = {{arXiv}:2003.00891 [cs, stat]},
	author = {Wolf, Steffen and Hamprecht, Fred A. and Funke, Jan},
	urldate = {2021-08-19},
	date = {2020-02-28},
	eprinttype = {arxiv},
	eprint = {2003.00891},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/RWUGJGN7/Wolf et al. - 2020 - Instance Separation Emerges from Inpainting.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/PY6V4QMS/2003.html:text/html},
}

@article{wolny_accurate_2020,
	title = {Accurate and versatile 3D segmentation of plant tissues at cellular resolution},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.57613},
	doi = {10.7554/eLife.57613},
	abstract = {Quantitative analysis of plant and animal morphogenesis requires accurate segmentation of individual cells in volumetric images of growing organs. In the last years, deep learning has provided robust automated algorithms that approach human performance, with applications to bio-image analysis now starting to emerge. Here, we present {PlantSeg}, a pipeline for volumetric segmentation of plant tissues into cells. {PlantSeg} employs a convolutional neural network to predict cell boundaries and graph partitioning to segment cells based on the neural network predictions. {PlantSeg} was trained on fixed and live plant organs imaged with confocal and light sheet microscopes. {PlantSeg} delivers accurate results and generalizes well across different tissues, scales, acquisition settings even on non plant samples. We present results of {PlantSeg} applications in diverse developmental contexts. {PlantSeg} is free and open-source, with both a command line and a user-friendly graphical interface.},
	pages = {e57613},
	journaltitle = {{eLife}},
	author = {Wolny, Adrian and Cerrone, Lorenzo and Vijayan, Athul and Tofanelli, Rachele and Barro, Amaya Vilches and Louveaux, Marion and Wenzl, Christian and Strauss, Sören and Wilson-Sánchez, David and Lymbouridou, Rena and Steigleder, Susanne S and Pape, Constantin and Bailoni, Alberto and Duran-Nebreda, Salva and Bassel, George W and Lohmann, Jan U and Tsiantis, Miltos and Hamprecht, Fred A and Schneitz, Kay and Maizel, Alexis and Kreshuk, Anna},
	editor = {Hardtke, Christian S and Bergmann, Dominique C and Bergmann, Dominique C and Graeff, Moritz},
	urldate = {2021-08-19},
	date = {2020-07-29},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {deep learning, image analysis, cell segmentation, instance segmentation},
	file = {Full Text PDF:/home/ajl/Zotero/storage/ILNFD557/Wolny et al. - 2020 - Accurate and versatile 3D segmentation of plant ti.pdf:application/pdf},
}

@article{januszewski_high-precision_2018,
	title = {High-precision automated reconstruction of neurons with flood-filling networks},
	volume = {15},
	rights = {2018 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-018-0049-4},
	doi = {10.1038/s41592-018-0049-4},
	abstract = {Reconstruction of neural circuits from volume electron microscopy data requires the tracing of cells in their entirety, including all their neurites. Automated approaches have been developed for tracing, but their error rates are too high to generate reliable circuit diagrams without extensive human proofreading. We present flood-filling networks, a method for automated segmentation that, similar to most previous efforts, uses convolutional neural networks, but contains in addition a recurrent pathway that allows the iterative optimization and extension of individual neuronal processes. We used flood-filling networks to trace neurons in a dataset obtained by serial block-face electron microscopy of a zebra finch brain. Using our method, we achieved a mean error-free neurite path length of 1.1 mm, and we observed only four mergers in a test set with a path length of 97 mm. The performance of flood-filling networks was an order of magnitude better than that of previous approaches applied to this dataset, although with substantially increased computational costs.},
	pages = {605--610},
	number = {8},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Januszewski, Michał and Kornfeld, Jörgen and Li, Peter H. and Pope, Art and Blakely, Tim and Lindsey, Larry and Maitin-Shepard, Jeremy and Tyka, Mike and Denk, Winfried and Jain, Viren},
	urldate = {2021-08-19},
	date = {2018-08},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 8
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational neuroscience;Software
Subject\_term\_id: computational-neuroscience;software},
	file = {Full Text PDF:/home/ajl/Zotero/storage/Y4PK4N8G/Januszewski et al. - 2018 - High-precision automated reconstruction of neurons.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/J5GBGVTS/s41592-018-0049-4.html:text/html},
}

@article{marini_semi-supervised_2021,
	title = {Semi-supervised training of deep convolutional neural networks with heterogeneous data and few local annotations: An experiment on prostate histopathology image classification},
	volume = {73},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841521002115},
	doi = {10.1016/j.media.2021.102165},
	shorttitle = {Semi-supervised training of deep convolutional neural networks with heterogeneous data and few local annotations},
	abstract = {Convolutional neural networks ({CNNs}) are state-of-the-art computer vision techniques for various tasks, particularly for image classification. However, there are domains where the training of classification models that generalize on several datasets is still an open challenge because of the highly heterogeneous data and the lack of large datasets with local annotations of the regions of interest, such as histopathology image analysis. Histopathology concerns the microscopic analysis of tissue specimens processed in glass slides to identify diseases such as cancer. Digital pathology concerns the acquisition, management and automatic analysis of digitized histopathology images that are large, having in the order of 100′0002 pixels per image. Digital histopathology images are highly heterogeneous due to the variability of the image acquisition procedures. Creating locally labeled regions (required for the training) is time-consuming and often expensive in the medical field, as physicians usually have to annotate the data. Despite the advances in deep learning, leveraging strongly and weakly annotated datasets to train classification models is still an unsolved problem, mainly when data are very heterogeneous. Large amounts of data are needed to create models that generalize well. This paper presents a novel approach to train {CNNs} that generalize to heterogeneous datasets originating from various sources and without local annotations. The data analysis pipeline targets Gleason grading on prostate images and includes two models in sequence, following a teacher/student training paradigm. The teacher model (a high-capacity neural network) automatically annotates a set of pseudo-labeled patches used to train the student model (a smaller network). The two models are trained with two different teacher/student approaches: semi-supervised learning and semi-weekly supervised learning. For each of the two approaches, three student training variants are presented. The baseline is provided by training the student model only with the strongly annotated data. Classification performance is evaluated on the student model at the patch level (using the local annotations of the Tissue Micro-Arrays Zurich dataset) and at the global level (using the {TCGA}-{PRAD}, The Cancer Genome Atlas-{PRostate} {ADenocarcinoma}, whole slide image Gleason score). The teacher/student paradigm allows the models to better generalize on both datasets, despite the inter-dataset heterogeneity and the small number of local annotations used. The classification performance is improved both at the patch-level (up to κ=0.6127±0.0133 from κ=0.5667±0.0285), at the {TMA} core-level (Gleason score) (up to κ=0.7645±0.0231 from κ=0.7186±0.0306) and at the {WSI}-level (Gleason score) (up to κ=0.4529±0.0512 from κ=0.2293±0.1350). The results show that with the teacher/student paradigm, it is possible to train models that generalize on datasets from entirely different sources, despite the inter-dataset heterogeneity and the lack of large datasets with local annotations.},
	pages = {102165},
	journaltitle = {Medical Image Analysis},
	shortjournal = {Medical Image Analysis},
	author = {Marini, Niccolò and Otálora, Sebastian and Müller, Henning and Atzori, Manfredo},
	urldate = {2021-08-19},
	date = {2021-10-01},
	langid = {english},
	keywords = {Computational pathology, Deep learning, Prostate cancer, Semi-supervision},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/54N3282T/Marini et al. - 2021 - Semi-supervised training of deep convolutional neu.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/X4C78VHS/S1361841521002115.html:text/html},
}

@article{sedai_uncertainty_2021,
	title = {Uncertainty guided semi-supervised segmentation of retinal layers in {OCT} images},
	url = {http://arxiv.org/abs/2103.02083},
	abstract = {Deep convolutional neural networks have shown outstanding performance in medical image segmentation tasks. The usual problem when training supervised deep learning methods is the lack of labeled data which is time-consuming and costly to obtain. In this paper, we propose a novel uncertainty-guided semi-supervised learning based on a student-teacher approach for training the segmentation network using limited labeled samples and a large number of unlabeled images. First, a teacher segmentation model is trained from the labeled samples using Bayesian deep learning. The trained model is used to generate soft segmentation labels and uncertainty maps for the unlabeled set. The student model is then updated using the softly segmented samples and the corresponding pixel-wise confidence of the segmentation quality estimated from the uncertainty of the teacher model using a newly designed loss function. Experimental results on a retinal layer segmentation task show that the proposed method improves the segmentation performance in comparison to the fully supervised approach and is on par with the expert annotator. The proposed semi-supervised segmentation framework is a key contribution and applicable for biomedical image segmentation across various imaging modalities where access to annotated medical images is challenging},
	journaltitle = {{arXiv}:2103.02083 [cs]},
	author = {Sedai, Suman and Antony, Bhavna and Rai, Ravneet and Jones, Katie and Ishikawa, Hiroshi and Schuman, Joel and Gadi, Wollstein and Garnavi, Rahil},
	urldate = {2021-08-19},
	date = {2021-03-02},
	eprinttype = {arxiv},
	eprint = {2103.02083},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/2C6YCDFG/Sedai et al. - 2021 - Uncertainty guided semi-supervised segmentation of.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/LCEVC2MA/2103.html:text/html},
}

@article{caicedo_data-analysis_2017,
	title = {Data-analysis strategies for image-based cell profiling},
	volume = {14},
	rights = {2017 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/nmeth.4397},
	doi = {10.1038/nmeth.4397},
	abstract = {This Review covers the steps required to create high-quality image-based profiles from high-throughput microscopy images.},
	pages = {849--863},
	number = {9},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Caicedo, Juan C. and Cooper, Sam and Heigwer, Florian and Warchal, Scott and Qiu, Peng and Molnar, Csaba and Vasilevich, Aliaksei S. and Barry, Joseph D. and Bansal, Harmanjit Singh and Kraus, Oren and Wawer, Mathias and Paavolainen, Lassi and Herrmann, Markus D. and Rohban, Mohammad and Hung, Jane and Hennig, Holger and Concannon, John and Smith, Ian and Clemons, Paul A. and Singh, Shantanu and Rees, Paul and Horvath, Peter and Linington, Roger G. and Carpenter, Anne E.},
	urldate = {2021-08-19},
	date = {2017-09},
	langid = {english},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 9
Primary\_atype: Reviews
Publisher: Nature Publishing Group
Subject\_term: Image processing;Machine learning
Subject\_term\_id: image-processing;machine-learning},
	file = {Full Text PDF:/home/ajl/Zotero/storage/7WTV9HA6/Caicedo et al. - 2017 - Data-analysis strategies for image-based cell prof.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/VCC4DNQ8/nmeth.html:text/html},
}

@software{reback_pandas-devpandas_2020,
	title = {pandas-dev/pandas: Pandas 1.0.3},
	url = {https://zenodo.org/record/3715232},
	shorttitle = {pandas-dev/pandas},
	abstract = {This is a minor bug-fix release in the 1.0.x series and includes some regression fixes and bug fixes. We recommend that all users upgrade to this version. See the full whatsnew for a list of all the changes. The release will be available on the defaults and conda-forge channels: conda install pandas Or via {PyPI}: python3 -m pip install --upgrade pandas Please report any issues with the release on the pandas issue tracker.},
	publisher = {Zenodo},
	author = {Reback, Jeff and {McKinney}, Wes and jbrockmendel and Bossche, Joris Van den and Augspurger, Tom and Cloud, Phillip and gfyoung and Sinhrks and Klein, Adam and Roeschke, Matthew and Hawkins, Simon and Tratner, Jeff and She, Chang and Ayd, William and Petersen, Terji and Garcia, Marc and Schendel, Jeremy and Hayden, Andy and {MomIsBestFriend} and Jancauskas, Vytautas and Battiston, Pietro and Seabold, Skipper and chris-b1 and h-vetinari and Hoyer, Stephan and Overmeire, Wouter and alimcmaster1 and Dong, Kaiqi and Whelan, Christopher and Mehyar, Mortada},
	urldate = {2021-08-19},
	date = {2020-03-18},
	doi = {10.5281/zenodo.3715232},
	file = {Zenodo Snapshot:/home/ajl/Zotero/storage/6D7CGFM7/3715232.html:text/html},
}

@software{sofroniew_naparinapari_2021,
	title = {napari/napari: 0.4.10rc0},
	url = {https://zenodo.org/record/4968798},
	shorttitle = {napari/napari},
	abstract = {pre-release 0.4.10rc0},
	publisher = {Zenodo},
	author = {Sofroniew, Nicholas and Lambert, Talley and Evans, Kira and Nunez-Iglesias, Juan and Winston, Philip and Bokota, Grzegorz and Peña-Castellanos, Gonzalo and Yamauchi, Kevin and Bussonnier, Matthias and Solak, Ahmet Can and ziyangczi and Pop, Draga Doncila and Buckley, Genevieve and Pam and alisterburt and Hilsenstein, Volker and Sweet, Andy and Hector and Freeman, Jeremy and Boone, Peter and Bragantini, Jordão and Lowe, Alan R. and Gohlke, Christoph and Royer, Loic and kir0ul and Har-Gil, Hagai and Migas, Lukasz and Kittisopikul, Mark and Haase, Robert and Axelrod, Shannon},
	urldate = {2021-08-19},
	date = {2021-06-16},
	doi = {10.5281/zenodo.4968798},
	file = {Zenodo Snapshot:/home/ajl/Zotero/storage/A362FH8M/4968798.html:text/html},
}

@online{noauthor_journal_nodate,
	title = {Journal of Open Source Software: seaborn: statistical data visualization},
	url = {https://joss.theoj.org/papers/10.21105/joss.03021},
	urldate = {2021-08-18},
	file = {Journal of Open Source Software\: seaborn\: statistical data visualization:/home/ajl/Zotero/storage/8VPS56WM/joss.html:text/html},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: A 2D graphics environment},
	volume = {9},
	doi = {10.1109/MCSE.2007.55},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	pages = {90--95},
	number = {3},
	journaltitle = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	date = {2007},
	note = {Publisher: {IEEE} {COMPUTER} {SOC}},
}

@inproceedings{kluyver_jupyter_2016,
	title = {Jupyter Notebooks – a publishing format for reproducible computational workflows},
	pages = {87 -- 90},
	booktitle = {Positioning and Power in Academic Publishing: Players, Agents and Agendas},
	publisher = {{IOS} Press},
	author = {Kluyver, Thomas and Ragan-Kelley, Benjamin and Pérez, Fernando and Granger, Brian and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and Grout, Jason and Corlay, Sylvain and Ivanov, Paul and Avila, Damián and Abdalla, Safia and Willing, Carol},
	editor = {Loizides, F. and Schmidt, B.},
	date = {2016},
}

@online{noauthor_scikit-image_nodate,
	title = {scikit-image: image processing in Python [{PeerJ}]},
	url = {https://peerj.com/articles/453/},
	urldate = {2021-08-18},
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python},
	volume = {17},
	doi = {10.1038/s41592-019-0686-2},
	pages = {261--272},
	journaltitle = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and van der Walt, Stéfan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, İlhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Antônio H. and Pedregosa, Fabian and van Mulbregt, Paul and {SciPy 1.0 Contributors}},
	date = {2020},
}

@online{noauthor_array_nodate,
	title = {Array programming with {NumPy} {\textbar} Nature},
	url = {https://www.nature.com/articles/s41586-020-2649-2},
	urldate = {2021-08-18},
}

@article{a_buslaev_albumentations_2018,
	title = {Albumentations: fast and flexible image augmentations},
	journaltitle = {{ArXiv} e-prints},
	author = {A. Buslaev, A. Parinov, E. Khvedchenya, V. I. Iglovikov and Kalinin, A. A.},
	date = {2018},
	note = {\_eprint: 1809.06839},
}

@misc{riad_learning_2022,
	title = {Learning strides in convolutional neural networks},
	url = {http://arxiv.org/abs/2202.01653},
	abstract = {Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, ﬁnding the best conﬁguration either requires crossvalidation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow ﬁnding better conﬁgurations at a lower computational cost. This work introduces {DiffStride}, the ﬁrst downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classiﬁcation show the generality and effectiveness of our solution: we use {DiffStride} as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a {ResNet}-18 architecture allows keeping consistent high performance on {CIFAR}10, {CIFAR}100 and {ImageNet} even when training starts from poor random stride conﬁgurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efﬁciency on {ImageNet}.},
	number = {{arXiv}:2202.01653},
	publisher = {{arXiv}},
	author = {Riad, Rachid and Teboul, Olivier and Grangier, David and Zeghidour, Neil},
	urldate = {2022-12-01},
	date = {2022-02-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.01653 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {Riad et al. - 2022 - Learning strides in convolutional neural networks.pdf:/home/ajl/Zotero/storage/99Y7LRGN/Riad et al. - 2022 - Learning strides in convolutional neural networks.pdf:application/pdf},
}

@misc{zvyagin_genslms_2022,
	title = {{GenSLMs}: Genome-scale language models reveal {SARS}-{CoV}-2 evolutionary dynamics},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1},
	doi = {10.1101/2022.10.10.511571},
	shorttitle = {{GenSLMs}},
	abstract = {Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially {SARS}-{CoV}-2, are identified and classified. By adapting large language models ({LLMs}) for genomic data, we build genome-scale language models ({GenSLMs}) which can learn the evolutionary landscape of {SARS}-{CoV}-2 genomes. By pretraining on over 110 million prokaryotic gene sequences, and then finetuning a {SARS}-{CoV}-2 specific model on 1.5 million genomes, we show that {GenSLM} can accurately and rapidly identify variants of concern. Thus, to our knowledge, {GenSLM} represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of {GenSLMs} on both {GPU}-based supercomputers and {AI}-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining {GenSLMs} in tracking the evolutionary dynamics of {SARS}-{CoV}-2, noting that its full potential on large biological data is yet to be realized.},
	publisher = {{bioRxiv}},
	author = {Zvyagin, Maxim and Brace, Alexander and Hippe, Kyle and Deng, Yuntian and Zhang, Bin and Bohorquez, Cindy Orozco and Clyde, Austin and Kale, Bharat and Perez-Rivera, Danilo and Ma, Heng and Mann, Carla M. and Irvin, Michael and Pauloski, J. Gregory and Ward, Logan and Hayot, Valerie and Emani, Murali and Foreman, Sam and Xie, Zhen and Lin, Diangen and Shukla, Maulik and Nie, Weili and Romero, Josh and Dallago, Christian and Vahdat, Arash and Xiao, Chaowei and Gibbs, Thomas and Foster, Ian and Davis, James J. and Papka, Michael E. and Brettin, Thomas and Stevens, Rick and Anandkumar, Anima and Vishwanath, Venkatram and Ramanathan, Arvind},
	urldate = {2022-12-01},
	date = {2022-10-11},
	langid = {english},
	note = {Pages: 2022.10.10.511571
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/EZEJEKAR/Zvyagin et al. - 2022 - GenSLMs Genome-scale language models reveal SARS-.pdf:application/pdf},
}

@misc{boyeau_empirical_2022,
	title = {An Empirical Bayes Method for Differential Expression Analysis of Single Cells with Deep Generative Models},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.27.493625v1},
	doi = {10.1101/2022.05.27.493625},
	abstract = {Detecting differentially expressed genes is important for characterizing subpopulations of cells. In {scRNA}-seq data, however, nuisance variation due to technical factors like sequencing depth and {RNA} capture efficiency obscures the underlying biological signal. Deep generative models have been extensively applied to {scRNA}-seq data, with a special focus on embedding cells into a low-dimensional latent space and correcting for batch effects. However, little attention has been given to the problem of utilizing the uncertainty from the deep generative model for differential expression. Furthermore, the existing approaches do not allow controlling for the effect size or the false discovery rate. Here, we present lvm-{DE}, a generic Bayesian approach for performing differential expression from using a fitted deep generative model, while controlling the false discovery rate. We apply the lvm-{DE} framework to {scVI} and {scSphere}, two deep generative models. The resulting approaches outperform the state-of-the-art methods at estimating the log fold change in gene expression levels, as well as detecting differentially expressed genes between subpopulations of cells.},
	publisher = {{bioRxiv}},
	author = {Boyeau, Pierre and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I. and Lopez, Romain and Yosef, Nir},
	urldate = {2022-12-01},
	date = {2022-05-29},
	langid = {english},
	note = {Pages: 2022.05.27.493625
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/BBC6TIGH/Boyeau et al. - 2022 - An Empirical Bayes Method for Differential Express.pdf:application/pdf},
}

@misc{tour_feature-space_2022,
	title = {Feature-space selection with banded ridge regression},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.05.490831v2},
	doi = {10.1101/2022.05.05.490831},
	abstract = {Encoding models provide a powerful framework to identify the information represented in brain recordings. In this framework, a stimulus representation is expressed within a feature space and is used in a regularized linear regression to predict brain activity. To account for a potential complementarity of different feature spaces, a joint model is fit on multiple feature spaces simultaneously. To adapt regularization strength to each feature space, ridge regression is extended to banded ridge regression, which optimizes a different regularization hyperparameter per feature space. The present paper proposes a method to decompose over feature spaces the variance explained by a banded ridge regression model. It also describes how banded ridge regression performs a feature-space selection, effectively ignoring non-predictive and redundant feature spaces. This feature-space selection leads to better prediction accuracy and to better interpretability. Banded ridge regression is then mathematically linked to a number of other regression methods with similar feature-space selection mechanisms. Finally, several methods are proposed to address the computational challenge of fitting banded ridge regressions on large numbers of voxels and feature spaces. All implementations are released in an open-source Python package called Himalaya.},
	publisher = {{bioRxiv}},
	author = {Tour, Tom Dupré la and Eickenberg, Michael and Nunez-Elizalde, Anwar O. and Gallant, Jack L.},
	urldate = {2022-12-01},
	date = {2022-09-16},
	langid = {english},
	note = {Pages: 2022.05.05.490831
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/34GYBR9H/Tour et al. - 2022 - Feature-space selection with banded ridge regressi.pdf:application/pdf},
}

@misc{dorrell_actionable_2022,
	title = {Actionable Neural Representations: Grid Cells from Minimal Constraints},
	url = {http://arxiv.org/abs/2209.15563},
	doi = {10.48550/arXiv.2209.15563},
	shorttitle = {Actionable Neural Representations},
	abstract = {To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.},
	number = {{arXiv}:2209.15563},
	publisher = {{arXiv}},
	author = {Dorrell, William and Latham, Peter E. and Behrens, Timothy E. J. and Whittington, James C. R.},
	urldate = {2022-12-01},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2209.15563 [q-bio]},
	keywords = {notion, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/95J6I8N7/Dorrell et al. - 2022 - Actionable Neural Representations Grid Cells from.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/8IEA94W5/2209.html:text/html},
}

@misc{andriushchenko_sgd_2022,
	title = {{SGD} with large step sizes learns sparse features},
	url = {http://arxiv.org/abs/2210.05337},
	doi = {10.48550/arXiv.2210.05337},
	abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent ({SGD}) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward simple predictors. Furthermore, we show empirically that the longer large step sizes keep {SGD} high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the {SGD} training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the {SGD} dynamics through the loss landscape of neural networks. We justify these findings theoretically through the study of simple neural network models as well as qualitative arguments inspired from stochastic processes. Finally, this analysis allows to shed a new light on some common practice and observed phenomena when training neural networks. The code of our experiments is available at https://github.com/tml-epfl/sgd-sparse-features.},
	number = {{arXiv}:2210.05337},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Varre, Aditya and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	urldate = {2022-12-01},
	date = {2022-10-11},
	eprinttype = {arxiv},
	eprint = {2210.05337 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/MHT6YDVW/Andriushchenko et al. - 2022 - SGD with large step sizes learns sparse features.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/LFGEVVPB/2210.html:text/html},
}

@misc{fort_what_2022,
	title = {What does a deep neural network confidently perceive? The effective dimension of high certainty class manifolds and their low confidence boundaries},
	url = {http://arxiv.org/abs/2210.05546},
	doi = {10.48550/arXiv.2210.05546},
	shorttitle = {What does a deep neural network confidently perceive?},
	abstract = {Deep neural network classifiers partition input space into high confidence regions for each class. The geometry of these class manifolds ({CMs}) is widely studied and intimately related to model performance; for example, the margin depends on {CM} boundaries. We exploit the notions of Gaussian width and Gordon's escape theorem to tractably estimate the effective dimension of {CMs} and their boundaries through tomographic intersections with random affine subspaces of varying dimension. We show several connections between the dimension of {CMs}, generalization, and robustness. In particular we investigate how {CM} dimension depends on 1) the dataset, 2) architecture (including {ResNet}, {WideResNet} {\textbackslash}\& Vision Transformer), 3) initialization, 4) stage of training, 5) class, 6) network width, 7) ensemble size, 8) label randomization, 9) training set size, and 10) robustness to data corruption. Together a picture emerges that higher performing and more robust models have higher dimensional {CMs}. Moreover, we offer a new perspective on ensembling via intersections of {CMs}. Our code is at https://github.com/stanislavfort/slice-dice-optimize/},
	number = {{arXiv}:2210.05546},
	publisher = {{arXiv}},
	author = {Fort, Stanislav and Cubuk, Ekin Dogus and Ganguli, Surya and Schoenholz, Samuel S.},
	urldate = {2022-12-01},
	date = {2022-10-11},
	eprinttype = {arxiv},
	eprint = {2210.05546 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/HYFJM2YC/Fort et al. - 2022 - What does a deep neural network confidently percei.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/XGEY85RF/2210.html:text/html},
}

@misc{chen_unified_2022,
	title = {A Unified Sequence Interface for Vision Tasks},
	url = {http://arxiv.org/abs/2206.07669},
	doi = {10.48550/arXiv.2206.07669},
	abstract = {While language tasks are naturally expressed in a single, unified, modeling framework, i.e., generating sequences of tokens, this has not been the case in computer vision. As a result, there is a proliferation of distinct architectures and loss functions for different vision tasks. In this work we show that a diverse set of "core" computer vision tasks can also be unified if formulated in terms of a shared pixel-to-sequence interface. We focus on four tasks, namely, object detection, instance segmentation, keypoint detection, and image captioning, all with diverse types of outputs, e.g., bounding boxes or dense masks. Despite that, by formulating the output of each task as a sequence of discrete tokens with a unified interface, we show that one can train a neural network with a single model architecture and loss function on all these tasks, with no task-specific customization. To solve a specific task, we use a short prompt as task description, and the sequence output adapts to the prompt so it can produce task-specific output. We show that such a model can achieve competitive performance compared to well-established task-specific models.},
	number = {{arXiv}:2206.07669},
	publisher = {{arXiv}},
	author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Lin, Tsung-Yi and Fleet, David J. and Hinton, Geoffrey},
	urldate = {2022-12-01},
	date = {2022-10-15},
	eprinttype = {arxiv},
	eprint = {2206.07669 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/94IKWRWS/Chen et al. - 2022 - A Unified Sequence Interface for Vision Tasks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/7CR2KSZ6/2206.html:text/html},
}

@misc{chen_analog_2022,
	title = {Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},
	url = {http://arxiv.org/abs/2208.04202},
	doi = {10.48550/arXiv.2208.04202},
	shorttitle = {Analog Bits},
	abstract = {We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both {CIFAR}-10 (which has 3K discrete 8-bit tokens) and {ImageNet}-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by {FID}) and efficiency. For image captioning on {MS}-{COCO} dataset, our approach achieves competitive results compared to autoregressive models.},
	number = {{arXiv}:2208.04202},
	publisher = {{arXiv}},
	author = {Chen, Ting and Zhang, Ruixiang and Hinton, Geoffrey},
	urldate = {2022-12-01},
	date = {2022-08-08},
	eprinttype = {arxiv},
	eprint = {2208.04202 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/XD6XNNJW/Chen et al. - 2022 - Analog Bits Generating Discrete Data using Diffus.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/QGT9XT37/2208.html:text/html},
}

@misc{kunin_asymmetric_2022,
	title = {The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks},
	url = {http://arxiv.org/abs/2210.03820},
	doi = {10.48550/arXiv.2210.03820},
	abstract = {In this work, we explore the maximum-margin bias of quasi-homogeneous neural networks trained with gradient flow on an exponential loss and past a point of separability. We introduce the class of quasi-homogeneous models, which is expressive enough to describe nearly all neural networks with homogeneous activations, even those with biases, residual connections, and normalization layers, while structured enough to enable geometric analysis of its gradient dynamics. Using this analysis, we generalize the existing results of maximum-margin bias for homogeneous networks to this richer class of models. We find that gradient flow implicitly favors a subset of the parameters, unlike in the case of a homogeneous model where all parameters are treated equally. We demonstrate through simple examples how this strong favoritism toward minimizing an asymmetric norm can degrade the robustness of quasi-homogeneous models. On the other hand, we conjecture that this norm-minimization discards, when possible, unnecessary higher-order parameters, reducing the model to a sparser parameterization. Lastly, by applying our theorem to sufficiently expressive neural networks with normalization layers, we reveal a universal mechanism behind the empirical phenomenon of Neural Collapse.},
	number = {{arXiv}:2210.03820},
	publisher = {{arXiv}},
	author = {Kunin, Daniel and Yamamura, Atsushi and Ma, Chao and Ganguli, Surya},
	urldate = {2022-12-01},
	date = {2022-10-07},
	eprinttype = {arxiv},
	eprint = {2210.03820 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/CFWAVYSD/Kunin et al. - 2022 - The Asymmetric Maximum Margin Bias of Quasi-Homoge.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/3FV9LAUB/2210.html:text/html},
}

@misc{park_propertydag_2022,
	title = {{PropertyDAG}: Multi-objective Bayesian optimization of partially ordered, mixed-variable properties for biological sequence design},
	url = {http://arxiv.org/abs/2210.04096},
	doi = {10.48550/arXiv.2210.04096},
	shorttitle = {{PropertyDAG}},
	abstract = {Bayesian optimization offers a sample-efficient framework for navigating the exploration-exploitation trade-off in the vast design space of biological sequences. Whereas it is possible to optimize the various properties of interest jointly using a multi-objective acquisition function, such as the expected hypervolume improvement ({EHVI}), this approach does not account for objectives with a hierarchical dependency structure. We consider a common use case where some regions of the Pareto frontier are prioritized over others according to a specified \${\textbackslash}textit\{partial ordering\}\$ in the objectives. For instance, when designing antibodies, we would like to maximize the binding affinity to a target antigen only if it can be expressed in live cell culture -- modeling the experimental dependency in which affinity can only be measured for antibodies that can be expressed and thus produced in viable quantities. In general, we may want to confer a partial ordering to the properties such that each property is optimized conditioned on its parent properties satisfying some feasibility condition. To this end, we present {PropertyDAG}, a framework that operates on top of the traditional multi-objective {BO} to impose this desired ordering on the objectives, e.g. expression \${\textbackslash}rightarrow\$ affinity. We demonstrate its performance over multiple simulated active learning iterations on a penicillin production task, toy numerical problem, and a real-world antibody design task.},
	number = {{arXiv}:2210.04096},
	publisher = {{arXiv}},
	author = {Park, Ji Won and Stanton, Samuel and Saremi, Saeed and Watkins, Andrew and Dwyer, Henri and Gligorijevic, Vladimir and Bonneau, Richard and Ra, Stephen and Cho, Kyunghyun},
	urldate = {2022-12-01},
	date = {2022-10-08},
	eprinttype = {arxiv},
	eprint = {2210.04096 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, notion, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SQC9BL9D/Park et al. - 2022 - PropertyDAG Multi-objective Bayesian optimization.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/JTIW77F6/2210.html:text/html},
}

@misc{hill_learning_2016,
	title = {Learning Distributed Representations of Sentences from Unlabelled Data},
	url = {http://arxiv.org/abs/1602.03483},
	doi = {10.48550/arXiv.1602.03483},
	abstract = {Unsupervised methods for learning distributed representations of words are ubiquitous in today's {NLP} research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.},
	number = {{arXiv}:1602.03483},
	publisher = {{arXiv}},
	author = {Hill, Felix and Cho, Kyunghyun and Korhonen, Anna},
	urldate = {2022-12-01},
	date = {2016-02-10},
	eprinttype = {arxiv},
	eprint = {1602.03483 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/MCD6FDDW/Hill et al. - 2016 - Learning Distributed Representations of Sentences .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/X6A4NC5H/1602.html:text/html},
}

@misc{takamoto_pdebench_2022,
	title = {{PDEBENCH}: An Extensive Benchmark for Scientific Machine Learning},
	url = {http://arxiv.org/abs/2210.07182},
	doi = {10.48550/arXiv.2210.07182},
	shorttitle = {{PDEBENCH}},
	abstract = {Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific {ML} that are easy to use but still challenging and representative of a wide range of problems. We introduce {PDEBench}, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations ({PDEs}). {PDEBench} comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of {PDEs} compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and {PDE} parameters; (3) more extensible source codes with user-friendly {APIs} for data generation and baseline results with popular machine learning models ({FNO}, U-Net, {PINN}, Gradient-Based Inverse Method). {PDEBench} allows researchers to extend the benchmark freely for their own purposes using a standardized {API} and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific {ML}. With those metrics we identify tasks which are challenging for recent {ML} methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/{PDEBench}.},
	number = {{arXiv}:2210.07182},
	publisher = {{arXiv}},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and {MacKinlay}, Dan and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	urldate = {2022-12-01},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2210.07182 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Physics - Fluid Dynamics, Physics - Geophysics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/BVSKNATX/Takamoto et al. - 2022 - PDEBENCH An Extensive Benchmark for Scientific Ma.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/BBIJII8B/2210.html:text/html},
}

@article{okumura_nonlinear_2022,
	title = {Nonlinear decision-making with enzymatic neural networks},
	volume = {610},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05218-7},
	doi = {10.1038/s41586-022-05218-7},
	abstract = {Artificial neural networks have revolutionized electronic computing. Similarly, molecular networks with neuromorphic architectures may enable molecular decision-making on a level comparable to gene regulatory networks1,2. Non-enzymatic networks could in principle support neuromorphic architectures, and seminal proofs-of-principle have been reported3,4. However, leakages (that is, the unwanted release of species), as well as issues with sensitivity, speed, preparation and the lack of strong nonlinear responses, make the composition of layers delicate, and molecular classifications equivalent to a multilayer neural network remain elusive (for example, the partitioning of a concentration space into regions that cannot be linearly separated). Here we introduce {DNA}-encoded enzymatic neurons with tuneable weights and biases, and which are assembled in multilayer architectures to classify nonlinearly separable regions. We first leverage the sharp decision margin of a neuron to compute various majority functions on 10 bits. We then compose neurons into a two-layer network and synthetize a parametric family of rectangular functions on a {microRNA} input. Finally, we connect neural and logical computations into a hybrid circuit that recursively partitions a concentration plane according to a decision tree in cell-sized droplets. This computational power and extreme miniaturization open avenues to query and manage molecular systems with complex contents, such as liquid biopsies or {DNA} databases.},
	pages = {496--501},
	number = {7932},
	journaltitle = {Nature},
	author = {Okumura, S. and Gines, G. and Lobato-Dauzier, N. and Baccouche, A. and Deteix, R. and Fujii, T. and Rondelez, Y. and Genot, A. J.},
	urldate = {2022-12-01},
	date = {2022-10},
	langid = {english},
	note = {Number: 7932
Publisher: Nature Publishing Group},
	keywords = {notion, {DNA} computing, {DNA} computing and cryptography},
	file = {Full Text PDF:/home/ajl/Zotero/storage/9JGKR6MH/Okumura et al. - 2022 - Nonlinear decision-making with enzymatic neural ne.pdf:application/pdf},
}

@misc{li_large_2022,
	title = {Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers},
	url = {http://arxiv.org/abs/2210.06313},
	doi = {10.48550/arXiv.2210.06313},
	shorttitle = {Large Models are Parsimonious Learners},
	abstract = {This paper studies the curious phenomenon for machine learning models with Transformer architectures that their activation maps are sparse. By activation map we refer to the intermediate output of the multi-layer perceptrons ({MLPs}) after a {ReLU} activation function, and by "sparse" we mean that on average very few entries (e.g., 3.0\% for T5-Base and 6.3\% for {ViT}-B16) are nonzero for each input to {MLP}. Moreover, larger Transformers with more layers and wider {MLP} hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels, as well as for other architectures including {MLP}-mixers and 2-layer {MLPs}. We show that sparsity also emerges using training datasets with random labels, or with random inputs, or with infinite amount of data, demonstrating that sparsity is not a result of a specific family of datasets. We discuss how sparsity immediately implies a way to significantly reduce the {FLOP} count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small value of k brings a collection of desired but missing properties for Transformers, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.},
	number = {{arXiv}:2210.06313},
	publisher = {{arXiv}},
	author = {Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J. and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and Kumar, Sanjiv},
	urldate = {2022-12-01},
	date = {2022-10-12},
	eprinttype = {arxiv},
	eprint = {2210.06313 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/TMHPCTCG/Li et al. - 2022 - Large Models are Parsimonious Learners Activation.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/WIPQ9UTA/2210.html:text/html},
}

@article{egami_how_2022,
	title = {How to make causal inferences using texts},
	volume = {8},
	url = {https://www.science.org/doi/10.1126/sciadv.abg2652},
	doi = {10.1126/sciadv.abg2652},
	abstract = {Text as data techniques offer a great promise: the ability to inductively discover measures that are useful for testing social science theories with large collections of text. Nearly all text-based causal inferences depend on a latent representation of the text, but we show that estimating this latent representation from the data creates underacknowledged risks: we may introduce an identification problem or overfit. To address these risks, we introduce a split-sample workflow for making rigorous causal inferences with discovered measures as treatments or outcomes. We then apply it to estimate causal effects from an experiment on immigration attitudes and a study on bureaucratic responsiveness.},
	pages = {eabg2652},
	number = {42},
	journaltitle = {Science Advances},
	author = {Egami, Naoki and Fong, Christian J. and Grimmer, Justin and Roberts, Margaret E. and Stewart, Brandon M.},
	urldate = {2022-12-01},
	date = {2022-10-19},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/G4HYT3WI/Egami et al. - 2022 - How to make causal inferences using texts.pdf:application/pdf},
}

@misc{liao_gaussian-bernoulli_2022,
	title = {Gaussian-Bernoulli {RBMs} Without Tears},
	url = {http://arxiv.org/abs/2210.10318},
	doi = {10.48550/arXiv.2210.10318},
	abstract = {We revisit the challenging problem of training Gaussian-Bernoulli restricted Boltzmann machines ({GRBMs}), introducing two innovations. We propose a novel Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs sampling. We propose a modified contrastive divergence ({CD}) algorithm so that one can generate images with {GRBMs} starting from noise. This enables direct comparison of {GRBMs} with deep generative models, improving evaluation protocols in the {RBM} literature. Moreover, we show that modified {CD} and gradient clipping are enough to robustly train {GRBMs} with large learning rates, thus removing the necessity of various tricks in the literature. Experiments on Gaussian Mixtures, {MNIST}, {FashionMNIST}, and {CelebA} show {GRBMs} can generate good samples, despite their single-hidden-layer architecture. Our code is released at: {\textbackslash}url\{https://github.com/lrjconan/{GRBM}\}.},
	number = {{arXiv}:2210.10318},
	publisher = {{arXiv}},
	author = {Liao, Renjie and Kornblith, Simon and Ren, Mengye and Fleet, David J. and Hinton, Geoffrey},
	urldate = {2022-12-01},
	date = {2022-10-19},
	eprinttype = {arxiv},
	eprint = {2210.10318 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/TWT9Q5KW/Liao et al. - 2022 - Gaussian-Bernoulli RBMs Without Tears.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/4HRLRZX9/2210.html:text/html},
}

@online{noauthor_csci_nodate,
	title = {{CSCI} 601.771 (Self-supervised Models)},
	url = {https://self-supervised.cs.jhu.edu/},
	abstract = {Discussing latest breakthroughs in self-supervised language models},
	titleaddon = {{CSCI} 601.771 (Self-supervised Models)},
	urldate = {2022-12-01},
	langid = {english},
}

@misc{hassani_dilated_2022,
	title = {Dilated Neighborhood Attention Transformer},
	url = {http://arxiv.org/abs/2209.15001},
	doi = {10.48550/arXiv.2209.15001},
	abstract = {Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention ({NA}) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention ({DiNA}), a natural, flexible and efficient extension to {NA} that can capture more global context and expand receptive fields exponentially at no additional cost. {NA}'s local attention and {DiNA}'s sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer ({DiNAT}), a new hierarchical vision transformer built upon both. {DiNAT} variants enjoy significant improvements over strong baselines such as {NAT}, Swin, and {ConvNeXt}. Our large model is faster and ahead of its Swin counterpart by 1.5\% box {AP} in {COCO} object detection, 1.3\% mask {AP} in {COCO} instance segmentation, and 1.1\% {mIoU} in {ADE}20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on {COCO} (58.2 {PQ}) and {ADE}20K (48.5 {PQ}), and instance segmentation model on Cityscapes (44.5 {AP}) and {ADE}20K (35.4 {AP}) (no extra data). It also matches the state of the art specialized semantic segmentation models on {ADE}20K (58.2 {mIoU}), and ranks second on Cityscapes (84.5 {mIoU}) (no extra data). We open-source our project.},
	number = {{arXiv}:2209.15001},
	publisher = {{arXiv}},
	author = {Hassani, Ali and Shi, Humphrey},
	urldate = {2022-12-01},
	date = {2022-11-10},
	eprinttype = {arxiv},
	eprint = {2209.15001 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/KLFID5G7/Hassani and Shi - 2022 - Dilated Neighborhood Attention Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZYWY9U54/2209.html:text/html},
}

@misc{rusch_gradient_2022,
	title = {Gradient Gating for Deep Multi-Rate Learning on Graphs},
	url = {http://arxiv.org/abs/2210.00513},
	doi = {10.48550/arXiv.2210.00513},
	abstract = {We present Gradient Gating (G\${\textasciicircum}2\$), a novel framework for improving the performance of Graph Neural Networks ({GNNs}). Our framework is based on gating the output of {GNN} layers with a mechanism for multi-rate flow of message passing information across nodes of the underlying graph. Local gradients are harnessed to further modulate message passing updates. Our framework flexibly allows one to use any basic {GNN} layer as a wrapper around which the multi-rate gradient gating mechanism is built. We rigorously prove that G\${\textasciicircum}2\$ alleviates the oversmoothing problem and allows the design of deep {GNNs}. Empirical results are presented to demonstrate that the proposed framework achieves state-of-the-art performance on a variety of graph learning tasks, including on large-scale heterophilic graphs.},
	number = {{arXiv}:2210.00513},
	publisher = {{arXiv}},
	author = {Rusch, T. Konstantin and Chamberlain, Benjamin P. and Mahoney, Michael W. and Bronstein, Michael M. and Mishra, Siddhartha},
	urldate = {2022-12-01},
	date = {2022-10-02},
	eprinttype = {arxiv},
	eprint = {2210.00513 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/QUCJKUFY/Rusch et al. - 2022 - Gradient Gating for Deep Multi-Rate Learning on Gr.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/F8E78TJ3/2210.html:text/html},
}

@misc{liu_omnigrok_2022,
	title = {Omnigrok: Grokking Beyond Algorithmic Data},
	url = {http://arxiv.org/abs/2210.01117},
	doi = {10.48550/arXiv.2210.01117},
	shorttitle = {Omnigrok},
	abstract = {Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "{LU} mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.},
	number = {{arXiv}:2210.01117},
	publisher = {{arXiv}},
	author = {Liu, Ziming and Michaud, Eric J. and Tegmark, Max},
	urldate = {2022-12-01},
	date = {2022-10-03},
	eprinttype = {arxiv},
	eprint = {2210.01117 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence, Physics - Data Analysis, Statistics and Probability, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/7A7VLGAY/Liu et al. - 2022 - Omnigrok Grokking Beyond Algorithmic Data.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/3G5M2EUL/2210.html:text/html},
}

@misc{richter-powell_neural_2022,
	title = {Neural Conservation Laws: A Divergence-Free Perspective},
	url = {http://arxiv.org/abs/2210.01741},
	doi = {10.48550/arXiv.2210.01741},
	shorttitle = {Neural Conservation Laws},
	abstract = {We investigate the parameterization of deep neural networks that by design satisfy the continuity equation, a fundamental conservation law. This is enabled by the observation that any solution of the continuity equation can be represented as a divergence-free vector field. We hence propose building divergence-free neural networks through the concept of differential forms, and with the aid of automatic differentiation, realize two practical constructions. As a result, we can parameterize pairs of densities and vector fields that always exactly satisfy the continuity equation, foregoing the need for extra penalty methods or expensive numerical simulation. Furthermore, we prove these models are universal and so can be used to represent any divergence-free vector field. Finally, we experimentally validate our approaches by computing neural network-based solutions to fluid equations, solving for the Hodge decomposition, and learning dynamical optimal transport maps.},
	number = {{arXiv}:2210.01741},
	publisher = {{arXiv}},
	author = {Richter-Powell, Jack and Lipman, Yaron and Chen, Ricky T. Q.},
	urldate = {2022-12-01},
	date = {2022-11-01},
	eprinttype = {arxiv},
	eprint = {2210.01741 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SI335I2S/Richter-Powell et al. - 2022 - Neural Conservation Laws A Divergence-Free Perspe.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/SX4D9DEN/2210.html:text/html},
}

@article{qiu_interpretable_2022,
	title = {Interpretable machine learning prediction of all-cause mortality},
	volume = {2},
	rights = {2022 The Author(s)},
	issn = {2730-664X},
	url = {https://www.nature.com/articles/s43856-022-00180-x},
	doi = {10.1038/s43856-022-00180-x},
	abstract = {Unlike linear models which are traditionally used to study all-cause mortality, complex machine learning models can capture non-linear interrelations and provide opportunities to identify unexplored risk factors. Explainable artificial intelligence can improve prediction accuracy over linear models and reveal great insights into outcomes like mortality. This paper comprehensively analyzes all-cause mortality by explaining complex machine learning models.},
	pages = {1--15},
	number = {1},
	journaltitle = {Communications Medicine},
	shortjournal = {Commun Med},
	author = {Qiu, Wei and Chen, Hugh and Dincer, Ayse Berceste and Lundberg, Scott and Kaeberlein, Matt and Lee, Su-In},
	urldate = {2022-12-01},
	date = {2022-10-03},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Computational biology and bioinformatics, Epidemiology, Prognostic markers},
	file = {Full Text PDF:/home/ajl/Zotero/storage/G3FAUGHP/Qiu et al. - 2022 - Interpretable machine learning prediction of all-c.pdf:application/pdf},
}

@misc{johnson_contrastive_2022,
	title = {Contrastive Learning Can Find An Optimal Basis For Approximately View-Invariant Functions},
	url = {http://arxiv.org/abs/2210.01883},
	doi = {10.48550/arXiv.2210.01883},
	abstract = {Contrastive learning is a powerful framework for learning self-supervised representations that generalize well to downstream supervised tasks. We show that multiple existing contrastive learning methods can be reinterpreted as learning kernel functions that approximate a fixed positive-pair kernel. We then prove that a simple representation obtained by combining this kernel with {PCA} provably minimizes the worst-case approximation error of linear predictors, under a straightforward assumption that positive pairs have similar labels. Our analysis is based on a decomposition of the target function in terms of the eigenfunctions of a positive-pair Markov chain, and a surprising equivalence between these eigenfunctions and the output of Kernel {PCA}. We give generalization bounds for downstream linear prediction using our Kernel {PCA} representation, and show empirically on a set of synthetic tasks that applying Kernel {PCA} to contrastive learning models can indeed approximately recover the Markov chain eigenfunctions, although the accuracy depends on the kernel parameterization as well as on the augmentation strength.},
	number = {{arXiv}:2210.01883},
	publisher = {{arXiv}},
	author = {Johnson, Daniel D. and Hanchi, Ayoub El and Maddison, Chris J.},
	urldate = {2022-12-01},
	date = {2022-10-04},
	eprinttype = {arxiv},
	eprint = {2210.01883 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/25WLEUBY/Johnson et al. - 2022 - Contrastive Learning Can Find An Optimal Basis For.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/LSX496RE/2210.html:text/html},
}

@article{machado_multiregion_2022,
	title = {Multiregion neuronal activity: the forest and the trees},
	volume = {23},
	rights = {2022 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-022-00634-0},
	doi = {10.1038/s41583-022-00634-0},
	shorttitle = {Multiregion neuronal activity},
	abstract = {The past decade has witnessed remarkable advances in the simultaneous measurement of neuronal activity across many brain regions, enabling fundamentally new explorations of the brain-spanning cellular dynamics that underlie sensation, cognition and action. These recently developed multiregion recording techniques have provided many experimental opportunities, but thoughtful consideration of methodological trade-offs is necessary, especially regarding field of view, temporal acquisition rate and ability to guarantee cellular resolution. When applied in concert with modern optogenetic and computational tools, multiregion recording has already made possible fundamental biological discoveries — in part via the unprecedented ability to perform unbiased neural activity screens for principles of brain function, spanning dozens of brain areas and from local to global scales.},
	pages = {683--704},
	number = {11},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Machado, Timothy A. and Kauvar, Isaac V. and Deisseroth, Karl},
	urldate = {2022-12-01},
	date = {2022-11},
	langid = {english},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {notion, Neural circuits, Neuronal physiology},
	file = {Full Text PDF:/home/ajl/Zotero/storage/8JD6U28J/Machado et al. - 2022 - Multiregion neuronal activity the forest and the .pdf:application/pdf},
}

@misc{moschella_relative_2022,
	title = {Relative representations enable zero-shot latent space communication},
	url = {http://arxiv.org/abs/2209.15430},
	doi = {10.48550/arXiv.2209.15430},
	abstract = {Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, distinct latent spaces typically differ by an unknown quasi-isometric transformation: that is, in each space, the distances between the encodings do not change. In this work, we propose to adopt pairwise similarities as an alternative data representation, that can be used to enforce the desired invariance without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, latent isometry invariance, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., {CNNs}, {GCNs}, transformers).},
	number = {{arXiv}:2209.15430},
	publisher = {{arXiv}},
	author = {Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodolà, Emanuele},
	urldate = {2022-12-01},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2209.15430 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, 68T07, I.2.6},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JVSMNLPP/Moschella et al. - 2022 - Relative representations enable zero-shot latent s.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/6GP6T8TJ/2209.html:text/html},
}

@misc{dorrell_actionable_2022-1,
	title = {Actionable Neural Representations: Grid Cells from Minimal Constraints},
	url = {http://arxiv.org/abs/2209.15563},
	doi = {10.48550/arXiv.2209.15563},
	shorttitle = {Actionable Neural Representations},
	abstract = {To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won't have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an `actionable representation'. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.},
	number = {{arXiv}:2209.15563},
	publisher = {{arXiv}},
	author = {Dorrell, William and Latham, Peter E. and Behrens, Timothy E. J. and Whittington, James C. R.},
	urldate = {2022-12-01},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2209.15563 [q-bio]},
	keywords = {notion, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/BWIYKJW6/Dorrell et al. - 2022 - Actionable Neural Representations Grid Cells from.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/TMVTFKHV/2209.html:text/html},
}

@online{noauthor_learning_nodate-1,
	title = {Learning Differential Operators for Interpretable Time Series Modeling {\textbar} Proceedings of the 28th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	url = {https://dl.acm.org/doi/10.1145/3534678.3539245},
	urldate = {2022-12-01},
}

@article{ding_cooperative_2022,
	title = {Cooperative learning for multiview analysis},
	volume = {119},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2202113119},
	doi = {10.1073/pnas.2202113119},
	abstract = {We propose a method for supervised learning with multiple sets of features (“views”). The multiview problem is especially important in biology and medicine, where “-omics” data, such as genomics, proteomics, and radiomics, are measured on a common set of samples. “Cooperative learning” combines the usual squared-error loss of predictions with an “agreement” penalty to encourage the predictions from different data views to agree. By varying the weight of the agreement penalty, we get a continuum of solutions that include the well-known early and late fusion approaches. Cooperative learning chooses the degree of agreement (or fusion) in an adaptive manner, using a validation set or cross-validation to estimate test set prediction error. One version of our fitting procedure is modular, where one can choose different fitting mechanisms (e.g., lasso, random forests, boosting, or neural networks) appropriate for different data views. In the setting of cooperative regularized linear regression, the method combines the lasso penalty with the agreement penalty, yielding feature sparsity. The method can be especially powerful when the different data views share some underlying relationship in their signals that can be exploited to boost the signals. We show that cooperative learning achieves higher predictive accuracy on simulated data and real multiomics examples of labor-onset prediction. By leveraging aligned signals and allowing flexible fitting mechanisms for different modalities, cooperative learning offers a powerful approach to multiomics data fusion.},
	pages = {e2202113119},
	number = {38},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Ding, Daisy Yi and Li, Shuangning and Narasimhan, Balasubramanian and Tibshirani, Robert},
	urldate = {2022-12-01},
	date = {2022-09-20},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {notion},
	file = {Submitted Version:/home/ajl/Zotero/storage/ZLPYSTAB/Ding et al. - 2022 - Cooperative learning for multiview analysis.pdf:application/pdf},
}

@misc{daras_soft_2022,
	title = {Soft Diffusion: Score Matching for General Corruptions},
	url = {http://arxiv.org/abs/2209.05442},
	doi = {10.48550/arXiv.2209.05442},
	shorttitle = {Soft Diffusion},
	abstract = {We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for {CelebA}. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, {\textbackslash}textit\{that after corruption\}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art {FID} score \$1.85\$ on {CelebA}-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.},
	number = {{arXiv}:2209.05442},
	publisher = {{arXiv}},
	author = {Daras, Giannis and Delbracio, Mauricio and Talebi, Hossein and Dimakis, Alexandros G. and Milanfar, Peyman},
	urldate = {2022-12-01},
	date = {2022-10-04},
	eprinttype = {arxiv},
	eprint = {2209.05442 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ENJ25DP2/Daras et al. - 2022 - Soft Diffusion Score Matching for General Corrupt.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/X6EJRSAL/2209.html:text/html},
}

@misc{bengio_gflownet_2022,
	title = {{GFlowNet} Foundations},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks ({GFlowNets}) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of {GFlowNets}. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. {GFlowNets} amortize the work typically done by computationally expensive {MCMC} methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	number = {{arXiv}:2111.09266},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	urldate = {2022-12-01},
	date = {2022-08-15},
	eprinttype = {arxiv},
	eprint = {2111.09266 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/X56ERRGN/Bengio et al. - 2022 - GFlowNet Foundations.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/XG3HXP3D/2111.html:text/html},
}

@online{noauthor_parallel_nodate,
	title = {Parallel Processing with Dask on {GridEngine} Clusters - Eric J. Ma's Personal Site},
	url = {https://ericmjl.github.io/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/},
	urldate = {2022-12-01},
}

@misc{ainsworth_git_2022,
	title = {Git Re-Basin: Merging Models modulo Permutation Symmetries},
	url = {http://arxiv.org/abs/2209.04836},
	doi = {10.48550/arXiv.2209.04836},
	shorttitle = {Git Re-Basin},
	abstract = {The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is {NP}-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. (2021). We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained {ResNet} models on {CIFAR}-10 and {CIFAR}-100. Additionally, we investigate intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory.},
	number = {{arXiv}:2209.04836},
	publisher = {{arXiv}},
	author = {Ainsworth, Samuel K. and Hayase, Jonathan and Srinivasa, Siddhartha},
	urldate = {2022-12-01},
	date = {2022-11-12},
	eprinttype = {arxiv},
	eprint = {2209.04836 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DJ4BHNZP/Ainsworth et al. - 2022 - Git Re-Basin Merging Models modulo Permutation Sy.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/JRSRRPUW/2209.html:text/html},
}

@misc{mahmood_probabilistic_2022,
	title = {Probabilistic Matrix Factorization for Gene Regulatory Network Inference},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.09.09.507305v2},
	doi = {10.1101/2022.09.09.507305},
	abstract = {Gene regulatory network inference methods are often heuristically designed for specific datasets and biological problems, making it challenging to include extra information and compare against other algorithms. Here, we propose the use of probabilistic matrix factorization for gene regulatory network ({PMF}-{GRN}) inference from single-cell gene expression datasets of any size, incorporating experimental evidence into prior distributions over latent factors. We use variational inference to infer {GRNs}, enabling hyperparameter search and direct comparison to other generative models. We evaluate our method using Saccharomyces cerevisiae and Bacillus subtilis, benchmarking against database-derived gold standard {GRNs}. We find that {PMF}-{GRN} infers {GRNs} more accurately than the current state-of-the-art method, while reducing the need for heuristic model selection. We demonstrate the necessity of incorporating prior information into any matrix factorization approach to {GRN} inference. Finally, we find that {PMF}-{GRN}’s uncertainty estimates are well-calibrated.},
	publisher = {{bioRxiv}},
	author = {Mahmood, Omar and Gibbs, Claudia Skok and Bonneau, Richard and Cho, Kyunghyun},
	urldate = {2022-12-01},
	date = {2022-09-13},
	langid = {english},
	note = {Pages: 2022.09.09.507305
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/RFLCPHJT/Mahmood et al. - 2022 - Probabilistic Matrix Factorization for Gene Regula.pdf:application/pdf},
}

@online{noauthor_csep_nodate,
	title = {{CSEP} 590B},
	url = {https://sites.google.com/cs.washington.edu/csep590b},
	abstract = {General Information
Lecture time: Tuesdays, 6:30-9:20 pm
Location: Bill \& Melinda Gates Center ({CSE}2) G10
Instructors: Su-In Lee and Ian Covert
Teaching assistants: Hugh Chen and Chris Lin

Office hours
Su-In Lee: Thursdays 5:00-6:00pm @ Zoom
Ian Covert: Sundays 8:00-9:00pm @ Zoom
Hugh Chen:},
	urldate = {2022-12-01},
	langid = {english},
	keywords = {notion},
	file = {Snapshot:/home/ajl/Zotero/storage/VY32V7V3/csep590b.html:text/html},
}

@misc{brandstetter_clifford_2022,
	title = {Clifford Neural Layers for {PDE} Modeling},
	url = {http://arxiv.org/abs/2209.04934},
	doi = {10.48550/arXiv.2209.04934},
	abstract = {Partial differential equations ({PDEs}) see widespread use in sciences and engineering to describe simulation of physical processes as scalar and vector fields interacting and coevolving over time. Due to the computationally expensive nature of their standard solution methods, neural {PDE} surrogates have become an active research topic to accelerate these simulations. However, current methods do not explicitly take into account the relationship between different fields and their internal components, which are often correlated. Viewing the time evolution of such correlated fields through the lens of multivector fields allows us to overcome these limitations. Multivector fields consist of scalar, vector, as well as higher-order components, such as bivectors and trivectors. Their algebraic properties, such as multiplication, addition and other arithmetic operations can be described by Clifford algebras. To our knowledge, this paper presents the first usage of such multivector representations together with Clifford convolutions and Clifford Fourier transforms in the context of deep learning. The resulting Clifford neural layers are universally applicable and will find direct use in the areas of fluid dynamics, weather forecasting, and the modeling of physical systems in general. We empirically evaluate the benefit of Clifford neural layers by replacing convolution and Fourier operations in common neural {PDE} surrogates by their Clifford counterparts on two-dimensional Navier-Stokes and weather modeling tasks, as well as three-dimensional Maxwell equations. Clifford neural layers consistently improve generalization capabilities of the tested neural {PDE} surrogates.},
	number = {{arXiv}:2209.04934},
	publisher = {{arXiv}},
	author = {Brandstetter, Johannes and Berg, Rianne van den and Welling, Max and Gupta, Jayesh K.},
	urldate = {2022-12-01},
	date = {2022-09-08},
	eprinttype = {arxiv},
	eprint = {2209.04934 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/26BIW66S/Brandstetter et al. - 2022 - Clifford Neural Layers for PDE Modeling.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/9QZM6ZNZ/2209.html:text/html},
}

@misc{han_which_2022,
	title = {Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post hoc Explanations},
	url = {http://arxiv.org/abs/2206.01254},
	doi = {10.48550/arXiv.2206.01254},
	shorttitle = {Which Explanation Should I Choose?},
	abstract = {Despite the plethora of post hoc model explanation methods, the basic properties and behavior of these methods and the conditions under which each one is effective are not well understood. In this work, we bridge these gaps and address a fundamental question: Which explanation method should one use in a given situation? To this end, we adopt a function approximation perspective and formalize the local function approximation ({LFA}) framework. We show that popular explanation methods are instances of this framework, performing function approximations of the underlying model in different neighborhoods using different loss functions. We introduce a no free lunch theorem for explanation methods which demonstrates that no single method can perform optimally across all neighbourhoods and calls for choosing among methods. To choose among methods, we set forth a guiding principle based on the function approximation perspective, considering a method to be effective if it recovers the underlying model when the model is a member of the explanation function class. Then, we analyze the conditions under which popular explanation methods are effective and provide recommendations for choosing among explanation methods and creating new ones. Lastly, we empirically validate our theoretical results using various real world datasets, model classes, and prediction tasks. By providing a principled mathematical framework which unifies diverse explanation methods, our work characterizes the behaviour of these methods and their relation to one another, guides the choice of explanation methods, and paves the way for the creation of new ones.},
	number = {{arXiv}:2206.01254},
	publisher = {{arXiv}},
	author = {Han, Tessa and Srinivas, Suraj and Lakkaraju, Himabindu},
	urldate = {2022-12-01},
	date = {2022-06-02},
	eprinttype = {arxiv},
	eprint = {2206.01254 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/L9C79IUG/Han et al. - 2022 - Which Explanation Should I Choose A Function Appr.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/E2GF9PAD/2206.html:text/html},
}

@misc{bubeck_convex_2015,
	title = {Convex Optimization: Algorithms and Complexity},
	url = {http://arxiv.org/abs/1405.4980},
	doi = {10.48550/arXiv.1405.4980},
	shorttitle = {Convex Optimization},
	abstract = {This monograph presents the main complexity theorems in convex optimization and their corresponding algorithms. Starting from the fundamental theory of black-box optimization, the material progresses towards recent advances in structural optimization and stochastic optimization. Our presentation of black-box optimization, strongly influenced by Nesterov's seminal book and Nemirovski's lecture notes, includes the analysis of cutting plane methods, as well as (accelerated) gradient descent schemes. We also pay special attention to non-Euclidean settings (relevant algorithms include Frank-Wolfe, mirror descent, and dual averaging) and discuss their relevance in machine learning. We provide a gentle introduction to structural optimization with {FISTA} (to optimize a sum of a smooth and a simple non-smooth term), saddle-point mirror prox (Nemirovski's alternative to Nesterov's smoothing), and a concise description of interior point methods. In stochastic optimization we discuss stochastic gradient descent, mini-batches, random coordinate descent, and sublinear algorithms. We also briefly touch upon convex relaxation of combinatorial problems and the use of randomness to round solutions, as well as random walks based methods.},
	number = {{arXiv}:1405.4980},
	publisher = {{arXiv}},
	author = {Bubeck, Sébastien},
	urldate = {2022-12-01},
	date = {2015-11-16},
	eprinttype = {arxiv},
	eprint = {1405.4980 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Computational Complexity, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/A9ZUSPP5/Bubeck - 2015 - Convex Optimization Algorithms and Complexity.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/KGNQ8JW3/1405.html:text/html},
}

@misc{henaff_object_2022,
	title = {Object discovery and representation networks},
	url = {http://arxiv.org/abs/2203.08777},
	doi = {10.48550/arXiv.2203.08777},
	abstract = {The promise of self-supervised learning ({SSL}) is to leverage large amounts of unlabeled data to solve complex tasks. While there has been excellent progress with simple, image-level learning, recent methods have shown the advantage of including knowledge of image structure. However, by introducing hand-crafted image segmentations to define regions of interest, or specialized augmentation strategies, these methods sacrifice the simplicity and generality that makes {SSL} so powerful. Instead, we propose a self-supervised learning paradigm that discovers this image structure by itself. Our method, Odin, couples object discovery and representation networks to discover meaningful image segmentations without any supervision. The resulting learning paradigm is simpler, less brittle, and more general, and achieves state-of-the-art transfer learning results for object detection and instance segmentation on {COCO}, and semantic segmentation on {PASCAL} and Cityscapes, while strongly surpassing supervised pre-training for video segmentation on {DAVIS}.},
	number = {{arXiv}:2203.08777},
	publisher = {{arXiv}},
	author = {Hénaff, Olivier J. and Koppula, Skanda and Shelhamer, Evan and Zoran, Daniel and Jaegle, Andrew and Zisserman, Andrew and Carreira, João and Arandjelović, Relja},
	urldate = {2022-12-01},
	date = {2022-07-27},
	eprinttype = {arxiv},
	eprint = {2203.08777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/U3TGLGGY/Hénaff et al. - 2022 - Object discovery and representation networks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/HG39ZIVX/2203.html:text/html},
}

@software{tang_papers_2022,
	title = {papers with data to mine},
	url = {https://github.com/crazyhottommy/papers_with_data_to_mine},
	abstract = {published papers with a lot of data},
	author = {Tang, Ming},
	urldate = {2022-12-01},
	date = {2022-11-09},
	note = {original-date: 2020-01-30T15:13:26Z},
}

@misc{beyer_knowledge_2022,
	title = {Knowledge distillation: A good teacher is patient and consistent},
	url = {http://arxiv.org/abs/2106.05237},
	doi = {10.48550/arXiv.2106.05237},
	shorttitle = {Knowledge distillation},
	abstract = {There is a growing discrepancy in computer vision between large-scale models that achieve state-of-the-art performance and models that are affordable in practical applications. In this paper we address this issue and significantly bridge the gap between these two types of models. Throughout our empirical investigation we do not aim to necessarily propose a new method, but strive to identify a robust and effective recipe for making state-of-the-art large scale models affordable in practice. We demonstrate that, when performed correctly, knowledge distillation can be a powerful tool for reducing the size of large models without compromising their performance. In particular, we uncover that there are certain implicit design choices, which may drastically affect the effectiveness of distillation. Our key contribution is the explicit identification of these design choices, which were not previously articulated in the literature. We back up our findings by a comprehensive empirical study, demonstrate compelling results on a wide range of vision datasets and, in particular, obtain a state-of-the-art {ResNet}-50 model for {ImageNet}, which achieves 82.8\% top-1 accuracy.},
	number = {{arXiv}:2106.05237},
	publisher = {{arXiv}},
	author = {Beyer, Lucas and Zhai, Xiaohua and Royer, Amélie and Markeeva, Larisa and Anil, Rohan and Kolesnikov, Alexander},
	urldate = {2022-12-01},
	date = {2022-06-21},
	eprinttype = {arxiv},
	eprint = {2106.05237 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/94BKNXU9/Beyer et al. - 2022 - Knowledge distillation A good teacher is patient .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/LWRMYDQR/2106.html:text/html},
}

@misc{veit_residual_2016,
	title = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
	url = {http://arxiv.org/abs/1605.06431},
	doi = {10.48550/arXiv.1605.06431},
	abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
	number = {{arXiv}:1605.06431},
	publisher = {{arXiv}},
	author = {Veit, Andreas and Wilber, Michael and Belongie, Serge},
	urldate = {2022-12-01},
	date = {2016-10-26},
	eprinttype = {arxiv},
	eprint = {1605.06431 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/EENAN9JZ/Veit et al. - 2016 - Residual Networks Behave Like Ensembles of Relativ.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/G5NDYU2N/1605.html:text/html},
}

@misc{xu_representation_2018,
	title = {Representation Learning on Graphs with Jumping Knowledge Networks},
	url = {http://arxiv.org/abs/1806.03536},
	doi = {10.48550/arXiv.1806.03536},
	abstract = {Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of "neighboring" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge ({JK}) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the {JK} framework with models like Graph Convolutional Networks, {GraphSAGE} and Graph Attention Networks consistently improves those models' performance.},
	number = {{arXiv}:1806.03536},
	publisher = {{arXiv}},
	author = {Xu, Keyulu and Li, Chengtao and Tian, Yonglong and Sonobe, Tomohiro and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	urldate = {2022-12-01},
	date = {2018-06-25},
	eprinttype = {arxiv},
	eprint = {1806.03536 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/T4GILY6C/Xu et al. - 2018 - Representation Learning on Graphs with Jumping Kno.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/36523524/1806.html:text/html},
}

@misc{angelopoulos_gentle_2022,
	title = {A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification},
	url = {http://arxiv.org/abs/2107.07511},
	doi = {10.48550/arXiv.2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
	number = {{arXiv}:2107.07511},
	publisher = {{arXiv}},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	urldate = {2022-12-01},
	date = {2022-09-03},
	eprinttype = {arxiv},
	eprint = {2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ZPMBQXES/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/GSF6M2GP/2107.html:text/html},
}

@misc{jain_interpretable_2021,
	title = {Interpretable multi-timescale models for predicting {fMRI} responses to continuous natural speech},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.10.02.324392v2},
	doi = {10.1101/2020.10.02.324392},
	abstract = {Natural language contains information at multiple timescales. To understand how the human brain represents this information, one approach is to build encoding models that predict {fMRI} responses to natural language using representations extracted from neural network language models ({LMs}). However, these {LM}-derived representations do not explicitly separate information at different timescales, making it difficult to interpret the encoding models. In this work we construct interpretable multi-timescale representations by forcing individual units in an {LSTM} {LM} to integrate information over specific temporal scales. This allows us to explicitly and directly map the timescale of information encoded by each individual {fMRI} voxel. Further, the standard {fMRI} encoding procedure does not account for varying temporal properties in the encoding features. We modify the procedure so that it can capture both short- and long-timescale information. This approach outperforms other encoding models, particularly for voxels that represent long-timescale information. It also provides a finer-grained map of timescale information in the human language pathway. This serves as a framework for future work investigating temporal hierarchies across artificial and biological language systems.},
	publisher = {{bioRxiv}},
	author = {Jain, Shailee and Mahto, Shivangi and Turek, Javier S. and Vo, Vy A. and {LeBel}, Amanda and Huth, Alexander G.},
	urldate = {2022-12-01},
	date = {2021-02-16},
	langid = {english},
	note = {Pages: 2020.10.02.324392
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/2LBGGUBA/Jain et al. - 2021 - Interpretable multi-timescale models for predictin.pdf:application/pdf},
}

@misc{tour_feature-space_2022-1,
	title = {Feature-space selection with banded ridge regression},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.05.490831v2},
	doi = {10.1101/2022.05.05.490831},
	abstract = {Encoding models provide a powerful framework to identify the information represented in brain recordings. In this framework, a stimulus representation is expressed within a feature space and is used in a regularized linear regression to predict brain activity. To account for a potential complementarity of different feature spaces, a joint model is fit on multiple feature spaces simultaneously. To adapt regularization strength to each feature space, ridge regression is extended to banded ridge regression, which optimizes a different regularization hyperparameter per feature space. The present paper proposes a method to decompose over feature spaces the variance explained by a banded ridge regression model. It also describes how banded ridge regression performs a feature-space selection, effectively ignoring non-predictive and redundant feature spaces. This feature-space selection leads to better prediction accuracy and to better interpretability. Banded ridge regression is then mathematically linked to a number of other regression methods with similar feature-space selection mechanisms. Finally, several methods are proposed to address the computational challenge of fitting banded ridge regressions on large numbers of voxels and feature spaces. All implementations are released in an open-source Python package called Himalaya.},
	publisher = {{bioRxiv}},
	author = {Tour, Tom Dupré la and Eickenberg, Michael and Nunez-Elizalde, Anwar O. and Gallant, Jack L.},
	urldate = {2022-12-01},
	date = {2022-09-16},
	langid = {english},
	note = {Pages: 2022.05.05.490831
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/9KYAHIXS/Tour et al. - 2022 - Feature-space selection with banded ridge regressi.pdf:application/pdf},
}

@misc{han_mlpinit_2022,
	title = {{MLPInit}: Embarrassingly Simple {GNN} Training Acceleration with {MLP} Initialization},
	url = {http://arxiv.org/abs/2210.00102},
	doi = {10.48550/arXiv.2210.00102},
	shorttitle = {{MLPInit}},
	abstract = {Training graph neural networks ({GNNs}) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons ({MLPs}) with only node features. {MLPs}, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based {GNNs}, we can trivially derive an analog {MLP} (we call this a {PeerMLP}) whose weights can be made identical, making us curious about how do {GNNs} using weights from a fully trained {PeerMLP} perform? Surprisingly, we find that {GNNs} initialized with such weights significantly outperform their {PeerMLPs} for graph data, motivating us to use {PeerMLP} training as a precursor, initialization step to {GNN} training. To this end, we propose an embarrassingly simple, yet hugely effective initialization method for {GNN} training acceleration, called {MLPInit}. Our extensive experiments on multiple large-scale graph datasets with diverse {GNN} architectures validate that {MLPInit} can accelerate the training of {GNNs} (up to 33X speedup on {OGB}-products) and often improve prediction performance (e.g., up to 7.97\% improvement for {GraphSAGE} across 7 datasets for node classification, and up to 17.81\% improvement across 4 datasets for link prediction on metric Hits@10). Most importantly, {MLPInit} is extremely simple to implement and can be flexibly used as a plug-and-play initialization method for message passing-based {GNNs}.},
	number = {{arXiv}:2210.00102},
	publisher = {{arXiv}},
	author = {Han, Xiaotian and Zhao, Tong and Liu, Yozen and Hu, Xia and Shah, Neil},
	urldate = {2022-12-01},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2210.00102 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AVK58MUC/Han et al. - 2022 - MLPInit Embarrassingly Simple GNN Training Accele.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/YD7BLZ9C/2210.html:text/html},
}

@misc{refinetti_dynamics_2022,
	title = {The dynamics of representation learning in shallow, non-linear autoencoders},
	url = {http://arxiv.org/abs/2201.02115},
	doi = {10.48550/arXiv.2201.02115},
	abstract = {Autoencoders are the simplest neural network for unsupervised learning, and thus an ideal framework for studying feature learning. While a detailed understanding of the dynamics of linear autoencoders has recently been obtained, the study of non-linear autoencoders has been hindered by the technical difficulty of handling training data with non-trivial correlations - a fundamental prerequisite for feature extraction. Here, we study the dynamics of feature learning in non-linear, shallow autoencoders. We derive a set of asymptotically exact equations that describe the generalisation dynamics of autoencoders trained with stochastic gradient descent ({SGD}) in the limit of high-dimensional inputs. These equations reveal that autoencoders learn the leading principal components of their inputs sequentially. An analysis of the long-time dynamics explains the failure of sigmoidal autoencoders to learn with tied weights, and highlights the importance of training the bias in {ReLU} autoencoders. Building on previous results for linear networks, we analyse a modification of the vanilla {SGD} algorithm which allows learning of the exact principal components. Finally, we show that our equations accurately describe the generalisation dynamics of non-linear autoencoders on realistic datasets such as {CIFAR}10.},
	number = {{arXiv}:2201.02115},
	publisher = {{arXiv}},
	author = {Refinetti, Maria and Goldt, Sebastian},
	urldate = {2022-12-01},
	date = {2022-06-16},
	eprinttype = {arxiv},
	eprint = {2201.02115 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/NR8M3XU9/Refinetti and Goldt - 2022 - The dynamics of representation learning in shallow.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/QRLDZNGK/2201.html:text/html},
}

@misc{boyeau_empirical_2022-1,
	title = {An Empirical Bayes Method for Differential Expression Analysis of Single Cells with Deep Generative Models},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.05.27.493625v1},
	doi = {10.1101/2022.05.27.493625},
	abstract = {Detecting differentially expressed genes is important for characterizing subpopulations of cells. In {scRNA}-seq data, however, nuisance variation due to technical factors like sequencing depth and {RNA} capture efficiency obscures the underlying biological signal. Deep generative models have been extensively applied to {scRNA}-seq data, with a special focus on embedding cells into a low-dimensional latent space and correcting for batch effects. However, little attention has been given to the problem of utilizing the uncertainty from the deep generative model for differential expression. Furthermore, the existing approaches do not allow controlling for the effect size or the false discovery rate. Here, we present lvm-{DE}, a generic Bayesian approach for performing differential expression from using a fitted deep generative model, while controlling the false discovery rate. We apply the lvm-{DE} framework to {scVI} and {scSphere}, two deep generative models. The resulting approaches outperform the state-of-the-art methods at estimating the log fold change in gene expression levels, as well as detecting differentially expressed genes between subpopulations of cells.},
	publisher = {{bioRxiv}},
	author = {Boyeau, Pierre and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I. and Lopez, Romain and Yosef, Nir},
	urldate = {2022-12-01},
	date = {2022-05-29},
	langid = {english},
	note = {Pages: 2022.05.27.493625
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/C2QGWRIM/Boyeau et al. - 2022 - An Empirical Bayes Method for Differential Express.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/YH355AIG/2022.05.27.html:text/html},
}

@misc{mahmood_probabilistic_2022-1,
	title = {Probabilistic Matrix Factorization for Gene Regulatory Network Inference},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.09.09.507305v2},
	doi = {10.1101/2022.09.09.507305},
	abstract = {Gene regulatory network inference methods are often heuristically designed for specific datasets and biological problems, making it challenging to include extra information and compare against other algorithms. Here, we propose the use of probabilistic matrix factorization for gene regulatory network ({PMF}-{GRN}) inference from single-cell gene expression datasets of any size, incorporating experimental evidence into prior distributions over latent factors. We use variational inference to infer {GRNs}, enabling hyperparameter search and direct comparison to other generative models. We evaluate our method using Saccharomyces cerevisiae and Bacillus subtilis, benchmarking against database-derived gold standard {GRNs}. We find that {PMF}-{GRN} infers {GRNs} more accurately than the current state-of-the-art method, while reducing the need for heuristic model selection. We demonstrate the necessity of incorporating prior information into any matrix factorization approach to {GRN} inference. Finally, we find that {PMF}-{GRN}’s uncertainty estimates are well-calibrated.},
	publisher = {{bioRxiv}},
	author = {Mahmood, Omar and Gibbs, Claudia Skok and Bonneau, Richard and Cho, Kyunghyun},
	urldate = {2022-12-02},
	date = {2022-09-13},
	langid = {english},
	note = {Pages: 2022.09.09.507305
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/69KQNQSK/Mahmood et al. - 2022 - Probabilistic Matrix Factorization for Gene Regula.pdf:application/pdf},
}

@article{ding_cooperative_2022-1,
	title = {Cooperative learning for multiview analysis},
	volume = {119},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2202113119},
	doi = {10.1073/pnas.2202113119},
	abstract = {We propose a method for supervised learning with multiple sets of features (“views”). The multiview problem is especially important in biology and medicine, where “-omics” data, such as genomics, proteomics, and radiomics, are measured on a common set of samples. “Cooperative learning” combines the usual squared-error loss of predictions with an “agreement” penalty to encourage the predictions from different data views to agree. By varying the weight of the agreement penalty, we get a continuum of solutions that include the well-known early and late fusion approaches. Cooperative learning chooses the degree of agreement (or fusion) in an adaptive manner, using a validation set or cross-validation to estimate test set prediction error. One version of our fitting procedure is modular, where one can choose different fitting mechanisms (e.g., lasso, random forests, boosting, or neural networks) appropriate for different data views. In the setting of cooperative regularized linear regression, the method combines the lasso penalty with the agreement penalty, yielding feature sparsity. The method can be especially powerful when the different data views share some underlying relationship in their signals that can be exploited to boost the signals. We show that cooperative learning achieves higher predictive accuracy on simulated data and real multiomics examples of labor-onset prediction. By leveraging aligned signals and allowing flexible fitting mechanisms for different modalities, cooperative learning offers a powerful approach to multiomics data fusion.},
	pages = {e2202113119},
	number = {38},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Ding, Daisy Yi and Li, Shuangning and Narasimhan, Balasubramanian and Tibshirani, Robert},
	urldate = {2022-12-02},
	date = {2022-09-20},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/ZFXGZG2X/Ding et al. - 2022 - Cooperative learning for multiview analysis.pdf:application/pdf},
}

@inproceedings{luo_learning_2022,
	location = {New York, {NY}, {USA}},
	title = {Learning Differential Operators for Interpretable Time Series Modeling},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539245},
	doi = {10.1145/3534678.3539245},
	series = {{KDD} '22},
	abstract = {Modeling sequential patterns from data is at the core of various time series forecasting tasks. Deep learning models have greatly outperformed many traditional models, but these black-box models generally lack explainability in prediction and decision making. To reveal the underlying trend with understandable mathematical expressions, scientists and economists tend to use partial differential equations ({PDEs}) to explain the highly nonlinear dynamics of sequential patterns. However, it usually requires domain expert knowledge and a series of simplified assumptions, which is not always practical and can deviate from the ever-changing world. Is it possible to learn the differential relations from data dynamically to explain the time-evolving dynamics? In this work, we propose an learning framework that can automatically obtain interpretable {PDE} models from sequential data. Particularly, this framework is comprised of learnable differential blocks, named P-blocks, which is proved to be able to approximate any time-evolving complex continuous functions in theory. Moreover, to capture the dynamics shift, this framework introduces a meta-learning controller to dynamically optimize the hyper-parameters of a hybrid {PDE} model. Extensive experiments on times series forecasting of financial, engineering, and health data show that our model can provide valuable interpretability and achieve comparable performance to state-of-the-art models. From empirical studies, we find that learning a few differential operators may capture the major trend of sequential dynamics without massive computational complexity.},
	pages = {1192--1201},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining},
	publisher = {Association for Computing Machinery},
	author = {Luo, Yingtao and Xu, Chang and Liu, Yang and Liu, Weiqing and Zheng, Shun and Bian, Jiang},
	urldate = {2022-12-01},
	date = {2022-08-14},
	keywords = {notion, differential equations, interpretability, meta-learning, time series},
	file = {Full Text PDF:/home/ajl/Zotero/storage/4BQEUR8I/Luo et al. - 2022 - Learning Differential Operators for Interpretable .pdf:application/pdf},
}

@misc{luo_deep_2022,
	title = {Deep Stable Representation Learning on Electronic Health Records},
	url = {http://arxiv.org/abs/2209.01321},
	doi = {10.48550/arXiv.2209.01321},
	abstract = {Deep learning models have achieved promising disease prediction performance of the Electronic Health Records ({EHR}) of patients. However, most models developed under the I.I.D. hypothesis fail to consider the agnostic distribution shifts, diminishing the generalization ability of deep learning models to Out-Of-Distribution ({OOD}) data. In this setting, spurious statistical correlations that may change in different environments will be exploited, which can cause sub-optimal performances of deep learning models. The unstable correlation between procedures and diagnoses existed in the training distribution can cause spurious correlation between historical {EHR} and future diagnosis. To address this problem, we propose to use a causal representation learning method called Causal Healthcare Embedding ({CHE}). {CHE} aims at eliminating the spurious statistical relationship by removing the dependencies between diagnoses and procedures. We introduce the Hilbert-Schmidt Independence Criterion ({HSIC}) to measure the degree of independence between the embedded diagnosis and procedure features. Based on causal view analyses, we perform the sample weighting technique to get rid of such spurious relationship for the stable learning of {EHR} across different environments. Moreover, our proposed {CHE} method can be used as a flexible plug-and-play module that can enhance existing deep learning models on {EHR}. Extensive experiments on two public datasets and five state-of-the-art baselines unequivocally show that {CHE} can improve the prediction accuracy of deep learning models on out-of-distribution data by a large margin. In addition, the interpretability study shows that {CHE} could successfully leverage causal structures to reflect a more reasonable contribution of historical records for predictions.},
	number = {{arXiv}:2209.01321},
	publisher = {{arXiv}},
	author = {Luo, Yingtao and Liu, Zhaocheng and Liu, Qiang},
	urldate = {2022-12-02},
	date = {2022-09-03},
	eprinttype = {arxiv},
	eprint = {2209.01321 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/QMPIFER2/Luo et al. - 2022 - Deep Stable Representation Learning on Electronic .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/AT4WXPY2/2209.html:text/html},
}

@misc{cheng_generalizing_2020,
	title = {Generalizing Variational Autoencoders with Hierarchical Empirical Bayes},
	url = {http://arxiv.org/abs/2007.10389},
	doi = {10.48550/arXiv.2007.10389},
	abstract = {Variational Autoencoders ({VAEs}) have experienced recent success as data-generating models by using simple architectures that do not require significant fine-tuning of hyperparameters. However, {VAEs} are known to suffer from over-regularization which can lead to failure to escape local maxima. This phenomenon, known as posterior collapse, prevents learning a meaningful latent encoding of the data. Recent methods have mitigated this issue by deterministically moment-matching an aggregated posterior distribution to an aggregate prior. However, abandoning a probabilistic framework (and thus relying on point estimates) can both lead to a discontinuous latent space and generate unrealistic samples. Here we present Hierarchical Empirical Bayes Autoencoder ({HEBAE}), a computationally stable framework for probabilistic generative models. Our key contributions are two-fold. First, we make gains by placing a hierarchical prior over the encoding distribution, enabling us to adaptively balance the trade-off between minimizing the reconstruction loss function and avoiding over-regularization. Second, we show that assuming a general dependency structure between variables in the latent space produces better convergence onto the mean-field assumption for improved posterior inference. Overall, {HEBAE} is more robust to a wide-range of hyperparameter initializations than an analogous {VAE}. Using data from {MNIST} and {CelebA}, we illustrate the ability of {HEBAE} to generate higher quality samples based on {FID} score than existing autoencoder-based approaches.},
	number = {{arXiv}:2007.10389},
	publisher = {{arXiv}},
	author = {Cheng, Wei and Darnell, Gregory and Ramachandran, Sohini and Crawford, Lorin},
	urldate = {2022-12-02},
	date = {2020-07-20},
	eprinttype = {arxiv},
	eprint = {2007.10389 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/2KK5N262/Cheng et al. - 2020 - Generalizing Variational Autoencoders with Hierarc.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/PXPHN2QS/2007.html:text/html},
}

@misc{trippe_randomized_2022,
	title = {Randomized gates eliminate bias in sort-seq assays},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NoDerivs} 4.0 International), {CC} {BY}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.02.17.480881v1},
	doi = {10.1101/2022.02.17.480881},
	abstract = {Sort-seq assays are a staple of the biological engineering toolkit, allowing researchers to profile many groups of cells based on any characteristic that can be tied to fluorescence. However, current approaches, which segregate cells into bins deterministically based on their measured fluorescence, introduce systematic bias. We describe a surprising result: one can obtain unbiased estimates by incorporating randomness into sorting. We validate this approach in simulation and experimentally, and describe extensions for both estimating group level variances and for using multi-bin sorters.},
	publisher = {{bioRxiv}},
	author = {Trippe, Brian L. and Huang, Buwei and {DeBenedictis}, Erika A. and Coventry, Brian and Bhattacharya, Nicholas and Yang, Kevin K. and Baker, David and Crawford, Lorin},
	urldate = {2022-12-02},
	date = {2022-02-19},
	langid = {english},
	note = {Pages: 2022.02.17.480881
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/WFS2NPGT/Trippe et al. - 2022 - Randomized gates eliminate bias in sort-seq assays.pdf:application/pdf},
}

@misc{kawashima_gaussian_2022,
	title = {Gaussian Process Koopman Mode Decomposition},
	url = {http://arxiv.org/abs/2209.04111},
	doi = {10.48550/arXiv.2209.04111},
	abstract = {In this paper, we propose a nonlinear probabilistic generative model of Koopman mode decomposition based on an unsupervised Gaussian process. Existing data-driven methods for Koopman mode decomposition have focused on estimating the quantities specified by Koopman mode decomposition, namely, eigenvalues, eigenfunctions, and modes. Our model enables the simultaneous estimation of these quantities and latent variables governed by an unknown dynamical system. Furthermore, we introduce an efficient strategy to estimate the parameters of our model by low-rank approximations of covariance matrices. Applying the proposed model to both synthetic data and a real-world epidemiological dataset, we show that various analyses are available using the estimated parameters.},
	number = {{arXiv}:2209.04111},
	publisher = {{arXiv}},
	author = {Kawashima, Takahiro and Hino, Hideitsu},
	urldate = {2022-12-02},
	date = {2022-09-08},
	eprinttype = {arxiv},
	eprint = {2209.04111 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/C4GPVZSH/Kawashima and Hino - 2022 - Gaussian Process Koopman Mode Decomposition.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/2JT9KXY4/2209.html:text/html},
}

@misc{bengio_gflownet_2022-1,
	title = {{GFlowNet} Foundations},
	url = {http://arxiv.org/abs/2111.09266},
	doi = {10.48550/arXiv.2111.09266},
	abstract = {Generative Flow Networks ({GFlowNets}) have been introduced as a method to sample a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function. In this paper, we show a number of additional theoretical properties of {GFlowNets}. They can be used to estimate joint probability distributions and the corresponding marginal distributions where some variables are unspecified and, of particular interest, can represent distributions over composite objects like sets and graphs. {GFlowNets} amortize the work typically done by computationally expensive {MCMC} methods in a single but trained generative pass. They could also be used to estimate partition functions and free energies, conditional probabilities of supersets (supergraphs) given a subset (subgraph), as well as marginal distributions over all supersets (supergraphs) of a given set (graph). We introduce variations enabling the estimation of entropy and mutual information, sampling from a Pareto frontier, connections to reward-maximizing policies, and extensions to stochastic environments, continuous actions and modular energy functions.},
	number = {{arXiv}:2111.09266},
	publisher = {{arXiv}},
	author = {Bengio, Yoshua and Lahlou, Salem and Deleu, Tristan and Hu, Edward J. and Tiwari, Mo and Bengio, Emmanuel},
	urldate = {2022-12-02},
	date = {2022-08-15},
	eprinttype = {arxiv},
	eprint = {2111.09266 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SK5D6A7B/Bengio et al. - 2022 - GFlowNet Foundations.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/B244I2WN/2111.html:text/html},
}

@misc{daras_soft_2022-1,
	title = {Soft Diffusion: Score Matching for General Corruptions},
	url = {http://arxiv.org/abs/2209.05442},
	doi = {10.48550/arXiv.2209.05442},
	shorttitle = {Soft Diffusion},
	abstract = {We define a broader family of corruption processes that generalizes previously known diffusion models. To reverse these general diffusions, we propose a new objective called Soft Score Matching that provably learns the score function for any linear corruption process and yields state of the art results for {CelebA}. Soft Score Matching incorporates the degradation process in the network. Our new loss trains the model to predict a clean image, {\textbackslash}textit\{that after corruption\}, matches the diffused observation. We show that our objective learns the gradient of the likelihood under suitable regularity conditions for a family of corruption processes. We further develop a principled way to select the corruption levels for general diffusion processes and a novel sampling method that we call Momentum Sampler. We show experimentally that our framework works for general linear corruption processes, such as Gaussian blur and masking. We achieve state-of-the-art {FID} score \$1.85\$ on {CelebA}-64, outperforming all previous linear diffusion models. We also show significant computational benefits compared to vanilla denoising diffusion.},
	number = {{arXiv}:2209.05442},
	publisher = {{arXiv}},
	author = {Daras, Giannis and Delbracio, Mauricio and Talebi, Hossein and Dimakis, Alexandros G. and Milanfar, Peyman},
	urldate = {2022-12-02},
	date = {2022-10-04},
	eprinttype = {arxiv},
	eprint = {2209.05442 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JKMDBG3L/Daras et al. - 2022 - Soft Diffusion Score Matching for General Corrupt.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/TFJXW79Z/2209.html:text/html},
}

@article{su_ligand-receptor_2022,
	title = {Ligand-receptor promiscuity enables cellular addressing},
	volume = {13},
	issn = {2405-4712},
	url = {https://www.cell.com/cell-systems/abstract/S2405-4712(22)00128-4},
	doi = {10.1016/j.cels.2022.03.001},
	pages = {408--425.e12},
	number = {5},
	journaltitle = {Cell Systems},
	shortjournal = {cels},
	author = {Su, Christina J. and Murugan, Arvind and Linton, James M. and Yeluri, Akshay and Bois, Justin and Klumpe, Heidi and Langley, Matthew A. and Antebi, Yaron E. and Elowitz, Michael B.},
	urldate = {2022-12-02},
	date = {2022-05-18},
	pmid = {35421362},
	note = {Publisher: Elsevier},
	keywords = {notion, {BMP}, bone morphogenetic protein, cell-type specificity, combinatorial signaling, communication systems, information theory, ligand-receptor interactions, promiscuity, signal processing, signaling pathways},
	file = {Full Text PDF:/home/ajl/Zotero/storage/I2AFUHF4/Su et al. - 2022 - Ligand-receptor promiscuity enables cellular addre.pdf:application/pdf},
}

@misc{jha_neural_2022,
	title = {The Neural Process Family: Survey, Applications and Perspectives},
	url = {http://arxiv.org/abs/2209.00517},
	doi = {10.48550/arXiv.2209.00517},
	shorttitle = {The Neural Process Family},
	abstract = {The standard approaches to neural network implementation yield powerful function approximation capabilities but are limited in their abilities to learn meta representations and reason probabilistic uncertainties in their predictions. Gaussian processes, on the other hand, adopt the Bayesian learning scheme to estimate such uncertainties but are constrained by their efficiency and approximation capacity. The Neural Processes Family ({NPF}) intends to offer the best of both worlds by leveraging neural networks for meta-learning predictive uncertainties. Such potential has brought substantial research activity to the family in recent years. Therefore, a comprehensive survey of {NPF} models is needed to organize and relate their motivation, methodology, and experiments. This paper intends to address this gap while digging deeper into the formulation, research themes, and applications concerning the family members. We shed light on their potential to bring several recent advances in other deep learning domains under one umbrella. We then provide a rigorous taxonomy of the family and empirically demonstrate their capabilities for modeling data generating functions operating on 1-d, 2-d, and 3-d input domains. We conclude by discussing our perspectives on the promising directions that can fuel the research advances in the field. Code for our experiments will be made available at https://github.com/{srvCodes}/neural-processes-survey.},
	number = {{arXiv}:2209.00517},
	publisher = {{arXiv}},
	author = {Jha, Saurav and Gong, Dong and Wang, Xuesong and Turner, Richard E. and Yao, Lina},
	urldate = {2022-12-02},
	date = {2022-11-21},
	eprinttype = {arxiv},
	eprint = {2209.00517 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/KAJW6S5S/Jha et al. - 2022 - The Neural Process Family Survey, Applications an.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/9867DSCE/2209.html:text/html},
}

@misc{majdandzic_correcting_2022,
	title = {Correcting gradient-based interpretations of deep neural networks for genomics},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.04.29.490102v2},
	doi = {10.1101/2022.04.29.490102},
	abstract = {Post-hoc attribution methods are widely applied to provide insights into patterns learned by deep neural networks ({DNNs}). Despite their success in regulatory genomics, {DNNs} can learn arbitrary functions outside the probabilistic simplex that defines one-hot encoded {DNA}. This introduces a random gradient component that manifests as noise in attribution scores. Here we demonstrate the pervasiveness of off-simplex gradient noise for genomic {DNNs} and introduce a statistical correction that is effective at improving the interpretability of attribution methods.},
	publisher = {{bioRxiv}},
	author = {Majdandzic, Antonio and Rajesh, Chandana and Koo, Peter K.},
	urldate = {2022-12-02},
	date = {2022-08-23},
	langid = {english},
	note = {Pages: 2022.04.29.490102
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/JWU9APA8/Majdandzic et al. - 2022 - Correcting gradient-based interpretations of deep .pdf:application/pdf},
}

@misc{janizek_uncovering_2021,
	title = {Uncovering expression signatures of synergistic drug response using an ensemble of explainable {AI} models},
	rights = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.06.463409v1},
	doi = {10.1101/2021.10.06.463409},
	abstract = {Complex machine learning models are poised to revolutionize the treatment of diseases like acute myeloid leukemia ({AML}) by helping physicians choose optimal combinations of anti-cancer drugs based on molecular features. While accurate predictions are important, it is equally important to be able to learn about the underlying molecular basis of anti-cancer drug synergy. Explainable {AI} ({XAI}) offers a promising new route for data-driven cancer pharmacology, combining highly accurate models with interpretable insights into model decisions. Due to the highly correlated, high-dimensional nature of cancer transcriptomic data, however, we find that existing {XAI} approaches are suboptimal when applied naively to large transcriptomic datasets. We show how a novel approach based on model ensembling helps to increase the quality of explanations. We then use our method to demonstrate that a hematopoietic differentiation signature underlies synergy for a variety of anti-{AML} drug combinations.},
	publisher = {{bioRxiv}},
	author = {Janizek, Joseph D. and Dincer, Ayse B. and Celik, Safiye and Chen, Hugh and Chen, William and Naxerova, Kamila and Lee, Su-In},
	urldate = {2022-12-02},
	date = {2021-10-07},
	langid = {english},
	note = {Pages: 2021.10.06.463409
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/GIM5Z77H/Janizek et al. - 2021 - Uncovering expression signatures of synergistic dr.pdf:application/pdf},
}

@misc{trauble_discrete_2022,
	title = {Discrete Key-Value Bottleneck},
	url = {http://arxiv.org/abs/2207.11240},
	doi = {10.48550/arXiv.2207.11240},
	abstract = {Deep neural networks perform well on prediction and classification tasks in the canonical setting where data streams are i.i.d., labeled data is abundant, and class labels are balanced. Challenges emerge with distribution shifts, including non-stationary or imbalanced data streams. One powerful approach that has addressed this challenge involves self-supervised pretraining of large encoders on volumes of unlabeled data, followed by task-specific tuning. Given a new task, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable (key, value) codes. In this setup, we follow the encode; process the representation via a discrete bottleneck; and decode paradigm, where the input is fed to the pretrained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to the decoder to solve the current task. The model can only fetch and re-use a limited number of these (key, value) pairs during inference, enabling localized and context-dependent model updates. We theoretically investigate the ability of the proposed model to minimize the effect of the distribution shifts and show that such a discrete bottleneck with (key, value) pairs reduces the complexity of the hypothesis class. We empirically verified the proposed methods' benefits under challenging distribution shift scenarios across various benchmark datasets and show that the proposed model reduces the common vulnerability to non-i.i.d. and non-stationary training distributions compared to various other baselines.},
	number = {{arXiv}:2207.11240},
	publisher = {{arXiv}},
	author = {Träuble, Frederik and Goyal, Anirudh and Rahaman, Nasim and Mozer, Michael and Kawaguchi, Kenji and Bengio, Yoshua and Schölkopf, Bernhard},
	urldate = {2022-12-02},
	date = {2022-07-22},
	eprinttype = {arxiv},
	eprint = {2207.11240 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DUL36FML/Träuble et al. - 2022 - Discrete Key-Value Bottleneck.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/RQTDCKIZ/2207.html:text/html},
}

@misc{phuong_formal_2022,
	title = {Formal Algorithms for Transformers},
	url = {http://arxiv.org/abs/2207.09238},
	doi = {10.48550/arXiv.2207.09238},
	abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic {ML} terminology and simpler neural network architectures such as {MLPs}.},
	number = {{arXiv}:2207.09238},
	publisher = {{arXiv}},
	author = {Phuong, Mary and Hutter, Marcus},
	urldate = {2022-12-02},
	date = {2022-07-19},
	eprinttype = {arxiv},
	eprint = {2207.09238 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/XEPLKSJN/Phuong and Hutter - 2022 - Formal Algorithms for Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/92GX92CB/2207.html:text/html},
}

@misc{tong_incremental_2022,
	title = {Incremental Learning of Structured Memory via Closed-Loop Transcription},
	url = {http://arxiv.org/abs/2202.05411},
	doi = {10.48550/arXiv.2202.05411},
	abstract = {This work proposes a minimal computational model for learning a structured memory of multiple object classes in an incremental setting. Our approach is based on establishing a closed-loop transcription between multiple classes and their corresponding subspaces, known as a linear discriminative representation, in a low-dimensional feature space. Our method is both simpler and more efficient than existing approaches to incremental learning, in terms of model size, storage, and computation: it requires only a single, fixed-capacity autoencoding network with a feature space that is used for both discriminative and generative purposes. All network parameters are optimized simultaneously without architectural manipulations, by solving a constrained minimax game between the encoding and decoding maps over a single rate reduction-based objective. Experimental results show that our method can effectively alleviate catastrophic forgetting, achieving significantly better performance than prior work for both generative and discriminative purposes.},
	number = {{arXiv}:2202.05411},
	publisher = {{arXiv}},
	author = {Tong, Shengbang and Dai, Xili and Wu, Ziyang and Li, Mingyang and Yi, Brent and Ma, Yi},
	urldate = {2022-12-02},
	date = {2022-02-14},
	eprinttype = {arxiv},
	eprint = {2202.05411 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/GILCAQMC/Tong et al. - 2022 - Incremental Learning of Structured Memory via Clos.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/7U46YKS9/2202.html:text/html},
}

@misc{neufeld_inference_2022,
	title = {Inference after latent variable estimation for single-cell {RNA} sequencing data},
	url = {http://arxiv.org/abs/2207.00554},
	doi = {10.48550/arXiv.2207.00554},
	abstract = {In the analysis of single-cell {RNA} sequencing data, researchers often characterize the variation between cells by estimating a latent variable, such as cell type or pseudotime, representing some aspect of the individual cell's state. They then test each gene for association with the estimated latent variable. If the same data are used for both of these steps, then standard methods for computing p-values and confidence intervals in the second step will fail to achieve statistical guarantees such as Type 1 error control. Furthermore, approaches such as sample splitting that can be applied to solve similar problems in other settings are not applicable in this context. In this paper, we introduce count splitting, a flexible framework that allows us to carry out valid inference in this setting, for virtually any latent variable estimation technique and inference approach, under a Poisson assumption. We demonstrate the Type 1 error control and power of count splitting in a simulation study, and apply count splitting to a dataset of pluripotent stem cells differentiating to cardiomyocytes.},
	number = {{arXiv}:2207.00554},
	publisher = {{arXiv}},
	author = {Neufeld, Anna and Gao, Lucy L. and Popp, Joshua and Battle, Alexis and Witten, Daniela},
	urldate = {2022-12-02},
	date = {2022-10-18},
	eprinttype = {arxiv},
	eprint = {2207.00554 [stat]},
	keywords = {notion, Statistics - Methodology, Statistics - Applications},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/3JXCZNC7/Neufeld et al. - 2022 - Inference after latent variable estimation for sin.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/WUT8QKP9/2207.html:text/html},
}

@misc{chen_local_2022,
	title = {Local Distance Preserving Auto-encoders using Continuous k-Nearest Neighbours Graphs},
	url = {http://arxiv.org/abs/2206.05909},
	doi = {10.48550/arXiv.2206.05909},
	abstract = {Auto-encoder models that preserve similarities in the data are a popular tool in representation learning. In this paper we introduce several auto-encoder models that preserve local distances when mapping from the data space to the latent space. We use a local distance preserving loss that is based on the continuous k-nearest neighbours graph which is known to capture topological features at all scales simultaneously. To improve training performance, we formulate learning as a constraint optimisation problem with local distance preservation as the main objective and reconstruction accuracy as a constraint. We generalise this approach to hierarchical variational auto-encoders thus learning generative models with geometrically consistent latent and data spaces. Our method provides state-of-the-art performance across several standard datasets and evaluation metrics.},
	number = {{arXiv}:2206.05909},
	publisher = {{arXiv}},
	author = {Chen, Nutan and van der Smagt, Patrick and Cseke, Botond},
	urldate = {2022-12-02},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2206.05909 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/66Z4YKQN/Chen et al. - 2022 - Local Distance Preserving Auto-encoders using Cont.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/TRIRIK9U/2206.html:text/html},
}

@misc{li_what_2022,
	title = {What Makes Convolutional Models Great on Long Sequence Modeling?},
	url = {http://arxiv.org/abs/2210.09298},
	doi = {10.48550/arXiv.2210.09298},
	abstract = {Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over {SoTA} on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution ({SGConv}). {SGConv} exhibits strong empirical performance over several tasks: 1) With faster speed, {SGConv} surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging {SGConv} into standard language and vision models, it shows the potential to improve both efficiency and performance.},
	number = {{arXiv}:2210.09298},
	publisher = {{arXiv}},
	author = {Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
	urldate = {2022-12-02},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2210.09298 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/S7JKPHVR/Li et al. - 2022 - What Makes Convolutional Models Great on Long Sequ.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/973ME74W/2210.html:text/html},
}

@misc{adebayo_sanity_2020,
	title = {Sanity Checks for Saliency Maps},
	url = {http://arxiv.org/abs/1810.03292},
	doi = {10.48550/arXiv.1810.03292},
	abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
	number = {{arXiv}:1810.03292},
	publisher = {{arXiv}},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	urldate = {2022-12-02},
	date = {2020-11-06},
	eprinttype = {arxiv},
	eprint = {1810.03292 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/VQXWFXEL/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/2TAAVMNZ/1810.html:text/html},
}

@article{lipkova_artificial_2022,
	title = {Artificial intelligence for multimodal data integration in oncology},
	volume = {40},
	issn = {1535-6108},
	url = {https://www.sciencedirect.com/science/article/pii/S153561082200441X},
	doi = {10.1016/j.ccell.2022.09.012},
	abstract = {In oncology, the patient state is characterized by a whole spectrum of modalities, ranging from radiology, histology, and genomics to electronic health records. Current artificial intelligence ({AI}) models operate mainly in the realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their potential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing {AI} closer to clinical practice. {AI} models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient outcomes or treatment resistance. The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets. To support these advances, here we present a synopsis of {AI} methods and strategies for multimodal data fusion and association discovery. We outline approaches for {AI} interpretability and directions for {AI}-driven exploration through multimodal data interconnections. We examine challenges in clinical adoption and discuss emerging solutions.},
	pages = {1095--1110},
	number = {10},
	journaltitle = {Cancer Cell},
	shortjournal = {Cancer Cell},
	author = {Lipkova, Jana and Chen, Richard J. and Chen, Bowen and Lu, Ming Y. and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J. and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew F. K. and Shaban, Muhammad and Chen, Tiffany Y. and Mahmood, Faisal},
	urldate = {2022-12-02},
	date = {2022-10-10},
	langid = {english},
	keywords = {deep learning, notion, {AI} in oncology, deep learning in oncology, multimodal {AI}, multimodal fusion, multimodal integration},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/5SVINJT8/Lipkova et al. - 2022 - Artificial intelligence for multimodal data integr.pdf:application/pdf},
}

@article{okumura_nonlinear_2022-1,
	title = {Nonlinear decision-making with enzymatic neural networks},
	volume = {610},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05218-7},
	doi = {10.1038/s41586-022-05218-7},
	abstract = {Artificial neural networks have revolutionized electronic computing. Similarly, molecular networks with neuromorphic architectures may enable molecular decision-making on a level comparable to gene regulatory networks1,2. Non-enzymatic networks could in principle support neuromorphic architectures, and seminal proofs-of-principle have been reported3,4. However, leakages (that is, the unwanted release of species), as well as issues with sensitivity, speed, preparation and the lack of strong nonlinear responses, make the composition of layers delicate, and molecular classifications equivalent to a multilayer neural network remain elusive (for example, the partitioning of a concentration space into regions that cannot be linearly separated). Here we introduce {DNA}-encoded enzymatic neurons with tuneable weights and biases, and which are assembled in multilayer architectures to classify nonlinearly separable regions. We first leverage the sharp decision margin of a neuron to compute various majority functions on 10 bits. We then compose neurons into a two-layer network and synthetize a parametric family of rectangular functions on a {microRNA} input. Finally, we connect neural and logical computations into a hybrid circuit that recursively partitions a concentration plane according to a decision tree in cell-sized droplets. This computational power and extreme miniaturization open avenues to query and manage molecular systems with complex contents, such as liquid biopsies or {DNA} databases.},
	pages = {496--501},
	number = {7932},
	journaltitle = {Nature},
	author = {Okumura, S. and Gines, G. and Lobato-Dauzier, N. and Baccouche, A. and Deteix, R. and Fujii, T. and Rondelez, Y. and Genot, A. J.},
	urldate = {2022-12-02},
	date = {2022-10},
	langid = {english},
	note = {Number: 7932
Publisher: Nature Publishing Group},
	keywords = {notion, {DNA} computing, {DNA} computing and cryptography},
	file = {Full Text PDF:/home/ajl/Zotero/storage/6WJ42YAU/Okumura et al. - 2022 - Nonlinear decision-making with enzymatic neural ne.pdf:application/pdf},
}

@misc{liu_concordance_2022,
	title = {Concordance of {MERFISH} Spatial Transcriptomics with Bulk and Single-cell {RNA} Sequencing},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.03.04.483068v3},
	doi = {10.1101/2022.03.04.483068},
	abstract = {Spatial transcriptomics extends single cell {RNA} sequencing ({scRNA}-seq) by providing spatial context for cell type identification and analysis. Imaging-based spatial technologies such as Multiplexed Error-Robust Fluorescence In Situ Hybridization ({MERFISH}) can achieve single-cell resolution, directly mapping single cell identities to spatial positions. {MERFISH} produces an intrinsically different data type than {scRNA}-seq and a technical comparison between the two modalities is necessary to ascertain how to best integrate them. We performed {MERFISH} on mouse liver and kidney and compared the resulting bulk and single-cell {RNA} statistics with those from the Tabula Muris Senis cell atlas as well as from two Visium datasets. {MERFISH} quantitatively reproduced the bulk {RNA}-seq and {scRNA}-seq results with improvements in overall dropout rates and sensitivity. Finally, we found that {MERFISH} independently resolved distinct cell types and spatial structure in both liver and kidney. Computational integration with the Tabula Muris Senis atlas did not enhance these results. We conclude that compared to {scRNA}-seq, {MERFISH} provides a quantitatively comparable method for measuring single-cell gene expression and can robustly identify cell types without the need for computational integration with {scRNA}-seq reference atlases.},
	publisher = {{bioRxiv}},
	author = {Liu, Jonathan and Tran, Vanessa and Vemuri, Venkata Naga Pranathi and Byrne, Ashley and Borja, Michael and Kim, Yang Joon and Agarwal, Snigdha and Wang, Ruofan and Awayan, Kyle and Murti, Abhishek and Taychameekiatchai, Aris and Wang, Bruce and Emanuel, George and He, Jiang and Haliburton, John and Pisco, Angela Oliveira and Neff, Norma},
	urldate = {2022-12-02},
	date = {2022-09-01},
	langid = {english},
	note = {Pages: 2022.03.04.483068
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/A4NCZ7ZH/Liu et al. - 2022 - Concordance of MERFISH Spatial Transcriptomics wit.pdf:application/pdf},
}

@misc{chen_local_2022-1,
	title = {Local Distance Preserving Auto-encoders using Continuous k-Nearest Neighbours Graphs},
	url = {http://arxiv.org/abs/2206.05909},
	doi = {10.48550/arXiv.2206.05909},
	abstract = {Auto-encoder models that preserve similarities in the data are a popular tool in representation learning. In this paper we introduce several auto-encoder models that preserve local distances when mapping from the data space to the latent space. We use a local distance preserving loss that is based on the continuous k-nearest neighbours graph which is known to capture topological features at all scales simultaneously. To improve training performance, we formulate learning as a constraint optimisation problem with local distance preservation as the main objective and reconstruction accuracy as a constraint. We generalise this approach to hierarchical variational auto-encoders thus learning generative models with geometrically consistent latent and data spaces. Our method provides state-of-the-art performance across several standard datasets and evaluation metrics.},
	number = {{arXiv}:2206.05909},
	publisher = {{arXiv}},
	author = {Chen, Nutan and van der Smagt, Patrick and Cseke, Botond},
	urldate = {2022-12-02},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2206.05909 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JQ2V8W3U/Chen et al. - 2022 - Local Distance Preserving Auto-encoders using Cont.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/G9TEYTKB/2206.html:text/html},
}

@misc{chen_modular_2022,
	title = {Modular cell type organization of cortical areas revealed by in situ sequencing},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.06.515380v1},
	doi = {10.1101/2022.11.06.515380},
	abstract = {The cortex is composed of neuronal types with diverse gene expression that are organized into specialized cortical areas. These areas, each with characteristic cytoarchitecture 1–3, connectivity4,5, and neuronal activity 6–10, are wired into modular networks4,5,11. However, it remains unclear whether cortical areas and their modular organization can be similarly defined by their transcriptomic signatures. Here we used {BARseq}, a high-throughput in situ sequencing technique, to interrogate the expression of 107 cell type marker genes in 1.2 million cells over a mouse forebrain hemisphere at cellular resolution. De novo clustering of gene expression in single neurons revealed transcriptomic types that were consistent with previous single-cell {RNAseq} studies12,13. Within medium-grained cell types that are shared across all cortical areas, gene expression and the distribution of fine-grained cell types vary along the contours of cortical areas. The compositions of transcriptomic types are highly predictive of cortical area identity. We grouped cortical areas into modules so that areas within a module, but not across modules, had similar compositions of transcriptomic types. Strikingly, these modules match cortical subnetworks that are highly interconnected 4,5,11, suggesting that cortical areas that are similar in cell types are also wired together. This “wire-by-similarity” rule reflects a novel organizing principle for the connectivity of cortical areas. Our {BARseq}-based strategy is high-throughput and low-cost, and scaling up this approach to many animals can potentially reveal the brain-wide molecular architecture across individuals, developmental times, and disease models.},
	publisher = {{bioRxiv}},
	author = {Chen, Xiaoyin and Fischer, Stephan and Zhang, Aixin and Gillis, Jesse and Zador, Anthony M.},
	urldate = {2022-12-02},
	date = {2022-11-06},
	langid = {english},
	note = {Pages: 2022.11.06.515380
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/HALCSSWV/Chen et al. - 2022 - Modular cell type organization of cortical areas r.pdf:application/pdf},
}

@misc{lachapelle_partial_2022,
	title = {Partial Disentanglement via Mechanism Sparsity},
	url = {http://arxiv.org/abs/2207.07732},
	doi = {10.48550/arXiv.2207.07732},
	abstract = {Disentanglement via mechanism sparsity was introduced recently as a principled approach to extract latent factors without supervision when the causal graph relating them in time is sparse, and/or when actions are observed and affect them sparsely. However, this theory applies only to ground-truth graphs satisfying a specific criterion. In this work, we introduce a generalization of this theory which applies to any ground-truth graph and specifies qualitatively how disentangled the learned representation is expected to be, via a new equivalence relation over models we call consistency. This equivalence captures which factors are expected to remain entangled and which are not based on the specific form of the ground-truth graph. We call this weaker form of identifiability partial disentanglement. The graphical criterion that allows complete disentanglement, proposed in an earlier work, can be derived as a special case of our theory. Finally, we enforce graph sparsity with constrained optimization and illustrate our theory and algorithm in simulations.},
	number = {{arXiv}:2207.07732},
	publisher = {{arXiv}},
	author = {Lachapelle, Sébastien and Lacoste-Julien, Simon},
	urldate = {2022-12-02},
	date = {2022-07-15},
	eprinttype = {arxiv},
	eprint = {2207.07732 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/TFALF7TP/Lachapelle and Lacoste-Julien - 2022 - Partial Disentanglement via Mechanism Sparsity.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/BSJSTPA8/2207.html:text/html},
}

@misc{moran_identifiable_2022,
	title = {Identifiable Deep Generative Models via Sparse Decoding},
	url = {http://arxiv.org/abs/2110.10804},
	doi = {10.48550/arXiv.2110.10804},
	abstract = {We develop the sparse {VAE} for unsupervised representation learning on high-dimensional data. The sparse {VAE} learns a set of latent factors (representations) which summarize the associations in the observed data features. The underlying model is sparse in that each observed feature (i.e. each dimension of the data) depends on a small subset of the latent factors. As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes. We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned. (In contrast, most deep generative models are not identifiable.) We empirically study the sparse {VAE} with both simulated and real data. We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.},
	number = {{arXiv}:2110.10804},
	publisher = {{arXiv}},
	author = {Moran, Gemma E. and Sridhar, Dhanya and Wang, Yixin and Blei, David M.},
	urldate = {2022-12-02},
	date = {2022-02-17},
	eprinttype = {arxiv},
	eprint = {2110.10804 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AKIPKQWU/Moran et al. - 2022 - Identifiable Deep Generative Models via Sparse Dec.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/TYDLGI4T/2110.html:text/html},
}

@article{lohoff_integration_2022,
	title = {Integration of spatial and single-cell transcriptomic data elucidates mouse organogenesis},
	volume = {40},
	rights = {2021 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-021-01006-2},
	doi = {10.1038/s41587-021-01006-2},
	abstract = {Molecular profiling of single cells has advanced our knowledge of the molecular basis of development. However, current approaches mostly rely on dissociating cells from tissues, thereby losing the crucial spatial context of regulatory processes. Here, we apply an image-based single-cell transcriptomics method, sequential fluorescence in situ hybridization ({seqFISH}), to detect {mRNAs} for 387 target genes in tissue sections of mouse embryos at the 8–12 somite stage. By integrating spatial context and multiplexed transcriptional measurements with two single-cell transcriptome atlases, we characterize cell types across the embryo and demonstrate that spatially resolved expression of genes not profiled by {seqFISH} can be imputed. We use this high-resolution spatial map to characterize fundamental steps in the patterning of the midbrain–hindbrain boundary ({MHB}) and the developing gut tube. We uncover axes of cell differentiation that are not apparent from single-cell {RNA}-sequencing ({scRNA}-seq) data, such as early dorsal–ventral separation of esophageal and tracheal progenitor populations in the gut tube. Our method provides an approach for studying cell fate decisions in complex tissues and development.},
	pages = {74--85},
	number = {1},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotechnol},
	author = {Lohoff, T. and Ghazanfar, S. and Missarova, A. and Koulena, N. and Pierson, N. and Griffiths, J. A. and Bardot, E. S. and Eng, C.-H. L. and Tyser, R. C. V. and Argelaguet, R. and Guibentif, C. and Srinivas, S. and Briscoe, J. and Simons, B. D. and Hadjantonakis, A.-K. and Göttgens, B. and Reik, W. and Nichols, J. and Cai, L. and Marioni, J. C.},
	urldate = {2022-12-02},
	date = {2022-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Computational biology and bioinformatics, Embryogenesis},
	file = {Full Text PDF:/home/ajl/Zotero/storage/UIX5V9B6/Lohoff et al. - 2022 - Integration of spatial and single-cell transcripto.pdf:application/pdf},
}

@misc{de_brebisson_artificial_2015,
	title = {Artificial Neural Networks Applied to Taxi Destination Prediction},
	url = {http://arxiv.org/abs/1508.00021},
	doi = {10.48550/arXiv.1508.00021},
	abstract = {We describe our first-place solution to the {ECML}/{PKDD} discovery challenge on taxi destination prediction. The task consisted in predicting the destination of a taxi based on the beginning of its trajectory, represented as a variable-length sequence of {GPS} points, and diverse associated meta-information, such as the departure time, the driver id and client information. Contrary to most published competitor approaches, we used an almost fully automated approach based on neural networks and we ranked first out of 381 teams. The architectures we tried use multi-layer perceptrons, bidirectional recurrent neural networks and models inspired from recently introduced memory networks. Our approach could easily be adapted to other applications in which the goal is to predict a fixed-length output from a variable-length sequence.},
	number = {{arXiv}:1508.00021},
	publisher = {{arXiv}},
	author = {de Brébisson, Alexandre and Simon, Étienne and Auvolat, Alex and Vincent, Pascal and Bengio, Yoshua},
	urldate = {2022-12-02},
	date = {2015-09-21},
	eprinttype = {arxiv},
	eprint = {1508.00021 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/Y7JITNZH/de Brébisson et al. - 2015 - Artificial Neural Networks Applied to Taxi Destina.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/5US26S5Q/1508.html:text/html},
}

@misc{chew_geometric_2022,
	title = {Geometric Scattering on Measure Spaces},
	url = {http://arxiv.org/abs/2208.08561},
	doi = {10.48550/arXiv.2208.08561},
	abstract = {The scattering transform is a multilayered, wavelet-based transform initially introduced as a model of convolutional neural networks ({CNNs}) that has played a foundational role in our understanding of these networks' stability and invariance properties. Subsequently, there has been widespread interest in extending the success of {CNNs} to data sets with non-Euclidean structure, such as graphs and manifolds, leading to the emerging field of geometric deep learning. In order to improve our understanding of the architectures used in this new field, several papers have proposed generalizations of the scattering transform for non-Euclidean data structures such as undirected graphs and compact Riemannian manifolds without boundary. In this paper, we introduce a general, unified model for geometric scattering on measure spaces. Our proposed framework includes previous work on geometric scattering as special cases but also applies to more general settings such as directed graphs, signed graphs, and manifolds with boundary. We propose a new criterion that identifies to which groups a useful representation should be invariant and show that this criterion is sufficient to guarantee that the scattering transform has desirable stability and invariance properties. Additionally, we consider finite measure spaces that are obtained from randomly sampling an unknown manifold. We propose two methods for constructing a data-driven graph on which the associated graph scattering transform approximates the scattering transform on the underlying manifold. Moreover, we use a diffusion-maps based approach to prove quantitative estimates on the rate of convergence of one of these approximations as the number of sample points tends to infinity. Lastly, we showcase the utility of our method on spherical images, directed graphs, and on high-dimensional single-cell data.},
	number = {{arXiv}:2208.08561},
	publisher = {{arXiv}},
	author = {Chew, Joyce and Hirn, Matthew and Krishnaswamy, Smita and Needell, Deanna and Perlmutter, Michael and Steach, Holly and Viswanath, Siddharth and Wu, Hau-Tieng},
	urldate = {2022-12-02},
	date = {2022-10-13},
	eprinttype = {arxiv},
	eprint = {2208.08561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, 68T07, Mathematics - Spectral Theory},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/K3T88B3I/Chew et al. - 2022 - Geometric Scattering on Measure Spaces.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/E2IJC4UE/2208.html:text/html},
}

@misc{tong_diffusion_2021,
	title = {Diffusion Earth Mover's Distance and Distribution Embeddings},
	url = {http://arxiv.org/abs/2102.12833},
	doi = {10.48550/arXiv.2102.12833},
	abstract = {We propose a new fast method of measuring distances between large numbers of related high dimensional datasets called the Diffusion Earth Mover's Distance ({EMD}). We model the datasets as distributions supported on common data graph that is derived from the affinity matrix computed on the combined data. In such cases where the graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion {EMD} is topologically equivalent to the standard {EMD} with a geodesic ground distance. Diffusion {EMD} can be computed in \${\textbackslash}tilde\{O\}(n)\$ time and is more accurate than similarly fast algorithms such as tree-based {EMDs}. We also show Diffusion {EMD} is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as deep neural networks. Finally, we demonstrate an application of Diffusion {EMD} to single cell data collected from 210 {COVID}-19 patient samples at Yale New Haven Hospital. Here, Diffusion {EMD} can derive distances between patients on the manifold of cells at least two orders of magnitude faster than equally accurate methods. This distance matrix between patients can be embedded into a higher level patient manifold which uncovers structure and heterogeneity in patients. More generally, Diffusion {EMD} is applicable to all datasets that are massively collected in parallel in many medical and biological systems.},
	number = {{arXiv}:2102.12833},
	publisher = {{arXiv}},
	author = {Tong, Alexander and Huguet, Guillaume and Natik, Amine and {MacDonald}, Kincaid and Kuchroo, Manik and Coifman, Ronald and Wolf, Guy and Krishnaswamy, Smita},
	urldate = {2022-12-02},
	date = {2021-07-27},
	eprinttype = {arxiv},
	eprint = {2102.12833 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/HKWGVTAY/Tong et al. - 2021 - Diffusion Earth Mover's Distance and Distribution .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/E77DSBEW/2102.html:text/html},
}

@article{burkhardt_quantifying_2021,
	title = {Quantifying the effect of experimental perturbations at single-cell resolution},
	volume = {39},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-020-00803-5},
	doi = {10.1038/s41587-020-00803-5},
	abstract = {Current methods for comparing single-cell {RNA} sequencing datasets collected in multiple conditions focus on discrete regions of the transcriptional state space, such as clusters of cells. Here we quantify the effects of perturbations at the single-cell level using a continuous measure of the effect of a perturbation across the transcriptomic space. We describe this space as a manifold and develop a relative likelihood estimate of observing each cell in each of the experimental conditions using graph signal processing. This likelihood estimate can be used to identify cell populations specifically affected by a perturbation. We also develop vertex frequency clustering to extract populations of affected cells at the level of granularity that matches the perturbation response. The accuracy of our algorithm at identifying clusters of cells that are enriched or depleted in each condition is, on average, 57\% higher than the next-best-performing algorithm tested. Gene signatures derived from these clusters are more accurate than those of six alternative algorithms in ground truth comparisons.},
	pages = {619--629},
	number = {5},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotechnol},
	author = {Burkhardt, Daniel B. and Stanley, Jay S. and Tong, Alexander and Perdigoto, Ana Luisa and Gigante, Scott A. and Herold, Kevan C. and Wolf, Guy and Giraldez, Antonio J. and van Dijk, David and Krishnaswamy, Smita},
	urldate = {2022-12-02},
	date = {2021-05},
	langid = {english},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {Machine learning, notion, Gene expression},
	file = {Full Text PDF:/home/ajl/Zotero/storage/7REI6U2Q/Burkhardt et al. - 2021 - Quantifying the effect of experimental perturbatio.pdf:application/pdf},
}

@article{cabral_metastable_2022,
	title = {Metastable oscillatory modes emerge from synchronization in the brain spacetime connectome},
	volume = {5},
	rights = {2022 The Author(s)},
	issn = {2399-3650},
	url = {https://www.nature.com/articles/s42005-022-00950-y},
	doi = {10.1038/s42005-022-00950-y},
	abstract = {A rich repertoire of oscillatory signals is detected from human brains with electro- and magnetoencephalography ({EEG}/{MEG}). However, the principles underwriting coherent oscillations and their link with neural activity remain under debate. Here, we revisit the mechanistic hypothesis that transient brain rhythms are a signature of metastable synchronization, occurring at reduced collective frequencies due to delays between brain areas. We consider a system of damped oscillators in the presence of background noise – approximating the short-lived gamma-frequency oscillations generated within neuronal circuits – coupled according to the diffusion weighted tractography between brain areas. Varying the global coupling strength and conduction speed, we identify a critical regime where spatially and spectrally resolved metastable oscillatory modes ({MOMs}) emerge at sub-gamma frequencies, approximating the {MEG} power spectra from 89 healthy individuals at rest. Further, we demonstrate that the frequency, duration, and scale of {MOMs} – as well as the frequency-specific envelope functional connectivity – can be controlled by global parameters, while the connectome structure remains unchanged. Grounded in the physics of delay-coupled oscillators, these numerical analyses demonstrate how interactions between locally generated fast oscillations in the connectome spacetime structure can lead to the emergence of collective brain rhythms organized in space and time.},
	pages = {1--13},
	number = {1},
	journaltitle = {Communications Physics},
	shortjournal = {Commun Phys},
	author = {Cabral, Joana and Castaldo, Francesca and Vohryzek, Jakub and Litvak, Vladimir and Bick, Christian and Lambiotte, Renaud and Friston, Karl and Kringelbach, Morten L. and Deco, Gustavo},
	urldate = {2022-12-02},
	date = {2022-07-15},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Complex networks, Computational biophysics, Nonlinear phenomena, Phase transitions and critical phenomena},
	file = {Full Text PDF:/home/ajl/Zotero/storage/6SH86R6F/Cabral et al. - 2022 - Metastable oscillatory modes emerge from synchroni.pdf:application/pdf},
}

@article{mikulasch_where_2022,
	title = {Where is the error? Hierarchical predictive coding through dendritic error computation},
	volume = {0},
	issn = {0166-2236, 1878-108X},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(22)00186-2},
	doi = {10.1016/j.tins.2022.09.007},
	shorttitle = {Where is the error?},
	number = {0},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Mikulasch, Fabian A. and Rudelt, Lucas and Wibral, Michael and Priesemann, Viola},
	urldate = {2022-12-02},
	date = {2022-11-18},
	note = {Publisher: Elsevier},
	keywords = {notion, cortical hierarchy, inference, predictive processing, pyramidal neuron, sensory processing, voltage-dependent plasticity},
	file = {Full Text PDF:/home/ajl/Zotero/storage/6Y7SUMC9/Mikulasch et al. - 2022 - Where is the error Hierarchical predictive coding.pdf:application/pdf},
}

@misc{luo_one_2022,
	title = {One Transformer Can Understand Both 2D \& 3D Molecular Data},
	url = {http://arxiv.org/abs/2210.01765},
	doi = {10.48550/arXiv.2210.01765},
	abstract = {Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.},
	number = {{arXiv}:2210.01765},
	publisher = {{arXiv}},
	author = {Luo, Shengjie and Chen, Tianlang and Xu, Yixian and Zheng, Shuxin and Liu, Tie-Yan and Wang, Liwei and He, Di},
	urldate = {2022-12-02},
	date = {2022-11-17},
	eprinttype = {arxiv},
	eprint = {2210.01765 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/MY35YUW7/Luo et al. - 2022 - One Transformer Can Understand Both 2D & 3D Molecu.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/LC8RN2QJ/2210.html:text/html},
}

@inproceedings{ahmed_semantic_2022,
	title = {Semantic Probabilistic Layers for Neuro-Symbolic Learning},
	url = {https://openreview.net/forum?id=o-mxIWAY1T8},
	abstract = {We design a predictive layer for structured-output prediction ({SOP}) that can be plugged into any neural network guaranteeing its predictions are consistent with a set of predefined symbolic constraints. Our Semantic Probabilistic Layer ({SPL}) can model intricate correlations, and hard constraints, over a structured output space all while being amenable to end-to-end learning via maximum likelihood. {SPLs} combine exact probabilistic inference with logical reasoning in a clean and modular way, learning complex distributions and restricting their support to solutions of the constraint. As such, they can faithfully, and efficiently, model complex {SOP} tasks beyond the reach of alternative neuro-symbolic approaches. We empirically demonstrate that {SPLs} outperform these competitors in terms of accuracy on challenging {SOP} tasks such as hierarchical multi-label classification, pathfinding and preference learning, while retaining perfect constraint satisfaction.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Ahmed, Kareem and Teso, Stefano and Chang, Kai-Wei and Broeck, Guy Van den and Vergari, Antonio},
	urldate = {2022-12-02},
	date = {2022-10-31},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/LC9DIURC/Ahmed et al. - 2022 - Semantic Probabilistic Layers for Neuro-Symbolic L.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/JKL54NXG/forum.html:text/html},
}

@article{dai_significance_2022,
	title = {Significance tests of feature relevance for a black-box learner},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/2103.04985},
	doi = {10.1109/TNNLS.2022.3185742},
	abstract = {An exciting recent development is the uptake of deep neural networks in many scientific fields, where the main objective is outcome prediction with the black-box nature. Significance testing is promising to address the black-box issue and explore novel scientific insights and interpretation of the decision-making process based on a deep learning model. However, testing for a neural network poses a challenge because of its black-box nature and unknown limiting distributions of parameter estimates while existing methods require strong assumptions or excessive computation. In this article, we derive one-split and two-split tests relaxing the assumptions and computational complexity of existing black-box tests and extending to examine the significance of a collection of features of interest in a dataset of possibly a complex type such as an image. The one-split test estimates and evaluates a black-box model based on estimation and inference subsets through sample splitting and data perturbation. The two-split test further splits the inference subset into two but require no perturbation. Also, we develop their combined versions by aggregating the p-values based on repeated sample splitting. By deflating the bias-sd-ratio, we establish asymptotic null distributions of the test statistics and the consistency in terms of Type {II} error. Numerically, we demonstrate the utility of the proposed tests on seven simulated examples and six real datasets. Accompanying this paper is our Python library dnn-inference (https://dnn-inference.readthedocs.io/en/latest/) that implements the proposed tests.},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	shortjournal = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Dai, Ben and Shen, Xiaotong and Pan, Wei},
	urldate = {2022-12-02},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2103.04985 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ER56VLBD/Dai et al. - 2022 - Significance tests of feature relevance for a blac.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZXLLCPU8/2103.html:text/html},
}

@article{sadtler_neural_2014,
	title = {Neural constraints on learning},
	volume = {512},
	rights = {2014 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature13665},
	doi = {10.1038/nature13665},
	abstract = {During learning, the new patterns of neural population activity that develop are constrained by the existing network structure so that certain patterns can be generated more readily than others.},
	pages = {423--426},
	number = {7515},
	journaltitle = {Nature},
	author = {Sadtler, Patrick T. and Quick, Kristin M. and Golub, Matthew D. and Chase, Steven M. and Ryu, Stephen I. and Tyler-Kabara, Elizabeth C. and Yu, Byron M. and Batista, Aaron P.},
	urldate = {2022-12-02},
	date = {2014-08},
	langid = {english},
	note = {Number: 7515
Publisher: Nature Publishing Group},
	keywords = {notion, Computational neuroscience, Cortex},
	file = {Full Text PDF:/home/ajl/Zotero/storage/XZAZYHLH/Sadtler et al. - 2014 - Neural constraints on learning.pdf:application/pdf},
}

@misc{cohen_towards_2022,
	title = {Towards a Grounded Theory of Causation for Embodied {AI}},
	url = {http://arxiv.org/abs/2206.13973},
	doi = {10.48550/arXiv.2206.13973},
	abstract = {There exist well-developed frameworks for causal modelling, but these require rather a lot of human domain expertise to define causal variables and perform interventions. In order to enable autonomous agents to learn abstract causal models through interactive experience, the existing theoretical foundations need to be extended and clarified. Existing frameworks give no guidance regarding variable choice / representation, and more importantly, give no indication as to which behaviour policies or physical transformations of state space shall count as interventions. The framework sketched in this paper describes actions as transformations of state space, for instance induced by an agent running a policy. This makes it possible to describe in a uniform way both transformations of the micro-state space and abstract models thereof, and say when the latter is veridical / grounded / natural. We then introduce (causal) variables, define a mechanism as an invariant predictor, and say when an action can be viewed as a ``surgical intervention'', thus bringing the objective of causal representation {\textbackslash}\& intervention skill learning into clearer focus.},
	number = {{arXiv}:2206.13973},
	publisher = {{arXiv}},
	author = {Cohen, Taco},
	urldate = {2022-12-02},
	date = {2022-08-12},
	eprinttype = {arxiv},
	eprint = {2206.13973 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/UJVX2ZSS/Cohen - 2022 - Towards a Grounded Theory of Causation for Embodie.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/7BDDNDEN/2206.html:text/html},
}

@misc{lachapelle_disentanglement_2022,
	title = {Disentanglement via Mechanism Sparsity Regularization: A New Principle for Nonlinear {ICA}},
	url = {http://arxiv.org/abs/2107.10098},
	doi = {10.48550/arXiv.2107.10098},
	shorttitle = {Disentanglement via Mechanism Sparsity Regularization},
	abstract = {This work introduces a novel principle we call disentanglement via mechanism sparsity regularization, which can be applied when the latent factors of interest depend sparsely on past latent factors and/or observed auxiliary variables. We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors and the sparse causal graphical model that relates them. We develop a rigorous identifiability theory, building on recent nonlinear independent component analysis ({ICA}) results, that formalizes this principle and shows how the latent variables can be recovered up to permutation if one regularizes the latent mechanisms to be sparse and if some graph connectivity criterion is satisfied by the data generating process. As a special case of our framework, we show how one can leverage unknown-target interventions on the latent factors to disentangle them, thereby drawing further connections between {ICA} and causality. We propose a {VAE}-based method in which the latent mechanisms are learned and regularized via binary masks, and validate our theory by showing it learns disentangled representations in simulations.},
	number = {{arXiv}:2107.10098},
	publisher = {{arXiv}},
	author = {Lachapelle, Sébastien and López, Pau Rodríguez and Sharma, Yash and Everett, Katie and Priol, Rémi Le and Lacoste, Alexandre and Lacoste-Julien, Simon},
	urldate = {2022-12-02},
	date = {2022-02-23},
	eprinttype = {arxiv},
	eprint = {2107.10098 [cs, stat]},
	note = {version: 3},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, I.2.6, I.5.1},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/XYLAERNA/Lachapelle et al. - 2022 - Disentanglement via Mechanism Sparsity Regularizat.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/UK8ALV7C/2107.html:text/html},
}

@article{deng_control_2022,
	title = {Control theory illustrates the energy efficiency in the dynamic reconfiguration of functional connectivity},
	volume = {5},
	rights = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-03196-0},
	doi = {10.1038/s42003-022-03196-0},
	abstract = {The brain’s functional connectivity fluctuates over time instead of remaining steady in a stationary mode even during the resting state. This fluctuation establishes the dynamical functional connectivity that transitions in a non-random order between multiple modes. Yet it remains unexplored how the transition facilitates the entire brain network as a dynamical system and what utility this mechanism for dynamic reconfiguration can bring over the widely used graph theoretical measurements. To address these questions, we propose to conduct an energetic analysis of functional brain networks using resting-state {fMRI} and behavioral measurements from the Human Connectome Project. Through comparing the state transition energy under distinct adjacent matrices, we justify that dynamic functional connectivity leads to 60\% less energy cost to support the resting state dynamics than static connectivity when driving the transition through default mode network. Moreover, we demonstrate that combining graph theoretical measurements and our energy-based control measurements as the feature vector can provide complementary prediction power for the behavioral scores (Combination vs. Control: t = 9.41, p = 1.64e−13; Combination vs. Graph: t = 4.92, p = 3.81e−6). Our approach integrates statistical inference and dynamical system inspection towards understanding brain networks.},
	pages = {1--12},
	number = {1},
	journaltitle = {Communications Biology},
	shortjournal = {Commun Biol},
	author = {Deng, Shikuang and Li, Jingwei and Thomas Yeo, B. T. and Gu, Shi},
	urldate = {2022-12-05},
	date = {2022-04-01},
	langid = {english},
	keywords = {Dynamical systems, Network models},
}

@misc{schulz_how_2022,
	title = {How gut hormones shape reward: a systematic review of the role of ghrelin and {GLP}-1 in human {fMRI}},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.11.30.518539v1},
	doi = {10.1101/2022.11.30.518539},
	shorttitle = {How gut hormones shape reward},
	abstract = {The gastrointestinal hormones ghrelin and glucagon-like peptide-1 ({GLP}-1) have opposite secretion patterns, as well as opposite effects on metabolism and food intake. Beyond their role in energy homeostasis, gastrointestinal hormones have also been suggested to modulate the reward system. However, the potential of ghrelin and {GLP}-1 to modulate reward responses in humans has not been systematically reviewed before. To evaluate the convergence of published results, we first conduct a multi-level kernel density meta-analysis of studies reporting a positive association of ghrelin (Ncomb = 353, 18 contrasts) and a negative association of {GLP}-1 (Ncomb = 258, 12 contrasts) and reward responses measured using task functional magnetic resonance imaging ({fMRI}). Second, we complement the meta-analysis using a systematic literature review, focusing on distinct reward phases and applications in clinical populations that may account for variability across studies. In line with preclinical research, we find that ghrelin increases reward responses across studies in key nodes of the motivational circuit, such as the nucleus accumbens, pallidum, putamen, substantia nigra, ventral tegmental area, and the dorsal mid insula. In contrast, for {GLP}-1, we did not find sufficient convergence in support of reduced reward responses. Instead, our systematic review identifies potential differences of {GLP}-1 on anticipatory versus consummatory reward responses. Based on a systematic synthesis of available findings, we conclude that there is considerable support for the neuromodulatory potential of gut-based circulating peptides on reward responses. To unlock their potential for clinical applications, future studies may move beyond anticipated rewards to cover other reward facets.
{\textless}img class="highwire-fragment fragment-image" alt="Figure" src="https://www.biorxiv.org/content/biorxiv/early/2022/12/01/2022.11.30.518539/F1.medium.gif" width="440" height="239"/{\textgreater}Download {figureOpen} in new tab},
	publisher = {{bioRxiv}},
	author = {Schulz, Corinna and Vezzani, Cecilia and Kroemer, Nils B.},
	urldate = {2022-12-05},
	date = {2022-12-01},
	langid = {english},
}

@misc{zhang_adam_2022,
	title = {Adam Can Converge Without Any Modification On Update Rules},
	url = {http://arxiv.org/abs/2208.09632},
	doi = {10.48550/arXiv.2208.09632},
	abstract = {Ever since Reddi et al. 2018 pointed out the divergence issue of Adam, many new variants have been designed to obtain convergence. However, vanilla Adam remains exceptionally popular and it works well in practice. Why is there a gap between theory and practice? We point out there is a mismatch between the settings of theory and practice: Reddi et al. 2018 pick the problem after picking the hyperparameters of Adam, i.e., \$({\textbackslash}beta\_1, {\textbackslash}beta\_2)\$; while practical applications often fix the problem first and then tune \$({\textbackslash}beta\_1, {\textbackslash}beta\_2)\$. Due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter. In this work, we confirm this conjecture. We prove that, when \${\textbackslash}beta\_2\$ is large and \${\textbackslash}beta\_1 {\textless} {\textbackslash}sqrt\{{\textbackslash}beta\_2\}{\textless}1\$, Adam converges to the neighborhood of critical points. The size of the neighborhood is propositional to the variance of stochastic gradients. Under an extra condition (strong growth condition), Adam converges to critical points. It is worth mentioning that our results cover a wide range of hyperparameters: as \${\textbackslash}beta\_2\$ increases, our convergence result can cover any \${\textbackslash}beta\_1 {\textbackslash}in [0,1)\$ including \${\textbackslash}beta\_1=0.9\$, which is the default setting in deep learning libraries. To our knowledge, this is the first result showing that Adam can converge without any modification on its update rules. Further, our analysis does not require assumptions of bounded gradients or bounded 2nd-order momentum. When \${\textbackslash}beta\_2\$ is small, we further point out a large region of \$({\textbackslash}beta\_1,{\textbackslash}beta\_2)\$ where Adam can diverge to infinity. Our divergence result considers the same setting as our convergence result, indicating a phase transition from divergence to convergence when increasing \${\textbackslash}beta\_2\$. These positive and negative results can provide suggestions on how to tune Adam hyperparameters.},
	number = {{arXiv}:2208.09632},
	publisher = {{arXiv}},
	author = {Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
	urldate = {2022-12-05},
	date = {2022-10-29},
	eprinttype = {arxiv},
	eprint = {2208.09632 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{iyer_self-supervised_2022,
	title = {Self-Supervised Pretraining Enables High-Performance Chest X-Ray Interpretation Across Clinical Distributions},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2022.11.19.22282519v1},
	doi = {10.1101/2022.11.19.22282519},
	abstract = {Chest X-rays ({CXRs}) are a rich source of information for physicians – essential for disease diagnosis and treatment selection. Recent deep learning models aim to alleviate strain on medical resources and improve patient care by automating the detection of diseases from {CXRs}. However, shortages of labeled {CXRs} can pose a serious challenge when training models. Currently, models are generally pretrained on {ImageNet}, but they often need to then be finetuned on hundreds of thousands of labeled {CXRs} to achieve high performance. Therefore, the current approach to model development is not viable on tasks with only a small amount of labeled data. An emerging method for reducing reliance on large amounts of labeled data is self-supervised learning ({SSL}), which uses unlabeled {CXR} datasets to automatically learn features that can be leveraged for downstream interpretation tasks. In this work, we investigated whether self-supervised pretraining methods could outperform traditional {ImageNet} pretraining for chest X-ray interpretation. We found that {SSL}-pretrained models outperformed {ImageNet}-pretrained models on thirteen different datasets representing high diversity in geographies, clinical settings, and prediction tasks. We thus show that {SSL} on unlabeled {CXR} data is a promising pretraining approach for a wide variety of {CXR} interpretation tasks, enabling a shift away from costly labeled datasets.},
	publisher = {{medRxiv}},
	author = {Iyer, Niveditha S. and Gulati, Aditya and Banerjee, Oishi and Logé, Cécile and Farhat, Maha and Saenz, Agustina D. and Rajpurkar, Pranav},
	urldate = {2022-12-05},
	date = {2022-11-25},
	langid = {english},
}

@misc{bharadhwaj_auditing_2021,
	title = {Auditing {AI} models for Verified Deployment under Semantic Specifications},
	url = {http://arxiv.org/abs/2109.12456},
	doi = {10.48550/arXiv.2109.12456},
	abstract = {Auditing trained deep learning ({DL}) models prior to deployment is vital for preventing unintended consequences. One of the biggest challenges in auditing is the lack of human-interpretable specifications for the {DL} models that are directly useful to the auditor. We address this challenge through a sequence of semantically-aligned unit tests, where each unit test verifies whether a predefined specification (e.g., accuracy over 95\%) is satisfied with respect to controlled and semantically aligned variations in the input space (e.g., in face recognition, the angle relative to the camera). We enable such unit tests through variations in a semantically-interpretable latent space of a generative model. Further, we conduct certified training for the {DL} model through a shared latent space representation with the generative model. With evaluations on four different datasets, covering images of chest X-rays, human faces, {ImageNet} classes, and towers, we show how {AuditAI} allows us to obtain controlled variations for certified training. Thus, our framework, {AuditAI}, bridges the gap between semantically-aligned formal verification and scalability. A blog post accompanying the paper is at this link https://developer.nvidia.com/blog/nvidia-research-auditing-ai-models-for-verified-deployment-under-semantic-specifications},
	number = {{arXiv}:2109.12456},
	publisher = {{arXiv}},
	author = {Bharadhwaj, Homanga and Huang, De-An and Xiao, Chaowei and Anandkumar, Anima and Garg, Animesh},
	urldate = {2022-12-05},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {2109.12456 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@article{lipkova_artificial_2022-1,
	title = {Artificial intelligence for multimodal data integration in oncology},
	volume = {40},
	issn = {1535-6108},
	url = {https://www.sciencedirect.com/science/article/pii/S153561082200441X},
	doi = {10.1016/j.ccell.2022.09.012},
	abstract = {In oncology, the patient state is characterized by a whole spectrum of modalities, ranging from radiology, histology, and genomics to electronic health records. Current artificial intelligence ({AI}) models operate mainly in the realm of a single modality, neglecting the broader clinical context, which inevitably diminishes their potential. Integration of different data modalities provides opportunities to increase robustness and accuracy of diagnostic and prognostic models, bringing {AI} closer to clinical practice. {AI} models are also capable of discovering novel patterns within and across modalities suitable for explaining differences in patient outcomes or treatment resistance. The insights gleaned from such models can guide exploration studies and contribute to the discovery of novel biomarkers and therapeutic targets. To support these advances, here we present a synopsis of {AI} methods and strategies for multimodal data fusion and association discovery. We outline approaches for {AI} interpretability and directions for {AI}-driven exploration through multimodal data interconnections. We examine challenges in clinical adoption and discuss emerging solutions.},
	pages = {1095--1110},
	number = {10},
	journaltitle = {Cancer Cell},
	shortjournal = {Cancer Cell},
	author = {Lipkova, Jana and Chen, Richard J. and Chen, Bowen and Lu, Ming Y. and Barbieri, Matteo and Shao, Daniel and Vaidya, Anurag J. and Chen, Chengkuan and Zhuang, Luoting and Williamson, Drew F. K. and Shaban, Muhammad and Chen, Tiffany Y. and Mahmood, Faisal},
	urldate = {2022-12-05},
	date = {2022-10-10},
	langid = {english},
	keywords = {deep learning, notion, {AI} in oncology, deep learning in oncology, multimodal {AI}, multimodal fusion, multimodal integration},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/PGN98IUG/Lipkova et al. - 2022 - Artificial intelligence for multimodal data integr.pdf:application/pdf},
}

@misc{giunchiglia_towards_2022,
	title = {Towards Training {GNNs} using Explanation Directed Message Passing},
	url = {http://arxiv.org/abs/2211.16731},
	doi = {10.48550/arXiv.2211.16731},
	abstract = {With the increasing use of Graph Neural Networks ({GNNs}) in critical real-world applications, several post hoc explanation methods have been proposed to understand their predictions. However, there has been no work in generating explanations on the fly during model training and utilizing them to improve the expressive power of the underlying {GNN} models. In this work, we introduce a novel explanation-directed neural message passing framework for {GNNs}, {EXPASS} ({EXplainable} message {PASSing}), which aggregates only embeddings from nodes and edges identified as important by a {GNN} explanation method. {EXPASS} can be used with any existing {GNN} architecture and subgraph-optimizing explainer to learn accurate graph embeddings. We theoretically show that {EXPASS} alleviates the oversmoothing problem in {GNNs} by slowing the layer wise loss of Dirichlet energy and that the embedding difference between the vanilla message passing and {EXPASS} framework can be upper bounded by the difference of their respective model weights. Our empirical results show that graph embeddings learned using {EXPASS} improve the predictive performance and alleviate the oversmoothing problems of {GNNs}, opening up new frontiers in graph machine learning to develop explanation-based training frameworks.},
	number = {{arXiv}:2211.16731},
	publisher = {{arXiv}},
	author = {Giunchiglia, Valentina and Shukla, Chirag Varun and Gonzalez, Guadalupe and Agarwal, Chirag},
	urldate = {2022-12-05},
	date = {2022-12-01},
	eprinttype = {arxiv},
	eprint = {2211.16731 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@online{noauthor_notitle_nodate,
	url = {https://academic.oup.com/HTTPHandlers/Sigma/LoginHandler.ashx?error=login_required&state=55fda9a4-f462-4408-84a7-52c48b93877eredirecturl%3Dhttpszazjzjacademiczwoupzwcomzjbioinformaticszjadvancezyarticlezjdoizj10zw1093zjbioinformaticszjbtac775zj6865029},
	urldate = {2022-12-05},
}

@article{dou_bi-order_2022,
	title = {Bi-order multimodal integration of single-cell data},
	volume = {23},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-022-02679-x},
	doi = {10.1186/s13059-022-02679-x},
	abstract = {Integration of single-cell multiomics profiles generated by different single-cell technologies from the same biological sample is still challenging. Previous approaches based on shared features have only provided approximate solutions. Here, we present a novel mathematical solution named bi-order canonical correlation analysis (bi-{CCA}), which extends the widely used {CCA} approach to iteratively align the rows and the columns between data matrices. Bi-{CCA} is generally applicable to combinations of any two single-cell modalities. Validations using co-assayed ground truth data and application to a {CAR}-{NK} study and a fetal muscle atlas demonstrate its capability in generating accurate multimodal co-embeddings and discovering cellular identity.},
	pages = {112},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Dou, Jinzhuang and Liang, Shaoheng and Mohanty, Vakul and Miao, Qi and Huang, Yuefan and Liang, Qingnan and Cheng, Xuesen and Kim, Sangbae and Choi, Jongsu and Li, Yumei and Li, Li and Daher, May and Basar, Rafet and Rezvani, Katayoun and Chen, Rui and Chen, Ken},
	urldate = {2022-12-05},
	date = {2022-05-09},
	keywords = {notion, Bi-order canonical correlation analysis, Cell type identity, Single-cell multi-omics},
	file = {Full Text PDF:/home/ajl/Zotero/storage/4IUSUTCP/Dou et al. - 2022 - Bi-order multimodal integration of single-cell dat.pdf:application/pdf},
}

@article{browaeys_nichenet_2020,
	title = {{NicheNet}: modeling intercellular communication by linking ligands to target genes},
	volume = {17},
	rights = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-019-0667-5},
	doi = {10.1038/s41592-019-0667-5},
	shorttitle = {{NicheNet}},
	abstract = {Computational methods that model how gene expression of a cell is influenced by interacting cells are lacking. We present {NicheNet} (https://github.com/saeyslab/nichenetr), a method that predicts ligand–target links between interacting cells by combining their expression data with prior knowledge on signaling and gene regulatory networks. We applied {NicheNet} to tumor and immune cell microenvironment data and demonstrate that {NicheNet} can infer active ligands and their gene regulatory effects on interacting cells.},
	pages = {159--162},
	number = {2},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Browaeys, Robin and Saelens, Wouter and Saeys, Yvan},
	urldate = {2022-12-05},
	date = {2020-02},
	langid = {english},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {notion, Computational biology and bioinformatics, Gene expression analysis, Systems biology},
	file = {Full Text PDF:/home/ajl/Zotero/storage/4PKNS94R/Browaeys et al. - 2020 - NicheNet modeling intercellular communication by .pdf:application/pdf},
}

@article{almet_landscape_2021,
	title = {The landscape of cell–cell communication through single-cell transcriptomics},
	volume = {26},
	issn = {2452-3100},
	url = {https://www.sciencedirect.com/science/article/pii/S2452310021000081},
	doi = {10.1016/j.coisb.2021.03.007},
	abstract = {Cell–cell communication is a fundamental process that shapes biological tissue. Historically, studies of cell–cell communication have been feasible for one or two cell types and a few genes. With the emergence of single-cell transcriptomics, we are now able to examine the genetic profiles of individual cells at unprecedented scale and depth. The availability of such data presents an exciting opportunity to construct a more comprehensive description of cell–cell communication. This review discusses the recent explosion of methods that have been developed to infer cell–cell communication from non-spatial single-cell and spatial transcriptomics, two promising technologies that have complementary strengths and limitations. We propose several avenues to propel this rapidly expanding field forward in meaningful ways.},
	pages = {12--23},
	journaltitle = {Current Opinion in Systems Biology},
	shortjournal = {Current Opinion in Systems Biology},
	author = {Almet, Axel A. and Cang, Zixuan and Jin, Suoqin and Nie, Qing},
	urldate = {2022-12-05},
	date = {2021-06-01},
	langid = {english},
	keywords = {notion, Cell signaling, Cell–cell interactions, Inference, Intercellular communication, Ligand–receptor interactions, Signaling networks, Single-cell {RNA}-Seq, Spatial transcriptomics},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/RZRDMVGR/Almet et al. - 2021 - The landscape of cell–cell communication through s.pdf:application/pdf},
}

@article{walker_deciphering_2022,
	title = {Deciphering tissue structure and function using spatial transcriptomics},
	volume = {5},
	rights = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-03175-5},
	doi = {10.1038/s42003-022-03175-5},
	abstract = {The rapid development of spatial transcriptomics ({ST}) techniques has allowed the measurement of transcriptional levels across many genes together with the spatial positions of cells. This has led to an explosion of interest in computational methods and techniques for harnessing both spatial and transcriptional information in analysis of {ST} datasets. The wide diversity of approaches in aim, methodology and technology for {ST} provides great challenges in dissecting cellular functions in spatial contexts. Here, we synthesize and review the key problems in analysis of {ST} data and methods that are currently applied, while also expanding on open questions and areas of future development.},
	pages = {1--10},
	number = {1},
	journaltitle = {Communications Biology},
	shortjournal = {Commun Biol},
	author = {Walker, Benjamin L. and Cang, Zixuan and Ren, Honglei and Bourgain-Chang, Eric and Nie, Qing},
	urldate = {2022-12-05},
	date = {2022-03-10},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Computational biology and bioinformatics, Systems biology},
	file = {Full Text PDF:/home/ajl/Zotero/storage/VTFSUA7A/Walker et al. - 2022 - Deciphering tissue structure and function using sp.pdf:application/pdf},
}

@article{murdoch_definitions_2019,
	title = {Definitions, methods, and applications in interpretable machine learning},
	volume = {116},
	url = {https://www.pnas.org/doi/10.1073/pnas.1900654116},
	doi = {10.1073/pnas.1900654116},
	abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant ({PDR}) framework for discussing interpretations. The {PDR} framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the {PDR} framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	pages = {22071--22080},
	number = {44},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
	urldate = {2022-12-05},
	date = {2019-10-29},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/7I78RBJX/Murdoch et al. - 2019 - Definitions, methods, and applications in interpre.pdf:application/pdf},
}

@article{yu_veridical_2020,
	title = {Veridical data science},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.1901326117},
	doi = {10.1073/pnas.1901326117},
	abstract = {Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability ({PCS}) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The {PCS} workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the {PCS} workflow, we develop {PCS} inference procedures, namely {PCS} perturbation intervals and {PCS} hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate {PCS} inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic ({ROC}) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose {PCS} documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The {PCS} workflow and documentation are demonstrated in a genomics case study available on Zenodo.},
	pages = {3920--3929},
	number = {8},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Yu, Bin and Kumbier, Karl},
	urldate = {2022-12-05},
	date = {2020-02-25},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3GBQY4WP/Yu and Kumbier - 2020 - Veridical data science.pdf:application/pdf},
}

@article{moses_museum_2022,
	title = {Museum of spatial transcriptomics},
	volume = {19},
	rights = {2022 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01409-2},
	doi = {10.1038/s41592-022-01409-2},
	abstract = {The function of many biological systems, such as embryos, liver lobules, intestinal villi, and tumors, depends on the spatial organization of their cells. In the past decade, high-throughput technologies have been developed to quantify gene expression in space, and computational methods have been developed that leverage spatial gene expression data to identify genes with spatial patterns and to delineate neighborhoods within tissues. To comprehensively document spatial gene expression technologies and data-analysis methods, we present a curated review of literature on spatial transcriptomics dating back to 1987, along with a thorough analysis of trends in the field, such as usage of experimental techniques, species, tissues studied, and computational approaches used. Our Review places current methods in a historical context, and we derive insights about the field that can guide current research strategies. A companion supplement offers a more detailed look at the technologies and methods analyzed: https://pachterlab.github.io/{LP}\_2021/.},
	pages = {534--546},
	number = {5},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Moses, Lambda and Pachter, Lior},
	urldate = {2022-12-05},
	date = {2022-05},
	langid = {english},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {notion, Fluorescence in situ hybridization, {RNA} sequencing, Software},
	file = {Full Text PDF:/home/ajl/Zotero/storage/CHVGXDZJ/Moses and Pachter - 2022 - Museum of spatial transcriptomics.pdf:application/pdf},
}

@article{ma_belayer_2022,
	title = {Belayer: Modeling discrete and continuous spatial variation in gene expression from spatially resolved transcriptomics},
	volume = {13},
	issn = {2405-4712},
	url = {https://www.cell.com/cell-systems/abstract/S2405-4712(22)00354-4},
	doi = {10.1016/j.cels.2022.09.002},
	shorttitle = {Belayer},
	pages = {786--797.e13},
	number = {10},
	journaltitle = {Cell Systems},
	shortjournal = {cels},
	author = {Ma, Cong and Chitra, Uthsav and Zhang, Shirley and Raphael, Benjamin J.},
	urldate = {2022-12-05},
	date = {2022-10-19},
	pmid = {36265465},
	note = {Publisher: Elsevier},
	keywords = {notion, conformal maps, gene expression, layered tissues, segmented regression, spatial variation, spatially resolved transcriptomics},
	file = {Snapshot:/home/ajl/Zotero/storage/ZKRSBIVN/S2405-4712(22)00354-4.html:text/html;Submitted Version:/home/ajl/Zotero/storage/A6WKMST6/Ma et al. - 2022 - Belayer Modeling discrete and continuous spatial .pdf:application/pdf},
}

@article{pasquini_dynamic_nodate-1,
	title = {Dynamic autonomic nervous system states arise during emotions and manifest in basal physiology},
	volume = {n/a},
	issn = {1469-8986},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/psyp.14218},
	doi = {10.1111/psyp.14218},
	abstract = {The outflow of the autonomic nervous system ({ANS}) is continuous and dynamic, but its functional organization is not well understood. Whether {ANS} patterns accompany emotions, or arise in basal physiology, remain unsettled questions in the field. Here, we searched for brief {ANS} patterns amidst continuous, multichannel physiological recordings in 45 healthy older adults. Participants completed an emotional reactivity task in which they viewed video clips that elicited a target emotion (awe, sadness, amusement, disgust, or nurturant love); each video clip was preceded by a pre-trial baseline period and followed by a post-trial recovery period. Participants also sat quietly for a separate 2-min resting period to assess basal physiology. Using principal components analysis and unsupervised clustering algorithms to reduce the second-by-second physiological data during the emotional reactivity task, we uncovered five {ANS} states. Each {ANS} state was characterized by a unique constellation of patterned physiological changes that differentiated among the trials of the emotional reactivity task. These {ANS} states emerged and dissipated over time, with each instance lasting several seconds on average. {ANS} states with similar structures were also detectable in the resting period but were intermittent and of smaller magnitude. Our results offer new insights into the functional organization of the {ANS}. By assembling short-lived, patterned changes, the {ANS} is equipped to generate a wide range of physiological states that accompany emotions and that contribute to the architecture of basal physiology.},
	pages = {e14218},
	issue = {n/a},
	journaltitle = {Psychophysiology},
	author = {Pasquini, Lorenzo and Noohi, Fatemeh and Veziris, Christina R. and Kosik, Eena L. and Holley, Sarah R. and Lee, Alex and Brown, Jesse A. and Roy, Ashlin R. K. and Chow, Tiffany E. and Allen, Isabel and Rosen, Howard J. and Kramer, Joel H. and Miller, Bruce L. and Saggar, Manish and Seeley, William W. and Sturm, Virginia E.},
	urldate = {2022-12-05},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/psyp.14218},
	keywords = {autonomic nervous system, baseline, dynamic systems, emotions, physiology, resting state, notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/NZ8U8YIB/Pasquini et al. - Dynamic autonomic nervous system states arise duri.pdf:application/pdf;Snapshot:/home/ajl/Zotero/storage/EZW4GEPE/psyp.html:text/html},
}

@misc{lan_adsorbml_2022,
	title = {{AdsorbML}: Accelerating Adsorption Energy Calculations with Machine Learning},
	url = {http://arxiv.org/abs/2211.16486},
	doi = {10.48550/arXiv.2211.16486},
	shorttitle = {{AdsorbML}},
	abstract = {Computational catalysis is playing an increasingly significant role in the design of catalysts across a wide range of applications. A common task for many computational methods is the need to accurately compute the minimum binding energy - the adsorption energy - for an adsorbate and a catalyst surface of interest. Traditionally, the identification of low energy adsorbate-surface configurations relies on heuristic methods and researcher intuition. As the desire to perform high-throughput screening increases, it becomes challenging to use heuristics and intuition alone. In this paper, we demonstrate machine learning potentials can be leveraged to identify low energy adsorbate-surface configurations more accurately and efficiently. Our algorithm provides a spectrum of trade-offs between accuracy and efficiency, with one balanced option finding the lowest energy configuration, within a 0.1 {eV} threshold, 86.63\% of the time, while achieving a 1387x speedup in computation. To standardize benchmarking, we introduce the Open Catalyst Dense dataset containing nearly 1,000 diverse surfaces and 87,045 unique configurations.},
	number = {{arXiv}:2211.16486},
	publisher = {{arXiv}},
	author = {Lan, Janice and Palizhati, Aini and Shuaibi, Muhammed and Wood, Brandon M. and Wander, Brook and Das, Abhishek and Uyttendaele, Matt and Zitnick, C. Lawrence and Ulissi, Zachary W.},
	urldate = {2022-12-07},
	date = {2022-11-29},
	eprinttype = {arxiv},
	eprint = {2211.16486 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science},
}

@misc{ying_hierarchical_2019,
	title = {Hierarchical Graph Representation Learning with Differentiable Pooling},
	url = {http://arxiv.org/abs/1806.08804},
	doi = {10.48550/arXiv.1806.08804},
	abstract = {Recently, graph neural networks ({GNNs}) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current {GNN} methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose {DiffPool}, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. {DiffPool} learns a differentiable soft cluster assignment for nodes at each layer of a deep {GNN}, mapping nodes to a set of clusters, which then form the coarsened input for the next {GNN} layer. Our experimental results show that combining existing {GNN} methods with {DiffPool} yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	number = {{arXiv}:1806.08804},
	publisher = {{arXiv}},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	urldate = {2022-12-07},
	date = {2019-02-20},
	eprinttype = {arxiv},
	eprint = {1806.08804 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks},
}

@article{tarashansky_self-assembling_2019,
	title = {Self-assembling manifolds in single-cell {RNA} sequencing data},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.48994},
	doi = {10.7554/eLife.48994},
	abstract = {Single-cell {RNA} sequencing has spurred the development of computational methods that enable researchers to classify cell types, delineate developmental trajectories, and measure molecular responses to external perturbations. Many of these technologies rely on their ability to detect genes whose cell-to-cell variations arise from the biological processes of interest rather than transcriptional or technical noise. However, for datasets in which the biologically relevant differences between cells are subtle, identifying these genes is challenging. We present the self-assembling manifold ({SAM}) algorithm, an iterative soft feature selection strategy to quantify gene relevance and improve dimensionality reduction. We demonstrate its advantages over other state-of-the-art methods with experimental validation in identifying novel stem cell populations of Schistosoma mansoni, a prevalent parasite that infects hundreds of millions of people. Extending our analysis to a total of 56 datasets, we show that {SAM} is generalizable and consistently outperforms other methods in a variety of biological and quantitative benchmarks.},
	pages = {e48994},
	journaltitle = {{eLife}},
	author = {Tarashansky, Alexander J and Xue, Yuan and Li, Pengyang and Quake, Stephen R and Wang, Bo},
	editor = {Shalek, Alex K and Barkai, Naama and Yanai, Itai},
	urldate = {2022-12-07},
	date = {2019-09-16},
	keywords = {feature selection, manifold reconstruction, schistosome, single-cell analysis, stem cells},
}

@misc{keller_topographic_2022,
	title = {Topographic {VAEs} learn Equivariant Capsules},
	url = {http://arxiv.org/abs/2109.01394},
	doi = {10.48550/arXiv.2109.01394},
	abstract = {In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic {VAE}: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on {MNIST}. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences -- a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. "capsules") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.},
	number = {{arXiv}:2109.01394},
	publisher = {{arXiv}},
	author = {Keller, T. Anderson and Welling, Max},
	urldate = {2022-12-07},
	date = {2022-01-09},
	eprinttype = {arxiv},
	eprint = {2109.01394 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@misc{bardes_vicreg_2022,
	title = {{VICReg}: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
	url = {http://arxiv.org/abs/2105.04906},
	doi = {10.48550/arXiv.2105.04906},
	shorttitle = {{VICReg}},
	abstract = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce {VICReg} (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. {VICReg} combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
	number = {{arXiv}:2105.04906},
	publisher = {{arXiv}},
	author = {Bardes, Adrien and Ponce, Jean and {LeCun}, Yann},
	urldate = {2022-12-07},
	date = {2022-01-28},
	eprinttype = {arxiv},
	eprint = {2105.04906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SPECWXHA/Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/9HLESTDT/2105.html:text/html},
}

@misc{hammernik_physics-driven_2022,
	title = {Physics-Driven Deep Learning for Computational Magnetic Resonance Imaging},
	url = {http://arxiv.org/abs/2203.12215},
	doi = {10.48550/arXiv.2203.12215},
	abstract = {Physics-driven deep learning methods have emerged as a powerful tool for computational magnetic resonance imaging ({MRI}) problems, pushing reconstruction performance to new limits. This article provides an overview of the recent developments in incorporating physics information into learning-based {MRI} reconstruction. We consider inverse problems with both linear and non-linear forward models for computational {MRI}, and review the classical approaches for solving these. We then focus on physics-driven deep learning approaches, covering physics-driven loss functions, plug-and-play methods, generative models, and unrolled networks. We highlight domain-specific challenges such as real- and complex-valued building blocks of neural networks, and translational applications in {MRI} with linear and non-linear forward models. Finally, we discuss common issues and open challenges, and draw connections to the importance of physics-driven learning when combined with other downstream tasks in the medical imaging pipeline.},
	number = {{arXiv}:2203.12215},
	publisher = {{arXiv}},
	author = {Hammernik, Kerstin and Küstner, Thomas and Yaman, Burhaneddin and Huang, Zhengnan and Rueckert, Daniel and Knoll, Florian and Akçakaya, Mehmet},
	urldate = {2022-12-07},
	date = {2022-10-13},
	eprinttype = {arxiv},
	eprint = {2203.12215 [physics]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, Physics - Medical Physics},
}

@misc{razborov_improved_2022,
	title = {Improved Convergence Guarantees for Shallow Neural Networks},
	url = {http://arxiv.org/abs/2212.02323},
	doi = {10.48550/arXiv.2212.02323},
	abstract = {We continue a long line of research aimed at proving convergence of depth 2 neural networks, trained via gradient descent, to a global minimum. Like in many previous works, our model has the following features: regression with quadratic loss function, fully connected feedforward architecture, {RelU} activations, Gaussian data instances and network initialization, adversarial labels. It is more general in the sense that we allow both layers to be trained simultaneously and at \{{\textbackslash}em different\} rates. Our results improve on state-of-the-art [Oymak Soltanolkotabi 20] (training the first layer only) and [Nguyen 21, Section 3.2] (training both layers with Le Cun's initialization). We also report several simple experiments with synthetic data. They strongly suggest that, at least in our model, the convergence phenomenon extends well beyond the ``{NTK} regime''.},
	number = {{arXiv}:2212.02323},
	publisher = {{arXiv}},
	author = {Razborov, Alexander},
	urldate = {2022-12-07},
	date = {2022-12-05},
	eprinttype = {arxiv},
	eprint = {2212.02323 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{li_graph_2022,
	title = {Graph representation learning in biomedicine and healthcare},
	rights = {2022 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-022-00942-x},
	doi = {10.1038/s41551-022-00942-x},
	abstract = {Networks—or graphs—are universal descriptors of systems of interacting elements. In biomedicine and healthcare, they can represent, for example, molecular interactions, signalling pathways, disease co-morbidities or healthcare systems. In this Perspective, we posit that representation learning can realize principles of network medicine, discuss successes and current limitations of the use of representation learning on graphs in biomedicine and healthcare, and outline algorithmic strategies that leverage the topology of graphs to embed them into compact vectorial spaces. We argue that graph representation learning will keep pushing forward machine learning for biomedicine and healthcare applications, including the identification of genetic variants underlying complex traits, the disentanglement of single-cell behaviours and their effects on health, the assistance of patients in diagnosis and treatment, and the development of safe and effective medicines.},
	pages = {1--17},
	journaltitle = {Nature Biomedical Engineering},
	shortjournal = {Nat. Biomed. Eng},
	author = {Li, Michelle M. and Huang, Kexin and Zitnik, Marinka},
	urldate = {2022-12-07},
	date = {2022-10-31},
	langid = {english},
	keywords = {Machine learning, Systems biology, Health care, Molecular medicine, Network topology},
}

@misc{soin_chexstray_2022,
	title = {{CheXstray}: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging {AI}},
	url = {http://arxiv.org/abs/2202.02833},
	doi = {10.48550/arXiv.2202.02833},
	shorttitle = {{CheXstray}},
	abstract = {Clinical Artificial lntelligence ({AI}) applications are rapidly expanding worldwide, and have the potential to impact to all areas of medical practice. Medical imaging applications constitute a vast majority of approved clinical {AI} applications. Though healthcare systems are eager to adopt {AI} solutions a fundamental question remains: {\textbackslash}textit\{what happens after the {AI} model goes into production?\} We use the {CheXpert} and {PadChest} public datasets to build and test a medical imaging {AI} drift monitoring workflow to track data and model drift without contemporaneous ground truth. We simulate drift in multiple experiments to compare model performance with our novel multi-modal drift metric, which uses {DICOM} metadata, image appearance representation from a variational autoencoder ({VAE}), and model output probabilities as input. Through experimentation, we demonstrate a strong proxy for ground truth performance using unsupervised distributional shifts in relevant metadata, predicted probabilities, and {VAE} latent representation. Our key contributions include (1) proof-of-concept for medical imaging drift detection that includes the use of {VAE} and domain specific statistical methods, (2) a multi-modal methodology to measure and unify drift metrics, (3) new insights into the challenges and solutions to observe deployed medical imaging {AI}, and (4) creation of open-source tools that enable others to easily run their own workflows and scenarios. This work has important implications. It addresses the concerning translation gap found in continuous medical imaging {AI} model monitoring common in dynamic healthcare environments.},
	number = {{arXiv}:2202.02833},
	publisher = {{arXiv}},
	author = {Soin, Arjun and Merkow, Jameson and Long, Jin and Cohen, Joseph Paul and Saligrama, Smitha and Kaiser, Stephen and Borg, Steven and Tarapov, Ivan and Lungren, Matthew P.},
	urldate = {2022-12-07},
	date = {2022-03-17},
	eprinttype = {arxiv},
	eprint = {2202.02833 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{bandyopadhyay_mean-field_2022,
	title = {Mean-field approximation of network of biophysical neurons driven by conductance-based ion exchange},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2021.10.29.466427v4},
	doi = {10.1101/2021.10.29.466427},
	abstract = {Numerous network and whole-brain modeling approaches use mean-field models, facilitating the study of dynamics due to their relative simplicity. They correspond to lumped descriptions of neuronal assemblies connected via synapses. Mean-field models do not usually consider the ionic composition of the extracellular space, which can change in physiological and pathological conditions, with strong effects on neuronal activity. Here we derive a mean-field model of a population of Hodgkin–Huxley type neurons, which links the neuronal intra- and extra-cellular ion concentrations to the mean membrane potential and the mean synaptic input in terms of the synaptic conductance. Thus, the model provides a mean-field approximation, for locally homogeneous mesoscopic networks of biophysical neurons driven by an ion-exchange mechanism. The model can generate various physiological brain activities, including multi-stability during simulated healthy states, pathological spiking, bursting behaviors, and depolarization block. The results from the analytical solution of the mean-field model agree with the mean behavior of numerical simulations of large-scale networks of strongly synchronized neurons. The mean-field model exhibits emergent activity regimes qualitatively similar to those observed in weakly synchronized neuronal networks and experimentally observed in-vitro. This approach maintains a detailed biophysical level of description, such as the evolution of ionic concentrations while describing dynamics at the neural mass scale. Hence, these results may provide the missing link between high-level neural mass approaches, used in brain network modeling, and physiological parameters that drive the neuronal dynamics.
Significance Statement In this study, we applied a mathematical formalism to estimate the mean-field behavior of a large neuronal ensemble, taking into account the ion exchange between the intracellular and extracellular space. This approach establishes a link between the biophysical description at the cellular scale and the dynamics observable at the mesoscopic scale. The model reproduces different brain activities, including spiking behavior, and seizures, as a function of the extracellular ion concentration. Thus, this neural mass model enables studying the influence of changes in extracellular ionic conditions on whole brain dynamics in health and disease.},
	publisher = {{bioRxiv}},
	author = {Bandyopadhyay, Abhirup and Rabuffo, Giovanni and Calabrese, Carmela and Gudibanda, Kashyap and Depannemaecker, Damien and Ivanov, Anton and Bernard, Christophe and Jirsa, Viktor K. and Petkoski, Spase},
	urldate = {2022-12-07},
	date = {2022-12-03},
	langid = {english},
}

@article{jiang_how_2022,
	title = {How synonymous mutations alter enzyme structure and function over long timescales},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1755-4349},
	url = {https://www.nature.com/articles/s41557-022-01091-z},
	doi = {10.1038/s41557-022-01091-z},
	abstract = {The specific activity of enzymes can be altered over long timescales in cells by synonymous mutations that alter a messenger {RNA} molecule’s sequence but not the encoded protein’s primary structure. How this happens at the molecular level is unknown. Here, we use multiscale modelling of three Escherichia coli enzymes (type {III} chloramphenicol acetyltransferase, d-alanine–d-alanine ligase B and dihydrofolate reductase) to understand experimentally measured changes in specific activity due to synonymous mutations. The modelling involves coarse-grained simulations of protein synthesis and post-translational behaviour, all-atom simulations to test robustness and quantum mechanics/molecular mechanics calculations to characterize enzymatic function. We show that changes in codon translation rates induced by synonymous mutations cause shifts in co-translational and post-translational folding pathways that kinetically partition molecules into subpopulations that very slowly interconvert to the native, functional state. Structurally, these states resemble the native state, with localized misfolding near the active sites of the enzymes. These long-lived states exhibit reduced catalytic activity, as shown by their increased activation energies for the reactions they catalyse.},
	pages = {1--11},
	journaltitle = {Nature Chemistry},
	shortjournal = {Nat. Chem.},
	author = {Jiang, Yang and Neti, Syam Sundar and Sitarik, Ian and Pradhan, Priya and To, Philip and Xia, Yingzi and Fried, Stephen D. and Booker, Squire J. and O’Brien, Edward P.},
	urldate = {2022-12-06},
	date = {2022-12-05},
	langid = {english},
	keywords = {Computational biophysics, Computational chemistry, Enzyme mechanisms, Molecular modelling, Protein folding},
}

@misc{chen_minimalistic_2022,
	title = {Minimalistic Unsupervised Learning with the Sparse Manifold Transform},
	url = {http://arxiv.org/abs/2209.15261},
	doi = {10.48550/arXiv.2209.15261},
	abstract = {We describe a minimalistic and interpretable method for unsupervised learning, without resorting to data augmentation, hyperparameter tuning, or other engineering designs, that achieves performance close to the {SOTA} {SSL} methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic sparse manifold transform, one can achieve 99.3\% {KNN} top-1 accuracy on {MNIST}, 81.1\% {KNN} top-1 accuracy on {CIFAR}-10 and 53.2\% on {CIFAR}-100. With a simple gray-scale augmentation, the model gets 83.2\% {KNN} top-1 accuracy on {CIFAR}-10 and 57\% on {CIFAR}-100. These results significantly close the gap between simplistic ``white-box'' methods and the {SOTA} methods. Additionally, we provide visualization to explain how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of {VICReg}. Though there remains a small performance gap between our simple constructive model and {SOTA} methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised learning.},
	number = {{arXiv}:2209.15261},
	publisher = {{arXiv}},
	author = {Chen, Yubei and Yun, Zeyu and Ma, Yi and Olshausen, Bruno and {LeCun}, Yann},
	urldate = {2022-12-07},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2209.15261 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DVKQF6K5/Chen et al. - 2022 - Minimalistic Unsupervised Learning with the Sparse.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/2N7HH7IC/2209.html:text/html},
}

@article{wiskott_slow_2002,
	title = {Slow feature analysis: unsupervised learning of invariances},
	volume = {14},
	issn = {0899-7667},
	doi = {10.1162/089976602317318938},
	shorttitle = {Slow feature analysis},
	abstract = {Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis ({SFA}) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decorrelated features, which are ordered by their degree of invariance. {SFA} can be applied hierarchically to process high-dimensional input signals and extract complex features. {SFA} is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of {SFA}. Finally, a hierarchical network of {SFA} modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously.},
	pages = {715--770},
	number = {4},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Comput},
	author = {Wiskott, Laurenz and Sejnowski, Terrence J.},
	date = {2002-04},
	pmid = {11936959},
	keywords = {notion, Algorithms, Artificial Intelligence, Computer Simulation, Neural Networks, Computer, Nonlinear Dynamics, Pattern Recognition, Automated, Photoreceptor Cells, Visual Perception},
}

@article{hinton_forward-forward_nodate,
	title = {The Forward-Forward Algorithm: Some Preliminary Investigations},
	abstract = {The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth serious investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes can be separated in time, the negative passes can be done ofﬂine, which makes the learning much simpler in the positive pass and allows video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.},
	pages = {17},
	author = {Hinton, Geoffrey},
	langid = {english},
	keywords = {notion},
	file = {Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf:/home/ajl/Zotero/storage/D3Z2DZUE/Hinton - The Forward-Forward Algorithm Some Preliminary In.pdf:application/pdf},
}

@article{chow_increasing_2023,
	title = {Increasing empathic concern relates to salience network hyperconnectivity in cognitively healthy older adults with elevated amyloid-β burden},
	volume = {37},
	issn = {2213-1582},
	url = {https://www.sciencedirect.com/science/article/pii/S2213158222003473},
	doi = {https://doi.org/10.1016/j.nicl.2022.103282},
	abstract = {Enhanced emotional empathy, the ability to share others’ affective experiences, can be a feature of Alzheimer’s disease ({AD}), but whether emotional empathy increases in the preclinical phase of the disease is unknown. We measured emotional empathy over time (range = 0 – 7.3 years, mean = 2.4 years) in 86 older adults during a period in which they were cognitively healthy, functionally normal, and free of dementia symptoms. For each participant, we computed longitudinal trajectories for empathic concern (i.e., an other-oriented form of emotional empathy that promotes prosocial actions) and emotional contagion (i.e., a self-focused form of emotional empathy often accompanied by feelings of distress) from informant ratings of participants’ empathy on the Interpersonal Reactivity Index. Amyloid-β (Aβ) positron emission tomography ({PET}) scans were used to classify participants as either Aβ positive (Aβ+, n = 23) or negative (Aβ-, n = 63) based on Aβ-{PET} cortical binding. Participants also underwent structural and task-free functional magnetic resonance imaging approximately two years on average after their last empathy assessment, at which time most participants remained cognitively healthy. Results indicated that empathic concern, but not emotional contagion, increased more over time in Aβ+ participants than in Aβ- participants despite no initial group difference at the first measurement. Higher connectivity between certain salience network node-pairs (i.e., pregenual anterior cingulate cortex and periaqueductal gray) predicted longitudinal increases in empathic concern in the Aβ+ group but not in the Aβ- group. The Aβ+ participants also had higher overall salience network connectivity than Aβ- participants despite no differences in gray matter volume. These results suggest gains in empathic concern may be a very early feature of {AD} pathophysiology that relates to hyperconnectivity in the salience network, a system that supports emotion generation and interoception. A better understanding of emotional empathy trajectories in the early stages of {AD} pathophysiology will broaden the lens on preclinical {AD} changes and help clinicians to identify older adults who should be screened for {AD} biomarkers.},
	pages = {103282},
	journaltitle = {{NeuroImage}: Clinical},
	author = {Chow, Tiffany E. and Veziris, Christina R. and Joie, Renaud La and Lee, Alex J. and Brown, Jesse A. and Yokoyama, Jennifer S. and Rankin, Katherine P. and Kramer, Joel H. and Miller, Bruce L. and Rabinovici, Gil D. and Seeley, William W. and Sturm, Virginia E.},
	date = {2023},
	keywords = {Alzheimer's disease, Anterior cingulate cortex, Compassion, Preclinical, Salience network, Social cognition},
}

@online{noauthor_spot_nodate,
	title = {{SPOT}: Spatial Optimal Transport for Analyzing Cellular Microenvironments {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=knxP0g5K8A},
	urldate = {2022-12-14},
	file = {SPOT\: Spatial Optimal Transport for Analyzing Cellular Microenvironments | OpenReview:/home/ajl/Zotero/storage/MPLCC3RM/forum.html:text/html},
}

@article{radhakrishnan_simple_2022,
	title = {Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks},
	volume = {119},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2108.00131},
	doi = {10.1073/pnas.2115064119},
	abstract = {Matrix completion problems arise in many applications including recommendation systems, computer vision, and genomics. Increasingly larger neural networks have been successful in many of these applications, but at considerable computational costs. Remarkably, taking the width of a neural network to infinity allows for improved computational performance. In this work, we develop an infinite width neural network framework for matrix completion that is simple, fast, and flexible. Simplicity and speed come from the connection between the infinite width limit of neural networks and kernels known as neural tangent kernels ({NTK}). In particular, we derive the {NTK} for fully connected and convolutional neural networks for matrix completion. The flexibility stems from a feature prior, which allows encoding relationships between coordinates of the target matrix, akin to semi-supervised learning. The effectiveness of our framework is demonstrated through competitive results for virtual drug screening and image inpainting/reconstruction. We also provide an implementation in Python to make our framework accessible on standard hardware to a broad audience.},
	pages = {e2115064119},
	number = {16},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Radhakrishnan, Adityanarayanan and Stefanakis, George and Belkin, Mikhail and Uhler, Caroline},
	urldate = {2022-12-14},
	date = {2022-04-19},
	eprinttype = {arxiv},
	eprint = {2108.00131 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/UKBDYBNR/Radhakrishnan et al. - 2022 - Simple, Fast, and Flexible Framework for Matrix Co.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/6FQDNQKR/2108.html:text/html},
}

@misc{jain_mechanism_2021,
	title = {A Mechanism for Producing Aligned Latent Spaces with Autoencoders},
	url = {http://arxiv.org/abs/2106.15456},
	doi = {10.48550/arXiv.2106.15456},
	abstract = {Aligned latent spaces, where meaningful semantic shifts in the input space correspond to a translation in the embedding space, play an important role in the success of downstream tasks such as unsupervised clustering and data imputation. In this work, we prove that linear and nonlinear autoencoders produce aligned latent spaces by stretching along the left singular vectors of the data. We fully characterize the amount of stretching in linear autoencoders and provide an initialization scheme to arbitrarily stretch along the top directions using these networks. We also quantify the amount of stretching in nonlinear autoencoders in a simplified setting. We use our theoretical results to align drug signatures across cell types in gene expression space and semantic shifts in word embedding spaces.},
	number = {{arXiv}:2106.15456},
	publisher = {{arXiv}},
	author = {Jain, Saachi and Radhakrishnan, Adityanarayanan and Uhler, Caroline},
	urldate = {2022-12-14},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15456 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/3XJ9F7SU/Jain et al. - 2021 - A Mechanism for Producing Aligned Latent Spaces wi.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/AEPHFVGW/2106.html:text/html},
}

@article{carmichael_folded_2021,
	title = {The folded concave Laplacian spectral penalty learns block diagonal sparsity patterns with the strong oracle property},
	url = {https://arxiv.org/abs/2107.03494v2},
	doi = {10.48550/arXiv.2107.03494},
	abstract = {Structured sparsity is an important part of the modern statistical toolkit. We say a set of model parameters has block diagonal sparsity up to permutations if its elements can be viewed as the edges of a graph that has multiple connected components. For example, a block diagonal correlation matrix with K blocks of variables corresponds to a graph with K connected components whose nodes are the variables and whose edges are the correlations. This type of sparsity captures clusters of model parameters. To learn block diagonal sparsity patterns we develop the folded concave Laplacian spectral penalty and provide a majorization-minimization algorithm for the resulting non-convex problem. We show this algorithm has the appealing computational and statistical guarantee of converging to the oracle estimator after two steps with high probability, even in high-dimensional settings. The theory is then demonstrated in several classical problems including covariance estimation, linear regression, and logistic regression.},
	author = {Carmichael, Iain},
	urldate = {2023-01-08},
	date = {2021-07-07},
	langid = {english},
	file = {Full Text PDF:/home/ajl/Zotero/storage/TIT4IZB9/Carmichael - 2021 - The folded concave Laplacian spectral penalty lear.pdf:application/pdf},
}

@article{eyuboglu_mutual_2023,
	title = {Mutual interactors as a principle for phenotype discovery in molecular interaction networks},
	volume = {28},
	issn = {2335-6936},
	abstract = {Biological networks are powerful representations for the discovery of molecular phenotypes. Fundamental to network analysis is the principle-rooted in social networks-that nodes that interact in the network tend to have similar properties. While this long-standing principle underlies powerful methods in biology that associate molecules with phenotypes on the basis of network proximity, interacting molecules are not necessarily similar, and molecules with similar properties do not necessarily interact. Here, we show that molecules are more likely to have similar phenotypes, not if they directly interact in a molecular network, but if they interact with the same molecules. We call this the mutual interactor principle and show that it holds for several kinds of molecular networks, including protein-protein interaction, genetic interaction, and signaling networks. We then develop a machine learning framework for predicting molecular phenotypes on the basis of mutual interactors. Strikingly, the framework can predict drug targets, disease proteins, and protein functions in different species, and it performs better than much more complex algorithms. The framework is robust to incomplete biological data and is capable of generalizing to phenotypes it has not seen during training. Our work represents a network-based predictive platform for phenotypic characterization of biological molecules.},
	pages = {61--72},
	journaltitle = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	shortjournal = {Pac Symp Biocomput},
	author = {Eyuboglu, Sabri and Zitnik, Marinka and Leskovec, Jure},
	date = {2023},
	pmid = {36540965},
	keywords = {Algorithms, Computational Biology, Machine Learning, Phenotype, Protein Interaction Maps, Proteins},
}

@misc{woo_convnext_2023,
	title = {{ConvNeXt} V2: Co-designing and Scaling {ConvNets} with Masked Autoencoders},
	url = {http://arxiv.org/abs/2301.00808},
	doi = {10.48550/arXiv.2301.00808},
	shorttitle = {{ConvNeXt} V2},
	abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern {ConvNets}, represented by {ConvNeXt}, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with {ImageNet} labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders ({MAE}). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization ({GRN}) layer that can be added to the {ConvNeXt} architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called {ConvNeXt} V2, which significantly improves the performance of pure {ConvNets} on various recognition benchmarks, including {ImageNet} classification, {COCO} detection, and {ADE}20K segmentation. We also provide pre-trained {ConvNeXt} V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on {ImageNet}, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
	number = {{arXiv}:2301.00808},
	publisher = {{arXiv}},
	author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
	urldate = {2023-01-06},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2301.00808 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zeng_statistical_2022,
	title = {Statistical and machine learning methods for spatially resolved transcriptomics data analysis},
	volume = {23},
	issn = {1474-760X},
	url = {https://doi.org/10.1186/s13059-022-02653-7},
	doi = {10.1186/s13059-022-02653-7},
	abstract = {The recent advancement in spatial transcriptomics technology has enabled multiplexed profiling of cellular transcriptomes and spatial locations. As the capacity and efficiency of the experimental technologies continue to improve, there is an emerging need for the development of analytical approaches. Furthermore, with the continuous evolution of sequencing protocols, the underlying assumptions of current analytical methods need to be re-evaluated and adjusted to harness the increasing data complexity. To motivate and aid future model development, we herein review the recent development of statistical and machine learning methods in spatial transcriptomics, summarize useful resources, and highlight the challenges and opportunities ahead.},
	pages = {83},
	number = {1},
	journaltitle = {Genome Biology},
	shortjournal = {Genome Biology},
	author = {Zeng, Zexian and Li, Yawei and Li, Yiming and Luo, Yuan},
	urldate = {2023-01-06},
	date = {2022-03-25},
}

@misc{abadie_flexible_2023,
	title = {Flexible and scalable control of T cell memory by a reversible epigenetic switch},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.31.521782v2},
	doi = {10.1101/2022.12.31.521782},
	abstract = {The immune system encodes information about the severity of a pathogenic threat in the quantity and type of memory cell populations formed in response. This encoding emerges from the decisions of lymphocytes to maintain or lose self-renewal and memory potential during a challenge. By tracking {CD}8 T cells at the single-cell and clonal level using time-resolved transcriptomics and quantitative imaging, we identify a flexible memory strategy, whereby T cells initially choose whether to maintain or lose memory potential early after antigen recognition, but following pathogen clearance may regain memory potential if initially lost. This flexibility is implemented by a cis-epigenetic switch silencing the memory regulator {TCF}1 in a stochastic, reversible manner in response to stimulatory inputs. Mathematical modeling shows how this strategy allows memory cell numbers to scale robustly with pathogen virulence and immune response magnitudes. Thus, flexibility in cellular decision making ensures optimal 39 immune responses against diverse threats.},
	publisher = {{bioRxiv}},
	author = {Abadie, Kathleen and Clark, Elisa and Ukogu, Obinna and Yang, Wei and Daza, Riza and Ng, Kenneth K. H. and Fathima, Jumana and Wang, Allan and Bhandoola, Avinash and Nourmohammad, Armita and Shendure, Jay and Cao, Junyue and Kueh, Hao Yuan},
	urldate = {2023-01-06},
	date = {2023-01-04},
	langid = {english},
}

@article{viana_integrated_2023,
	title = {Integrated intracellular organization and its variations in human {iPS} cells},
	rights = {2023 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05563-7},
	doi = {10.1038/s41586-022-05563-7},
	abstract = {Understanding how a subset of expressed genes dictates cellular phenotype is a considerable challenge owing to the large numbers of molecules involved, their combinatorics and the plethora of cellular behaviours that they determine1,2. Here we reduced this complexity by focusing on cellular organization—a key readout and driver of cell behaviour3,4—at the level of major cellular structures that represent distinct organelles and functional machines, and generated the {WTC}-11 {hiPSC} Single-Cell Image Dataset v1, which contains more than 200,000 live cells in 3D, spanning 25 key cellular structures. The scale and quality of this dataset permitted the creation of a generalizable analysis framework to convert raw image data of cells and their structures into dimensionally reduced, quantitative measurements that can be interpreted by humans, and to facilitate data exploration. This framework embraces the vast cell-to-cell variability that is observed within a normal population, facilitates the integration of cell-by-cell structural data and allows quantitative analyses of distinct, separable aspects of organization within and across different cell populations. We found that the integrated intracellular organization of interphase cells was robust to the wide range of variation in cell shape in the population; that the average locations of some structures became polarized in cells at the edges of colonies while maintaining the ‘wiring’ of their interactions with other structures; and that, by contrast, changes in the location of structures during early mitotic reorganization were accompanied by changes in their wiring.},
	pages = {1--10},
	journaltitle = {Nature},
	author = {Viana, Matheus P. and Chen, Jianxu and Knijnenburg, Theo A. and Vasan, Ritvik and Yan, Calysta and Arakaki, Joy E. and Bailey, Matte and Berry, Ben and Borensztejn, Antoine and Brown, Eva M. and Carlson, Sara and Cass, Julie A. and Chaudhuri, Basudev and Cordes Metzler, Kimberly R. and Coston, Mackenzie E. and Crabtree, Zach J. and Davidson, Steve and {DeLizo}, Colette M. and Dhaka, Shailja and Dinh, Stephanie Q. and Do, Thao P. and Domingus, Justin and Donovan-Maiye, Rory M. and Ferrante, Alexandra J. and Foster, Tyler J. and Frick, Christopher L. and Fujioka, Griffin and Fuqua, Margaret A. and Gehring, Jamie L. and Gerbin, Kaytlyn A. and Grancharova, Tanya and Gregor, Benjamin W. and Harrylock, Lisa J. and Haupt, Amanda and Hendershott, Melissa C. and Hookway, Caroline and Horwitz, Alan R. and Hughes, H. Christopher and Isaac, Eric J. and Johnson, Gregory R. and Kim, Brian and Leonard, Andrew N. and Leung, Winnie W. and Lucas, Jordan J. and Ludmann, Susan A. and Lyons, Blair M. and Malik, Haseeb and {McGregor}, Ryan and Medrash, Gabe E. and Meharry, Sean L. and Mitcham, Kevin and Mueller, Irina A. and Murphy-Stevens, Timothy L. and Nath, Aditya and Nelson, Angelique M. and Oluoch, Sandra A. and Paleologu, Luana and Popiel, T. Alexander and Riel-Mehan, Megan M. and Roberts, Brock and Schaefbauer, Lisa M. and Schwarzl, Magdalena and Sherman, Jamie and Slaton, Sylvain and Sluzewski, M. Filip and Smith, Jacqueline E. and Sul, Youngmee and Swain-Bowden, Madison J. and Tang, W. Joyce and Thirstrup, Derek J. and Toloudis, Daniel M. and Tucker, Andrew P. and Valencia, Veronica and Wiegraebe, Winfried and Wijeratna, Thushara and Yang, Ruian and Zaunbrecher, Rebecca J. and Labitigan, Ramon Lorenzo D. and Sanborn, Adrian L. and Johnson, Graham T. and Gunawardane, Ruwanthi N. and Gaudreault, Nathalie and Theriot, Julie A. and Rafelski, Susanne M.},
	urldate = {2023-01-06},
	date = {2023-01-04},
	langid = {english},
	keywords = {Image processing, Cellular imaging, Induced pluripotent stem cells, Organelles, Robustness},
}

@article{townes_nonnegative_2022,
	title = {Nonnegative spatial factorization applied to spatial genomics},
	rights = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01687-w},
	doi = {10.1038/s41592-022-01687-w},
	abstract = {Nonnegative matrix factorization ({NMF}) is widely used to analyze high-dimensional count data because, in contrast to real-valued alternatives such as factor analysis, it produces an interpretable parts-based representation. However, in applications such as spatial transcriptomics, {NMF} fails to incorporate known structure between observations. Here, we present nonnegative spatial factorization ({NSF}), a spatially-aware probabilistic dimension reduction model based on transformed Gaussian processes that naturally encourages sparsity and scales to tens of thousands of observations. {NSF} recovers ground truth factors more accurately than real-valued alternatives such as {MEFISTO} in simulations, and has lower out-of-sample prediction error than probabilistic {NMF} on three spatial transcriptomics datasets from mouse brain and liver. Since not all patterns of gene expression have spatial correlations, we also propose a hybrid extension of {NSF} that combines spatial and nonspatial components, enabling quantification of spatial importance for both observations and features. A {TensorFlow} implementation of {NSF} is available from https://github.com/willtownes/nsf-paper.},
	pages = {1--10},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Townes, F. William and Engelhardt, Barbara E.},
	urldate = {2023-01-06},
	date = {2022-12-31},
	langid = {english},
	keywords = {Machine learning, Gene expression analysis, Software, Statistical methods, Transcriptomics},
}

@misc{leibfried_tutorial_2022,
	title = {A Tutorial on Sparse Gaussian Processes and Variational Inference},
	url = {http://arxiv.org/abs/2012.13962},
	doi = {10.48550/arXiv.2012.13962},
	abstract = {Gaussian processes ({GPs}) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a {GP} model enjoys a posterior in closed form. However, identifying the posterior {GP} scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse {GPs} have been proposed that approximate the true posterior {GP} with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse {GPs} do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference ({VI}), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also problems with multidimensional labels. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both {GPs} and {VI}. A proper exposition to the subject enables also access to more recent advances (like importance-weighted {VI} as well as interdomain, multioutput and deep {GPs}) that can serve as an inspiration for new research ideas.},
	number = {{arXiv}:2012.13962},
	publisher = {{arXiv}},
	author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
	urldate = {2023-01-06},
	date = {2022-12-18},
	eprinttype = {arxiv},
	eprint = {2012.13962 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{tatsunami_sequencer_2022,
	title = {Sequencer: Deep {LSTM} for Image Classification},
	url = {http://arxiv.org/abs/2205.01972},
	doi = {10.48550/arXiv.2205.01972},
	shorttitle = {Sequencer},
	abstract = {In recent computer vision research, the advent of the Vision Transformer ({ViT}) has rapidly revolutionized various architectural design efforts: {ViT} achieved state-of-the-art image classification performance using self-attention found in natural language processing, and {MLP}-Mixer achieved competitive performance using simple multi-layer perceptrons. In contrast, several studies have also suggested that carefully redesigned convolutional neural networks ({CNNs}) can achieve advanced performance comparable to {ViT} without resorting to these new ideas. Against this background, there is growing interest in what inductive bias is suitable for computer vision. Here we propose Sequencer, a novel and competitive architecture alternative to {ViT} that provides a new perspective on these issues. Unlike {ViTs}, Sequencer models long-range dependencies using {LSTMs} rather than self-attention layers. We also propose a two-dimensional version of Sequencer module, where an {LSTM} is decomposed into vertical and horizontal {LSTMs} to enhance performance. Despite its simplicity, several experiments demonstrate that Sequencer performs impressively well: Sequencer2D-L, with 54M parameters, realizes 84.6\% top-1 accuracy on only {ImageNet}-1K. Not only that, we show that it has good transferability and the robust resolution adaptability on double resolution-band.},
	number = {{arXiv}:2205.01972},
	publisher = {{arXiv}},
	author = {Tatsunami, Yuki and Taki, Masato},
	urldate = {2023-01-06},
	date = {2022-09-17},
	eprinttype = {arxiv},
	eprint = {2205.01972 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{llorens-bobadilla_chromatin_2022,
	title = {Chromatin accessibility profiling in tissue sections by spatial {ATAC}},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2022.07.27.500203v1},
	doi = {10.1101/2022.07.27.500203},
	abstract = {Current methods for epigenomic profiling are limited in the ability to obtain genome wide information with spatial resolution. Here we introduce spatial {ATAC}, a method that integrates transposase-accessible chromatin profiling in tissue sections with barcoded solid-phase capture to perform spatially resolved epigenomics. We show that spatial {ATAC} enables the discovery of the regulatory programs underlying spatial gene expression during mouse organogenesis, lineage differentiation and in human pathological samples.},
	publisher = {{bioRxiv}},
	author = {Llorens-Bobadilla, Enric and Zamboni, Margherita and Marklund, Maja and Bhalla, Nayanika and Chen, Xinsong and Hartman, Johan and Frisén, Jonas and Ståhl, Patrik L.},
	urldate = {2023-01-06},
	date = {2022-07-29},
	langid = {english},
}

@misc{celik_biological_2022,
	title = {Biological Cartography: Building and Benchmarking Representations of Life},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.09.519400v1},
	doi = {10.1101/2022.12.09.519400},
	shorttitle = {Biological Cartography},
	abstract = {The continued scaling of genetic perturbation technologies combined with high-dimensional assays (microscopy and {RNA}-sequencing) has enabled genome-scale reverse-genetics experiments that go beyond single-endpoint measurements of growth or lethality. Datasets emerging from these experiments can be combined to construct “maps of biology”, in which perturbation readouts are placed in unified, relatable embedding spaces to capture known biological relationships and discover new ones. Construction of maps involves many technical choices in both experimental and computational protocols, motivating the design of benchmark procedures by which to evaluate map quality in a systematic, unbiased manner.
In this work, we propose a framework for the steps involved in map building and demonstrate key classes of benchmarks to assess the quality of a map. We describe univariate benchmarks assessing perturbation quality and multivariate benchmarks assessing recovery of known biological relationships from large-scale public data sources. We demonstrate the application and interpretation of these benchmarks through example maps of {scRNA}-seq and phenomic imaging data.},
	publisher = {{bioRxiv}},
	author = {Celik, Safiye and Hütter, Jan-Christian and Carlos, Sandra Melo and Lazar, Nathan H. and Mohan, Rahul and Tillinghast, Conor and Biancalani, Tommaso and Fay, Marta and Earnshaw, Berton A. and Haque, Imran S.},
	urldate = {2023-01-06},
	date = {2022-12-12},
	langid = {english},
}

@misc{chang_maskgit_2022,
	title = {{MaskGIT}: Masked Generative Image Transformer},
	url = {http://arxiv.org/abs/2202.04200},
	doi = {10.48550/arXiv.2202.04200},
	shorttitle = {{MaskGIT}},
	abstract = {Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term {MaskGIT}. During training, {MaskGIT} learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that {MaskGIT} significantly outperforms the state-of-the-art transformer model on the {ImageNet} dataset, and accelerates autoregressive decoding by up to 64x. Besides, we illustrate that {MaskGIT} can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation.},
	number = {{arXiv}:2202.04200},
	publisher = {{arXiv}},
	author = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
	urldate = {2023-01-06},
	date = {2022-02-08},
	eprinttype = {arxiv},
	eprint = {2202.04200 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{schuster_confident_2022,
	title = {Confident Adaptive Language Modeling},
	url = {http://arxiv.org/abs/2207.07061},
	doi = {10.48550/arXiv.2207.07061},
	abstract = {Recent advances in Transformer-based large language models ({LLMs}) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by {LLMs} is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling ({CALM}), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute -- potential speedup of up to \${\textbackslash}times 3\$ -- while provably maintaining high performance.},
	number = {{arXiv}:2207.07061},
	publisher = {{arXiv}},
	author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q. and Tay, Yi and Metzler, Donald},
	urldate = {2023-01-05},
	date = {2022-10-25},
	eprinttype = {arxiv},
	eprint = {2207.07061 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/26E4MVC4/Schuster et al. - 2022 - Confident Adaptive Language Modeling.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/VJZ2KKTE/2207.html:text/html},
}

@misc{chan_data_2022,
	title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
	url = {http://arxiv.org/abs/2205.05055},
	doi = {10.48550/arXiv.2205.05055},
	abstract = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
	number = {{arXiv}:2205.05055},
	publisher = {{arXiv}},
	author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and {McClelland}, Jay and Hill, Felix},
	urldate = {2023-01-05},
	date = {2022-11-17},
	eprinttype = {arxiv},
	eprint = {2205.05055 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/PQ5D2473/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-C.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/DW7SWNAQ/2205.html:text/html},
}

@article{khan_transformers_2022,
	title = {Transformers in Vision: A Survey},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	url = {http://arxiv.org/abs/2101.01169},
	doi = {10.1145/3505244},
	shorttitle = {Transformers in Vision},
	abstract = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory ({LSTM}). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works.},
	pages = {1--41},
	number = {10},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
	urldate = {2023-01-05},
	date = {2022-01-31},
	eprinttype = {arxiv},
	eprint = {2101.01169 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/X9AQW47E/Khan et al. - 2022 - Transformers in Vision A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/SPRKBFEK/2101.html:text/html},
}

@article{sun_dynamic_2022,
	title = {Dynamic visualization of high-dimensional data},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-022-00380-4},
	doi = {10.1038/s43588-022-00380-4},
	abstract = {Dimensionality reduction ({DR}) is commonly used to project high-dimensional data into lower dimensions for visualization, which could then generate new insights and hypotheses. However, {DR} algorithms introduce distortions in the visualization and cannot faithfully represent all relations in the data. Thus, there is a need for methods to assess the reliability of {DR} visualizations. Here we present {DynamicViz}, a framework for generating dynamic visualizations that capture the sensitivity of {DR} visualizations to perturbations in the data resulting from bootstrap sampling. {DynamicViz} can be applied to all commonly used {DR} methods. We show the utility of dynamic visualizations in diagnosing common interpretative pitfalls of static visualizations and extending existing single-cell analyses. We introduce the variance score to quantify the dynamic variability of observations in these visualizations. The variance score characterizes natural variability in the data and can be used to optimize {DR} algorithm implementations.},
	pages = {1--15},
	journaltitle = {Nature Computational Science},
	shortjournal = {Nat Comput Sci},
	author = {Sun, Eric D. and Ma, Rong and Zou, James},
	urldate = {2023-01-05},
	date = {2022-12-30},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Software, Computer science},
}

@misc{zhai_complete_2021,
	title = {Complete Dictionary Learning via \${\textbackslash}ell{\textasciicircum}4\$-Norm Maximization over the Orthogonal Group},
	url = {http://arxiv.org/abs/1906.02435},
	doi = {10.48550/arXiv.1906.02435},
	abstract = {This paper considers the fundamental problem of learning a complete (orthogonal) dictionary from samples of sparsely generated signals. Most existing methods solve the dictionary (and sparse representations) based on heuristic algorithms, usually without theoretical guarantees for either optimality or complexity. The recent \${\textbackslash}ell{\textasciicircum}1\$-minimization based methods do provide such guarantees but the associated algorithms recover the dictionary one column at a time. In this work, we propose a new formulation that maximizes the \${\textbackslash}ell{\textasciicircum}4\$-norm over the orthogonal group, to learn the entire dictionary. We prove that under a random data model, with nearly minimum sample complexity, the global optima of the \${\textbackslash}ell{\textasciicircum}4\$ norm are very close to signed permutations of the ground truth. Inspired by this observation, we give a conceptually simple and yet effective algorithm based on "matching, stretching, and projection" ({MSP}). The algorithm provably converges locally at a superlinear (cubic) rate and cost per iteration is merely an {SVD}. In addition to strong theoretical guarantees, experiments show that the new algorithm is significantly more efficient and effective than existing methods, including {KSVD} and \${\textbackslash}ell{\textasciicircum}1\$-based methods. Preliminary experimental results on mixed real imagery data clearly demonstrate advantages of so learned dictionary over classic {PCA} bases.},
	number = {{arXiv}:1906.02435},
	publisher = {{arXiv}},
	author = {Zhai, Yuexiang and Yang, Zitong and Liao, Zhenyu and Wright, John and Ma, Yi},
	urldate = {2023-01-05},
	date = {2021-04-06},
	eprinttype = {arxiv},
	eprint = {1906.02435 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Electrical Engineering and Systems Science - Signal Processing, Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ZVW7D2CX/Zhai et al. - 2021 - Complete Dictionary Learning via \$ell^4\$-Norm Max.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/UM3AWEXK/1906.html:text/html},
}

@misc{zhou_comments_2020,
	title = {Comments on Efficient Singular Value Thresholding Computation},
	url = {http://arxiv.org/abs/2011.06710},
	doi = {10.48550/arXiv.2011.06710},
	abstract = {We discuss how to evaluate the proximal operator of a convex and increasing function of a nuclear norm, which forms the key computational step in several first-order optimization algorithms such as (accelerated) proximal gradient descent and {ADMM}. Various special cases of the problem arise in low-rank matrix completion, dropout training in deep learning and high-order low-rank tensor recovery, although they have all been solved on a case-by-case basis. We provide an unified and efficiently computable procedure for solving this problem.},
	number = {{arXiv}:2011.06710},
	publisher = {{arXiv}},
	author = {Zhou, Zhengyuan and Ma, Yi},
	urldate = {2023-01-05},
	date = {2020-11-12},
	eprinttype = {arxiv},
	eprint = {2011.06710 [math]},
	keywords = {notion, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SBR76C8K/Zhou and Ma - 2020 - Comments on Efficient Singular Value Thresholding .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/JR955PV8/2011.html:text/html},
}

@article{dai_ctrl_2022,
	title = {{CTRL}: Closed-Loop Transcription to an {LDR} via Minimaxing Rate Reduction},
	volume = {24},
	issn = {1099-4300},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9031319/},
	doi = {10.3390/e24040456},
	shorttitle = {{CTRL}},
	abstract = {This work proposes a new computational framework for learning a structured generative model for real-world datasets. In particular, we propose to learn a Closed-loop Transcriptionbetween a multi-class, multi-dimensional data distribution and a Linear discriminative representation ({CTRL}) in the feature space that consists of multiple independent multi-dimensional linear subspaces. In particular, we argue that the optimal encoding and decoding mappings sought can be formulated as a two-player minimax game between the encoder and decoderfor the learned representation. A natural utility function for this game is the so-called rate reduction, a simple information-theoretic measure for distances between mixtures of subspace-like Gaussians in the feature space. Our formulation draws inspiration from closed-loop error feedback from control systems and avoids expensive evaluating and minimizing of approximated distances between arbitrary distributions in either the data space or the feature space. To a large extent, this new formulation unifies the concepts and benefits of Auto-Encoding and {GAN} and naturally extends them to the settings of learning a both discriminative and generative representation for multi-class and multi-dimensional real-world data. Our extensive experiments on many benchmark imagery datasets demonstrate tremendous potential of this new closed-loop formulation: under fair comparison, visual quality of the learned decoder and classification performance of the encoder is competitive and arguably better than existing methods based on {GAN}, {VAE}, or a combination of both. Unlike existing generative models, the so-learned features of the multiple classes are structured instead of hidden: different classes are explicitly mapped onto corresponding independent principal subspaces in the feature space, and diverse visual attributes within each class are modeled by the independent principal components within each subspace.},
	pages = {456},
	number = {4},
	journaltitle = {Entropy},
	shortjournal = {Entropy (Basel)},
	author = {Dai, Xili and Tong, Shengbang and Li, Mingyang and Wu, Ziyang and Psenka, Michael and Chan, Kwan Ho Ryan and Zhai, Pengyuan and Yu, Yaodong and Yuan, Xiaojun and Shum, Heung-Yeung and Ma, Yi},
	urldate = {2023-01-05},
	date = {2022-03-25},
	pmid = {35455120},
	pmcid = {PMC9031319},
	keywords = {notion},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/9G3QHWS9/Dai et al. - 2022 - CTRL Closed-Loop Transcription to an LDR via Mini.pdf:application/pdf},
}

@misc{bardes_vicreg_2022-1,
	title = {{VICReg}: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
	url = {http://arxiv.org/abs/2105.04906},
	doi = {10.48550/arXiv.2105.04906},
	shorttitle = {{VICReg}},
	abstract = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce {VICReg} (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. {VICReg} combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
	number = {{arXiv}:2105.04906},
	publisher = {{arXiv}},
	author = {Bardes, Adrien and Ponce, Jean and {LeCun}, Yann},
	urldate = {2023-01-08},
	date = {2022-01-28},
	eprinttype = {arxiv},
	eprint = {2105.04906 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/YHQHSNKC/Bardes et al. - 2022 - VICReg Variance-Invariance-Covariance Regularizat.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/EW2JT68G/2105.html:text/html},
}

@article{rukhlenko_control_2022,
	title = {Control of cell state transitions},
	volume = {609},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-022-05194-y},
	doi = {10.1038/s41586-022-05194-y},
	abstract = {Understanding cell state transitions and purposefully controlling them is a longstanding challenge in biology. Here we present cell state transition assessment and regulation ({cSTAR}), an approach for mapping cell states, modelling transitions between them and predicting targeted interventions to convert cell fate decisions. {cSTAR} uses omics data as input, classifies cell states, and develops a workflow that transforms the input data into mechanistic models that identify a core signalling network, which controls cell fate transitions by influencing whole-cell networks. By integrating signalling and phenotypic data, {cSTAR} models how cells manoeuvre in Waddington’s landscape1 and make decisions about which cell fate to adopt. Notably, {cSTAR} devises interventions to control the movement of cells in Waddington’s landscape. Testing {cSTAR} in a cellular model of differentiation and proliferation shows a high correlation between quantitative predictions and experimental data. Applying {cSTAR} to different types of perturbation and omics datasets, including single-cell data, demonstrates its flexibility and scalability and provides new biological insights. The ability of {cSTAR} to identify targeted perturbations that interconvert cell fates will enable designer approaches for manipulating cellular development pathways and mechanistically underpinned therapeutic interventions.},
	pages = {975--985},
	number = {7929},
	journaltitle = {Nature},
	author = {Rukhlenko, Oleksii S. and Halasz, Melinda and Rauch, Nora and Zhernovkov, Vadim and Prince, Thomas and Wynne, Kieran and Maher, Stephanie and Kashdan, Eugene and {MacLeod}, Kenneth and Carragher, Neil O. and Kolch, Walter and Kholodenko, Boris N.},
	urldate = {2023-01-05},
	date = {2022-09},
	langid = {english},
	note = {Number: 7929
Publisher: Nature Publishing Group},
	keywords = {Systems biology, Computational models},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2023-01-05},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/REMUX4RH/Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/TEPL7ZC2/1312.html:text/html},
}

@misc{yang_rethinking_2020,
	title = {Rethinking Bias-Variance Trade-off for Generalization of Neural Networks},
	url = {http://arxiv.org/abs/2002.11328},
	doi = {10.48550/arXiv.2002.11328},
	abstract = {The classical bias-variance trade-off predicts that bias decreases and variance increase with model complexity, leading to a U-shaped risk curve. Recent work calls this into question for neural networks and other over-parameterized models, for which it is often observed that larger models generalize better. We provide a simple explanation for this by measuring the bias and variance of neural networks: while the bias is monotonically decreasing as in the classical theory, the variance is unimodal or bell-shaped: it increases then decreases with the width of the network. We vary the network architecture, loss function, and choice of dataset and confirm that variance unimodality occurs robustly for all models we considered. The risk curve is the sum of the bias and variance curves and displays different qualitative shapes depending on the relative scale of bias and variance, with the double descent curve observed in recent literature as a special case. We corroborate these empirical results with a theoretical analysis of two-layer linear networks with random first layer. Finally, evaluation on out-of-distribution data shows that most of the drop in accuracy comes from increased bias while variance increases by a relatively small amount. Moreover, we find that deeper models decrease bias and increase variance for both in-distribution and out-of-distribution data.},
	number = {{arXiv}:2002.11328},
	publisher = {{arXiv}},
	author = {Yang, Zitong and Yu, Yaodong and You, Chong and Steinhardt, Jacob and Ma, Yi},
	urldate = {2023-01-05},
	date = {2020-12-07},
	eprinttype = {arxiv},
	eprint = {2002.11328 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/Y8425F2Q/Yang et al. - 2020 - Rethinking Bias-Variance Trade-off for Generalizat.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZD9W7ZT8/2002.html:text/html},
}

@misc{wu_optimal_2020,
	title = {On the Optimal Weighted \${\textbackslash}ell\_2\$ Regularization in Overparameterized Linear Regression},
	url = {http://arxiv.org/abs/2006.05800},
	abstract = {We consider the linear model y = Xβ + with X ∈ Rn×p in the overparameterized regime p {\textgreater} n. We estimate β via generalized (weighted) ridge regression: βˆλ = X X + λΣw †X y, where Σw is the weighting matrix. Under a random design setting with general data covariance Σx and anisotropic prior on the true coeﬃcients Eβ β = Σβ, we provide an exact characterization of the prediction risk E(y − x βˆλ)2 in the proportional asymptotic limit p/n → γ ∈ (1, ∞). Our general setup leads to a number of interesting ﬁndings. We outline precise conditions that decide the sign of the optimal setting λopt for the ridge parameter λ and conﬁrm the implicit 2 regularization eﬀect of overparameterization, which theoretically justiﬁes the surprising empirical observation that λopt can be negative in the overparameterized regime. We also characterize the double descent phenomenon for principal component regression ({PCR}) when X and β are both anisotropic. Finally, we determine the optimal weighting matrix Σw for both the ridgeless (λ → 0) and optimally regularized (λ = λopt) case, and demonstrate the advantage of the weighted objective over standard ridge regression and {PCR}.},
	number = {{arXiv}:2006.05800},
	publisher = {{arXiv}},
	author = {Wu, Denny and Xu, Ji},
	urldate = {2023-01-05},
	date = {2020-11-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.05800 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Mathematics - Statistics Theory},
	file = {Wu and Xu - 2020 - On the Optimal Weighted \$ell_2\$ Regularization in.pdf:/home/ajl/Zotero/storage/V43XS6AR/Wu and Xu - 2020 - On the Optimal Weighted \$ell_2\$ Regularization in.pdf:application/pdf},
}

@misc{yao_multi-center_2022,
	title = {Multi-center integrated analysis of non-coding {CRISPR} screens},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.21.520137v1},
	doi = {10.1101/2022.12.21.520137},
	abstract = {The {ENCODE} Consortium’s efforts to annotate non-coding, cis-regulatory elements ({CREs}) have advanced our understanding of gene regulatory landscapes which play a major role in health and disease. Pooled, non-coding {CRISPR} screens are a promising approach for systematically investigating gene regulatory mechanisms. Here, the {ENCODE} Functional Characterization Centers report 109 screens comprising 346,970 individual perturbations across 13.3Mb of the genome, using a variety of methods, readouts, and statistical analyses. Across 332 functionally confirmed {CRE}-gene links, we identify principles for screening endogenous, non-coding elements for causal regulatory mechanisms. Nearly all {CREs} show strong evidence of open chromatin, and targeting accessibility peak summits is a critical component of our proposed {sgRNA} design rules. We provide experimental guidelines to accurately detect {CREs} with variable, often low, transcriptional effects. We discover a previously undescribed {DNA} strand-bias for {CRISPRi} in transcribed regions with implications for screen design and analysis. Benchmarking five screen analysis tools, we find {CASA} produces the most conservative {CRE} calls and is robust to artifacts of low-specificity {sgRNAs}. Together, we provide an accessible data resource, predesigned {sgRNAs} targeting 3,275,697 {ENCODE} {SCREEN} candidate {CREs}, and screening guidelines to accelerate functional characterization of the non-coding genome.},
	publisher = {{bioRxiv}},
	author = {Yao, David and Tycko, Josh and Oh, Jin Woo and Bounds, Lexi R. and Gosai, Sager J. and Lataniotis, Lazaros and Mackay-Smith, Ava and Doughty, Benjamin R. and Gabdank, Idan and Schmidt, Henri and Youngworth, Ingrid and Andreeva, Kalina and Ren, Xingjie and Barrera, Alejandro and Luo, Yunhai and Siklenka, Keith and Yardımcı, Galip Gürkan and Consortium, The {ENCODE}4 and Tewhey, Ryan and Kundaje, Anshul and Greenleaf, William J. and Sabeti, Pardis C. and Leslie, Christina and Pritykin, Yuri and Moore, Jill E. and Beer, Michael A. and Gersbach, Charles A. and Reddy, Timothy E. and Shen, Yin and Engreitz, Jesse M. and Bassik, Michael C. and Reilly, Steven K.},
	urldate = {2023-01-04},
	date = {2022-12-22},
	langid = {english},
	note = {Pages: 2022.12.21.520137
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/8ELNBFPN/Yao et al. - 2022 - Multi-center integrated analysis of non-coding CRI.pdf:application/pdf},
}

@misc{valdes_lockout_2021,
	title = {Lockout: Sparse Regularization of Neural Networks},
	url = {http://arxiv.org/abs/2107.07160},
	doi = {10.48550/arXiv.2107.07160},
	shorttitle = {Lockout},
	abstract = {Many regression and classification procedures fit a parameterized function \$f(x;w)\$ of predictor variables \$x\$ to data \${\textbackslash}\{x\_\{i\},y\_\{i\}{\textbackslash}\}\_1{\textasciicircum}N\$ based on some loss criterion \$L(y,f)\$. Often, regularization is applied to improve accuracy by placing a constraint \$P(w){\textbackslash}leq t\$ on the values of the parameters \$w\$. Although efficient methods exist for finding solutions to these constrained optimization problems for all values of \$t{\textbackslash}geq0\$ in the special case when \$f\$ is a linear function, none are available when \$f\$ is non-linear (e.g. Neural Networks). Here we present a fast algorithm that provides all such solutions for any differentiable function \$f\$ and loss \$L\$, and any constraint \$P\$ that is an increasing monotone function of the absolute value of each parameter. Applications involving sparsity inducing regularization of arbitrary Neural Networks are discussed. Empirical results indicate that these sparse solutions are usually superior to their dense counterparts in both accuracy and interpretability. This improvement in accuracy can often make Neural Networks competitive with, and sometimes superior to, state-of-the-art methods in the analysis of tabular data.},
	number = {{arXiv}:2107.07160},
	publisher = {{arXiv}},
	author = {Valdes, Gilmer and Arbelo, Wilmer and Interian, Yannet and Friedman, Jerome H.},
	urldate = {2023-01-04},
	date = {2021-07-15},
	eprinttype = {arxiv},
	eprint = {2107.07160 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JYBNZFHE/Valdes et al. - 2021 - Lockout Sparse Regularization of Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/RRR5Y9AC/2107.html:text/html},
}

@misc{penzar_legnet_2022,
	title = {{LegNet}: resetting the bar in deep learning for accurate prediction of promoter activity and variant effects from massive parallel reporter assays},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.22.521582v1},
	doi = {10.1101/2022.12.22.521582},
	shorttitle = {{LegNet}},
	abstract = {Parallel reporter assays provide rich data to decipher gene regulatory regions with deep learning. Here we introduce {LegNet}, a convolutional network architecture that secured the first place for our autosome.org team in the {DREAM} 2022 challenge of predicting gene expression from gigantic parallel reporter assays. To construct {LegNet}, we drew inspiration from {EfficientNetV}2 and reformulated the sequence-to-expression regression problem as a soft-classification task. Here, with published data, we demonstrate that {LegNet} outperforms existing models and accurately predicts gene expression per se as well as the effects of sequence alterations, such as single-nucleotide variants.},
	publisher = {{bioRxiv}},
	author = {Penzar, Dmitry and Nogina, Daria and Meshcheryakov, Georgy and Lando, Andrey and Rafi, Abdul Muntakim and Boer, Carl de and Zinkevich, Arsenii and Kulakovskiy, Ivan V.},
	urldate = {2023-01-04},
	date = {2022-12-23},
	langid = {english},
	note = {Pages: 2022.12.22.521582
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/9UL7YRJ7/Penzar et al. - 2022 - LegNet resetting the bar in deep learning for acc.pdf:application/pdf},
}

@misc{villie_neural_2022,
	title = {Neural Networks beyond explainability: Selective inference for sequence motifs},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.23.521748v1},
	doi = {10.1101/2022.12.23.521748},
	shorttitle = {Neural Networks beyond explainability},
	abstract = {Over the past decade, neural networks have been successful at making predictions from biological sequences, especially in the context of regulatory genomics. As in other fields of deep learning, tools have been devised to extract features such as sequence motifs that can explain the predictions made by a trained network. Here we intend to go beyond explainable machine learning and introduce {SEISM}, a selective inference procedure to test the association between these extracted features and the predicted phenotype. In particular, we discuss how training a one-layer convolutional network is formally equivalent to selecting motifs maximizing some association score. We adapt existing sampling-based selective inference procedures by quantizing this selection over an infinite set to a large but finite grid. Finally, we show that sampling under a specific choice of parameters is sufficient to characterize the composite null hypothesis typically used for selective inference—a result that goes well beyond our particular framework. We illustrate the behavior of our method in terms of calibration, power and speed and discuss its power/speed trade-off with a simpler data-split strategy. {SEISM} paves the way to an easier analysis of neural networks used in regulatory genomics, and to more powerful methods for genome wide association studies ({GWAS}).},
	publisher = {{bioRxiv}},
	author = {Villié, Antoine and Veber, Philippe and Castro, Yohann De and Jacob, Laurent},
	urldate = {2023-01-04},
	date = {2022-12-23},
	langid = {english},
	note = {Pages: 2022.12.23.521748
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/UHYJGQ2G/Villié et al. - 2022 - Neural Networks beyond explainability Selective i.pdf:application/pdf},
}

@misc{radhakrishnan_feature_2022,
	title = {Feature learning in neural networks and kernel machines that recursively learn features},
	url = {http://arxiv.org/abs/2212.13881},
	doi = {10.48550/arXiv.2212.13881},
	abstract = {Neural networks have achieved impressive results on many technological and scientific tasks. Yet, their empirical successes have outpaced our fundamental understanding of their structure and function. By identifying mechanisms driving the successes of neural networks, we can provide principled approaches for improving neural network performance and develop simple and effective alternatives. In this work, we isolate the key mechanism driving feature learning in fully connected neural networks by connecting neural feature learning to the average gradient outer product. We subsequently leverage this mechanism to design {\textbackslash}textit\{Recursive Feature Machines\} ({RFMs}), which are kernel machines that learn features. We show that {RFMs} (1) accurately capture features learned by deep fully connected neural networks, (2) close the gap between kernel machines and fully connected networks, and (3) surpass a broad spectrum of models including neural networks on tabular data. Furthermore, we demonstrate that {RFMs} shed light on recently observed deep learning phenomena such as grokking, lottery tickets, simplicity biases, and spurious features. We provide a Python implementation to make our method broadly accessible [{\textbackslash}href\{https://github.com/aradha/recursive\_feature\_machines\}\{{GitHub}\}].},
	number = {{arXiv}:2212.13881},
	publisher = {{arXiv}},
	author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
	urldate = {2023-01-03},
	date = {2022-12-28},
	eprinttype = {arxiv},
	eprint = {2212.13881 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{george_lazy_2022,
	title = {Lazy vs hasty: linearization in deep networks impacts learning schedule based on example difficulty},
	url = {http://arxiv.org/abs/2209.09658},
	doi = {10.48550/arXiv.2209.09658},
	shorttitle = {Lazy vs hasty},
	abstract = {Among attempts at giving a theoretical account of the success of deep neural networks, a recent line of work has identified a so-called lazy training regime in which the network can be well approximated by its linearization around initialization. Here we investigate the comparative effect of the lazy (linear) and feature learning (non-linear) regimes on subgroups of examples based on their difficulty. Specifically, we show that easier examples are given more weight in feature learning mode, resulting in faster training compared to more difficult ones. In other words, the non-linear dynamics tends to sequentialize the learning of examples of increasing difficulty. We illustrate this phenomenon across different ways to quantify example difficulty, including c-score, label noise, and in the presence of easy-to-learn spurious correlations. Our results reveal a new understanding of how deep networks prioritize resources across example difficulty.},
	number = {{arXiv}:2209.09658},
	publisher = {{arXiv}},
	author = {George, Thomas and Lajoie, Guillaume and Baratin, Aristide},
	urldate = {2023-01-03},
	date = {2022-11-21},
	eprinttype = {arxiv},
	eprint = {2209.09658 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{smithe_mathematical_2022,
	title = {Mathematical Foundations for a Compositional Account of the Bayesian Brain},
	url = {http://arxiv.org/abs/2212.12538},
	doi = {10.48550/arXiv.2212.12538},
	abstract = {This dissertation reports some first steps towards a compositional account of active inference and the Bayesian brain. Specifically, we use the tools of contemporary applied category theory to supply functorial semantics for approximate inference. To do so, we define on the `syntactic' side the new notion of Bayesian lens and show that Bayesian updating composes according to the compositional lens pattern. Using Bayesian lenses, and inspired by compositional game theory, we define categories of statistical games and use them to classify various problems of statistical inference. On the `semantic' side, we present a new formalization of general open dynamical systems (particularly: deterministic, stochastic, and random; and discrete- and continuous-time) as certain coalgebras of polynomial functors, which we show collect into monoidal opindexed categories (or, alternatively, into algebras for multicategories of generalized polynomial functors). We use these opindexed categories to define monoidal bicategories of cilia: dynamical systems which control lenses, and which supply the target for our functorial semantics. Accordingly, we construct functors which explain the bidirectional compositional structure of predictive coding neural circuits under the free energy principle, thereby giving a formal mathematical underpinning to the bidirectionality observed in the cortex. Along the way, we explain how to compose rate-coded neural circuits using an algebra for a multicategory of linear circuit diagrams, showing subsequently that this is subsumed by lenses and polynomial functors. Because category theory is unfamiliar to many computational neuroscientists and cognitive scientists, we have made a particular effort to give clear, detailed, and approachable expositions of all the category-theoretic structures and results of which we make use.},
	number = {{arXiv}:2212.12538},
	publisher = {{arXiv}},
	author = {Smithe, Toby St Clere},
	urldate = {2023-01-02},
	date = {2022-12-23},
	eprinttype = {arxiv},
	eprint = {2212.12538 [cs, math, q-bio, stat]},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Mathematics - Category Theory, Mathematics - Dynamical Systems},
}

@article{santoro_higher-order_2023,
	title = {Higher-order organization of multivariate time series},
	rights = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-022-01852-0},
	doi = {10.1038/s41567-022-01852-0},
	abstract = {Time series analysis has proven to be a powerful method to characterize several phenomena in biology, neuroscience and economics, and to understand some of their underlying dynamical features. Several methods have been proposed for the analysis of multivariate time series, yet most of them neglect the effect of non-pairwise interactions on the emerging dynamics. Here, we propose a framework to characterize the temporal evolution of higher-order dependencies within multivariate time series. Using network analysis and topology, we show that our framework robustly differentiates various spatiotemporal regimes of coupled chaotic maps. This includes chaotic dynamical phases and various types of synchronization. Hence, using the higher-order co-fluctuation patterns in simulated dynamical processes as a guide, we highlight and quantify signatures of higher-order patterns in data from brain functional activity, financial markets and epidemics. Overall, our approach sheds light on the higher-order organization of multivariate time series, allowing a better characterization of dynamical group dependencies inherent to real-world data.},
	pages = {1--9},
	journaltitle = {Nature Physics},
	shortjournal = {Nat. Phys.},
	author = {Santoro, Andrea and Battiston, Federico and Petri, Giovanni and Amico, Enrico},
	urldate = {2023-01-02},
	date = {2023-01-02},
	langid = {english},
	keywords = {Applied physics, Information theory and computation, Statistical physics, thermodynamics and nonlinear dynamics},
}

@article{parkes_asymmetric_2022,
	title = {Asymmetric signaling across the hierarchy of cytoarchitecture within the human connectome},
	volume = {8},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.add2185},
	doi = {10.1126/sciadv.add2185},
	abstract = {Cortical variations in cytoarchitecture form a sensory-fugal axis that shapes regional profiles of extrinsic connectivity and is thought to guide signal propagation and integration across the cortical hierarchy. While neuroimaging work has shown that this axis constrains local properties of the human connectome, it remains unclear whether it also shapes the asymmetric signaling that arises from higher-order topology. Here, we used network control theory to examine the amount of energy required to propagate dynamics across the sensory-fugal axis. Our results revealed an asymmetry in this energy, indicating that bottom-up transitions were easier to complete compared to top-down. Supporting analyses demonstrated that asymmetries were underpinned by a connectome topology that is wired to support efficient bottom-up signaling. Lastly, we found that asymmetries correlated with differences in communicability and intrinsic neuronal time scales and lessened throughout youth. Our results show that cortical variation in cytoarchitecture may guide the formation of macroscopic connectome topology. 
          ,  
            Linking microstructure to the connectome shows that activity propagates more efficiently up the cortical hierarchy than down.},
	pages = {eadd2185},
	number = {50},
	journaltitle = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Parkes, Linden and Kim, Jason Z. and Stiso, Jennifer and Calkins, Monica E. and Cieslak, Matthew and Gur, Raquel E. and Gur, Ruben C. and Moore, Tyler M. and Ouellet, Mathieu and Roalf, David R. and Shinohara, Russell T. and Wolf, Daniel H. and Satterthwaite, Theodore D. and Bassett, Dani S.},
	urldate = {2022-12-23},
	date = {2022-12-14},
	langid = {english},
}

@misc{brenner_multimodal_2022,
	title = {Multimodal Teacher Forcing for Reconstructing Nonlinear Dynamical Systems},
	url = {http://arxiv.org/abs/2212.07892},
	doi = {10.48550/arXiv.2212.07892},
	abstract = {Many, if not most, systems of interest in science are naturally described as nonlinear dynamical systems ({DS}). Empirically, we commonly access these systems through time series measurements, where often we have time series from different types of data modalities simultaneously. For instance, we may have event counts in addition to some continuous signal. While by now there are many powerful machine learning ({ML}) tools for integrating different data modalities into predictive models, this has rarely been approached so far from the perspective of uncovering the underlying, data-generating {DS} (aka {DS} reconstruction). Recently, sparse teacher forcing ({TF}) has been suggested as an efficient control-theoretic method for dealing with exploding loss gradients when training {ML} models on chaotic {DS}. Here we incorporate this idea into a novel recurrent neural network ({RNN}) training framework for {DS} reconstruction based on multimodal variational autoencoders ({MVAE}). The forcing signal for the {RNN} is generated by the {MVAE} which integrates different types of simultaneously given time series data into a joint latent code optimal for {DS} reconstruction. We show that this training method achieves significantly better reconstructions on multimodal datasets generated from chaotic {DS} benchmarks than various alternative methods.},
	number = {{arXiv}:2212.07892},
	publisher = {{arXiv}},
	author = {Brenner, Manuel and Koppe, Georgia and Durstewitz, Daniel},
	urldate = {2022-12-23},
	date = {2022-12-15},
	eprinttype = {arxiv},
	eprint = {2212.07892 [nlin]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics},
}

@misc{jain_biological_2022,
	title = {Biological Sequence Design with {GFlowNets}},
	url = {http://arxiv.org/abs/2203.04115},
	doi = {10.48550/arXiv.2203.04115},
	abstract = {Design of de novo biological sequences with desired properties, like protein and {DNA} sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed {GFlowNets} as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in {GFlowNets}. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.},
	number = {{arXiv}:2203.04115},
	publisher = {{arXiv}},
	author = {Jain, Moksh and Bengio, Emmanuel and Garcia, Alex-Hernandez and Rector-Brooks, Jarrid and Dossou, Bonaventure F. P. and Ekbote, Chanakya and Fu, Jie and Zhang, Tianyu and Kilgour, Micheal and Zhang, Dinghuai and Simine, Lena and Das, Payel and Bengio, Yoshua},
	urldate = {2022-12-23},
	date = {2022-10-24},
	eprinttype = {arxiv},
	eprint = {2203.04115 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
}

@misc{bogatskiy_symmetry_2022,
	title = {Symmetry Group Equivariant Architectures for Physics},
	url = {http://arxiv.org/abs/2203.06153},
	doi = {10.48550/arXiv.2203.06153},
	abstract = {Physical theories grounded in mathematical symmetries are an essential component of our understanding of a wide range of properties of the universe. Similarly, in the domain of machine learning, an awareness of symmetries such as rotation or permutation invariance has driven impressive performance breakthroughs in computer vision, natural language processing, and other important applications. In this report, we argue that both the physics community and the broader machine learning community have much to understand and potentially to gain from a deeper investment in research concerning symmetry group equivariant machine learning architectures. For some applications, the introduction of symmetries into the fundamental structural design can yield models that are more economical (i.e. contain fewer, but more expressive, learned parameters), interpretable (i.e. more explainable or directly mappable to physical quantities), and/or trainable (i.e. more efficient in both data and computational requirements). We discuss various figures of merit for evaluating these models as well as some potential benefits and limitations of these methods for a variety of physics applications. Research and investment into these approaches will lay the foundation for future architectures that are potentially more robust under new computational paradigms and will provide a richer description of the physical systems to which they are applied.},
	number = {{arXiv}:2203.06153},
	publisher = {{arXiv}},
	author = {Bogatskiy, Alexander and Ganguly, Sanmay and Kipf, Thomas and Kondor, Risi and Miller, David W. and Murnane, Daniel and Offermann, Jan T. and Pettee, Mariel and Shanahan, Phiala and Shimmin, Chase and Thais, Savannah},
	urldate = {2022-12-23},
	date = {2022-03-11},
	eprinttype = {arxiv},
	eprint = {2203.06153 [astro-ph, physics:hep-ex, physics:hep-ph]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Astrophysics - Instrumentation and Methods for Astrophysics, High Energy Physics - Experiment, High Energy Physics - Phenomenology},
}

@misc{turinici_huber-energy_2022,
	title = {Huber-energy measure quantization},
	url = {http://arxiv.org/abs/2212.08162},
	doi = {10.48550/arXiv.2212.08162},
	abstract = {We describe a measure quantization procedure i.e., an algorithm which finds the best approximation of a target probability law (and more generally signed finite variation measure) by a sum of Q Dirac masses (Q being the quantization parameter). The procedure is implemented by minimizing the statistical distance between the original measure and its quantized version; the distance is built from a negative definite kernel and, if necessary, can be computed on the fly and feed to a stochastic optimization algorithm (such as {SGD}, Adam, ...). We investigate theoretically the fundamental questions of existence of the optimal measure quantizer and identify what are the required kernel properties that guarantee suitable behavior. We test the procedure, called {HEMQ}, on several databases: multi-dimensional Gaussian mixtures, Wiener space cubature, Italian wine cultivars and the {MNIST} image database. The results indicate that the {HEMQ} algorithm is robust and versatile and, for the class of Huber-energy kernels, it matches the expected intuitive behavior.},
	number = {{arXiv}:2212.08162},
	publisher = {{arXiv}},
	author = {Turinici, Gabriel},
	urldate = {2022-12-23},
	date = {2022-12-15},
	eprinttype = {arxiv},
	eprint = {2212.08162 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Numerical Analysis, Mathematics - Statistics Theory, Mathematics - Probability},
}

@misc{joy_capturing_2022,
	title = {Capturing Label Characteristics in {VAEs}},
	url = {http://arxiv.org/abs/2006.10102},
	doi = {10.48550/arXiv.2006.10102},
	abstract = {We present a principled approach to incorporating labels in {VAEs} that captures the rich characteristic information associated with those labels. While prior work has typically conflated these by learning latent variables that directly correspond to label values, we argue this is contrary to the intended effect of supervision in {VAEs}-capturing rich label characteristics with the latents. For example, we may want to capture the characteristics of a face that make it look young, rather than just the age of the person. To this end, we develop the {CCVAE}, a novel {VAE} model and concomitant variational objective which captures label characteristics explicitly in the latent space, eschewing direct correspondences between label values and latents. Through judicious structuring of mappings between such characteristic latents and labels, we show that the {CCVAE} can effectively learn meaningful representations of the characteristics of interest across a variety of supervision schemes. In particular, we show that the {CCVAE} allows for more effective and more general interventions to be performed, such as smooth traversals within the characteristics for a given label, diverse conditional generation, and transferring characteristics across datapoints.},
	number = {{arXiv}:2006.10102},
	publisher = {{arXiv}},
	author = {Joy, Tom and Schmon, Sebastian M. and Torr, Philip H. S. and Siddharth, N. and Rainforth, Tom},
	urldate = {2022-12-23},
	date = {2022-12-16},
	eprinttype = {arxiv},
	eprint = {2006.10102 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pogodin_efficient_2022,
	title = {Efficient Conditionally Invariant Representation Learning},
	url = {http://arxiv.org/abs/2212.08645},
	doi = {10.48550/arXiv.2212.08645},
	abstract = {We introduce the Conditional Independence Regression {CovariancE} ({CIRCE}), a measure of conditional independence for multivariate continuous-valued variables. {CIRCE} applies as a regularizer in settings where we wish to learn neural features \${\textbackslash}varphi(X)\$ of data \$X\$ to estimate a target \$Y\$, while being conditionally independent of a distractor \$Z\$ given \$Y\$. Both \$Z\$ and \$Y\$ are assumed to be continuous-valued but relatively low dimensional, whereas \$X\$ and its features may be complex and high dimensional. Relevant settings include domain-invariant learning, fairness, and causal learning. The procedure requires just a single ridge regression from \$Y\$ to kernelized features of \$Z\$, which can be done in advance. It is then only necessary to enforce independence of \${\textbackslash}varphi(X)\$ from residuals of this regression, which is possible with attractive estimation properties and consistency guarantees. By contrast, earlier measures of conditional feature dependence require multiple regressions for each step of feature learning, resulting in more severe bias and variance, and greater computational cost. When sufficiently rich features are used, we establish that {CIRCE} is zero if and only if \${\textbackslash}varphi(X) {\textbackslash}perp {\textbackslash}!{\textbackslash}!{\textbackslash}! {\textbackslash}perp Z {\textbackslash}mid Y\$. In experiments, we show superior performance to previous methods on challenging benchmarks, including learning conditionally invariant image features.},
	number = {{arXiv}:2212.08645},
	publisher = {{arXiv}},
	author = {Pogodin, Roman and Deka, Namrata and Li, Yazhe and Sutherland, Danica J. and Veitch, Victor and Gretton, Arthur},
	urldate = {2022-12-23},
	date = {2022-12-16},
	eprinttype = {arxiv},
	eprint = {2212.08645 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{agterberg_estimating_2022,
	title = {Estimating Higher-Order Mixed Memberships via the \${\textbackslash}ell\_\{2,{\textbackslash}infty\}\$ Tensor Perturbation Bound},
	url = {http://arxiv.org/abs/2212.08642},
	doi = {10.48550/arXiv.2212.08642},
	abstract = {Higher-order multiway data is ubiquitous in machine learning and statistics and often exhibits community-like structures, where each component (node) along each different mode has a community membership associated with it. In this paper we propose the tensor mixed-membership blockmodel, a generalization of the tensor blockmodel positing that memberships need not be discrete, but instead are convex combinations of latent communities. We establish the identifiability of our model and propose a computationally efficient estimation procedure based on the higher-order orthogonal iteration algorithm ({HOOI}) for tensor {SVD} composed with a simplex corner-finding algorithm. We then demonstrate the consistency of our estimation procedure by providing a per-node error bound, which showcases the effect of higher-order structures on estimation accuracy. To prove our consistency result, we develop the \${\textbackslash}ell\_\{2,{\textbackslash}infty\}\$ tensor perturbation bound for {HOOI} under independent, possibly heteroskedastic, subgaussian noise that may be of independent interest. Our analysis uses a novel leave-one-out construction for the iterates, and our bounds depend only on spectral properties of the underlying low-rank tensor under nearly optimal signal-to-noise ratio conditions such that tensor {SVD} is computationally feasible. Whereas other leave-one-out analyses typically focus on sequences constructed by analyzing the output of a given algorithm with a small part of the noise removed, our leave-one-out analysis constructions use both the previous iterates and the additional tensor structure to eliminate a potential additional source of error. Finally, we apply our methodology to real and simulated data, including applications to two flight datasets and a trade network dataset, demonstrating some effects not identifiable from the model with discrete community memberships.},
	number = {{arXiv}:2212.08642},
	publisher = {{arXiv}},
	author = {Agterberg, Joshua and Zhang, Anru},
	urldate = {2022-12-23},
	date = {2022-12-16},
	eprinttype = {arxiv},
	eprint = {2212.08642 [math, stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
}

@article{casaletto_latelife_2022,
	title = {Late‐life physical activity relates to brain tissue synaptic integrity markers in older adults},
	volume = {18},
	issn = {1552-5260, 1552-5279},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/alz.12530},
	doi = {10.1002/alz.12530},
	pages = {2023--2035},
	number = {11},
	journaltitle = {Alzheimer's \& Dementia},
	shortjournal = {Alzheimer's \& Dementia},
	author = {Casaletto, Kaitlin and Ramos‐Miguel, Alfredo and {VandeBunte}, Anna and Memel, Molly and Buchman, Aron and Bennett, David and Honer, William},
	urldate = {2022-12-23},
	date = {2022-11},
	langid = {english},
}

@inproceedings{luo_neighborhood-aware_2022,
	title = {Neighborhood-aware Scalable Temporal Network Representation Learning},
	url = {https://openreview.net/forum?id=EPUtNe7a9ta},
	abstract = {Temporal networks have been widely used to model real-world complex systems such as financial systems and e-commerce systems. In a temporal network, the joint neighborhood of a set of nodes often provides crucial structural information useful for predicting whether they may interact at a certain time. However, recent representation learning methods for temporal networks often fail to extract such information or depend on online construction of structural features, which is time-consuming. To address the issue, this work proposes Neighborhood-Aware Temporal network model ({NAT}). For each node in the network, {NAT} abandons the commonly-used one-single-vector-based representation while adopting a novel dictionary-type neighborhood representation. Such a dictionary representation records a downsampled set of the neighboring nodes as keys, and allows fast construction of structural features for a joint neighborhood of multiple nodes. We also design a dedicated data structure termed N-cache to support parallel access and update of those dictionary representations on {GPUs}. {NAT} gets evaluated over seven real-world large-scale temporal networks. {NAT} not only outperforms all cutting-edge baselines by averaged 1.2\% and 4.2\% in transductive and inductive link prediction accuracy, respectively, but also keeps scalable by achieving a speed-up of 4.1-76.7x against the baselines that adopt joint structural features and achieves a speed-up of 1.6-4.0x against the baselines that cannot adopt those features. The link to the code: https: //github.com/Graph-{COM}/Neighborhood-Aware-Temporal-Network.},
	eventtitle = {The First Learning on Graphs Conference},
	author = {Luo, Yuhong and Li, Pan},
	urldate = {2022-12-23},
	date = {2022-12-09},
	langid = {english},
}

@misc{isufi_graph-time_2021,
	title = {Graph-Time Convolutional Neural Networks},
	url = {http://arxiv.org/abs/2103.01730},
	doi = {10.48550/arXiv.2103.01730},
	abstract = {Spatiotemporal data can be represented as a process over a graph, which captures their spatial relationships either explicitly or implicitly. How to leverage such a structure for learning representations is one of the key challenges when working with graphs. In this paper, we represent the spatiotemporal relationships through product graphs and develop a first principle graph-time convolutional neural network ({GTCNN}). The {GTCNN} is a compositional architecture with each layer comprising a graph-time convolutional module, a graph-time pooling module, and a nonlinearity. We develop a graph-time convolutional filter by following the shift-and-sum principles of the convolutional operator to learn higher-level features over the product graph. The product graph itself is parametric so that we can learn also the spatiotemporal coupling from data. We develop a zero-pad pooling that preserves the spatial graph (the prior about the data) while reducing the number of active nodes and the parameters. Experimental results with synthetic and real data corroborate the different components and compare with baseline and state-of-the-art solutions.},
	number = {{arXiv}:2103.01730},
	publisher = {{arXiv}},
	author = {Isufi, Elvin and Mazzola, Gabriele},
	urldate = {2022-12-23},
	date = {2021-03-02},
	eprinttype = {arxiv},
	eprint = {2103.01730 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Signal Processing},
}

@misc{robnik_microcanonical_2022,
	title = {Microcanonical Hamiltonian Monte Carlo},
	url = {http://arxiv.org/abs/2212.08549},
	doi = {10.48550/arXiv.2212.08549},
	abstract = {We develop Microcanonical Hamiltonian Monte Carlo ({MCHMC}), a class of models which follow a fixed energy Hamiltonian dynamics, in contrast to Hamiltonian Monte Carlo ({HMC}), which follows canonical distribution with different energy levels. {MCHMC} tunes the Hamiltonian function such that the marginal of the uniform distribution on the constant-energy-surface over the momentum variables gives the desired target distribution. We show that {MCHMC} requires occasional energy conserving billiard-like momentum bounces for ergodicity, analogous to momentum resampling in {HMC}. We generalize the concept of bounces to a continuous version with partial direction preserving bounces at every step, which gives an energy conserving underdamped Langevin-like dynamics with non-Gaussian noise ({MCLMC}). {MCHMC} and {MCLMC} exhibit favorable scalings with condition number and dimensionality. We develop an efficient hyperparameter tuning scheme that achieves high performance and consistently outperforms {NUTS} {HMC} on several standard benchmark problems, in some cases by more than an order of magnitude.},
	number = {{arXiv}:2212.08549},
	publisher = {{arXiv}},
	author = {Robnik, Jakob and De Luca, G. Bruno and Silverstein, Eva and Seljak, Uroš},
	urldate = {2022-12-23},
	date = {2022-12-16},
	eprinttype = {arxiv},
	eprint = {2212.08549 [astro-ph, physics:hep-lat, physics:hep-th, stat]},
	keywords = {Statistics - Computation, Astrophysics - Instrumentation and Methods for Astrophysics, High Energy Physics - Lattice, High Energy Physics - Theory},
}

@misc{notin_trancepteve_2022,
	title = {{TranceptEVE}: Combining Family-specific and Family-agnostic Models of Protein Sequences for Improved Fitness Prediction},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.07.519495v1},
	doi = {10.1101/2022.12.07.519495},
	shorttitle = {{TranceptEVE}},
	abstract = {Modeling the fitness landscape of protein sequences has historically relied on training models on family-specific sets of homologous sequences called Multiple Sequence Alignments. Many proteins are however difficult to align or have shallow alignments which limits the potential scope of alignment-based methods. Not subject to these limitations, large protein language models trained on non-aligned sequences across protein families have achieved increasingly high predictive performance – but have not yet fully bridged the gap with their alignment-based counterparts. In this work, we introduce {TranceptEVE} – a hybrid method between family-specific and family-agnostic models that seeks to build on the relative strengths from each approach. Our method gracefully adapts to the depth of the alignment, fully relying on its autoregressive transformer when dealing with shallow alignments and leaning more heavily on the family-specifc models for proteins with deeper alignments. Besides its broader application scope, it achieves state-of-the-art performance for mutation effects prediction, both in terms of correlation with experimental assays and with clinical annotations from {ClinVar}.},
	publisher = {{bioRxiv}},
	author = {Notin, Pascal and Niekerk, Lood Van and Kollasch, Aaron W. and Ritter, Daniel and Gal, Yarin and Marks, Debora S.},
	urldate = {2022-12-23},
	date = {2022-12-10},
	langid = {english},
}

@article{coassolo_mapping_2022,
	title = {Mapping transcriptional heterogeneity and metabolic networks in fatty livers at single-cell resolution},
	issn = {2589-0042},
	url = {https://www.sciencedirect.com/science/article/pii/S2589004222020752},
	doi = {10.1016/j.isci.2022.105802},
	abstract = {Non-alcoholic fatty liver disease is a heterogeneous disease with unclear underlying molecular mechanisms. Here, we perform single-cell {RNA} sequencing of hepatocytes and hepatic nonparenchymal cells to map the lipid signatures in mice with non-alcoholic fatty liver disease ({NAFLD}). We uncover previously unidentified clusters of hepatocytes characterized by either high or low srebp1 expression. Surprisingly, the canonical lipid synthesis driver Srebp1 is not predictive of hepatic lipid accumulation, suggestive of other drivers of lipid metabolism. By combining transcriptional data at single cell resolution with computational network analyses, we find that {NAFLD} is associated with high constitutive androstane receptor ({CAR}) expression. Mechanistically, {CAR} interacts with four functional modules: cholesterol homeostasis, bile acid metabolism, fatty acid metabolism, and estrogen response. Nuclear expression of {CAR} positively correlates with steatohepatitis in human livers. These findings demonstrate significant cellular differences in lipid signatures and identify functional networks linked to hepatic steatosis in mice and humans.},
	pages = {105802},
	journaltitle = {{iScience}},
	shortjournal = {{iScience}},
	author = {Coassolo, Laetitia and Liu, Tianyun and Jung, Yunshin and Taylor, Nikki P. and Zhao, Meng and Charville, Gregory W. and Nissen, Silas Boye and Yki-Jarvinen, Hannele and Altman, Russ and Svensson, Katrin J.},
	urldate = {2022-12-23},
	date = {2022-12-15},
	langid = {english},
}

@misc{chiu_scaling_2020,
	title = {Scaling Hidden Markov Language Models},
	url = {http://arxiv.org/abs/2011.04640},
	doi = {10.48550/arXiv.2011.04640},
	abstract = {The hidden Markov model ({HMM}) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure. However, this separation makes it difficult to fit {HMMs} to large datasets in modern {NLP}, and they have fallen out of use due to very poor performance compared to fully observed models. This work revisits the challenge of scaling {HMMs} to language modeling datasets, taking ideas from recent approaches to neural modeling. We propose methods for scaling {HMMs} to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization. Experiments show that this approach leads to models that are more accurate than previous {HMM} and n-gram-based methods, making progress towards the performance of state-of-the-art neural models.},
	number = {{arXiv}:2011.04640},
	publisher = {{arXiv}},
	author = {Chiu, Justin T. and Rush, Alexander M.},
	urldate = {2022-12-23},
	date = {2020-11-09},
	eprinttype = {arxiv},
	eprint = {2011.04640 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{oshea_direct_2022,
	title = {Direct neural perturbations reveal a dynamical mechanism for robust computation},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.16.520768v1},
	doi = {10.1101/2022.12.16.520768},
	abstract = {The rich repertoire of skilled mammalian behavior is the product of neural circuits that generate robust and flexible patterns of activity distributed across populations of neurons. Decades of associative studies have linked many behaviors to specific patterns of population activity, but association alone cannot reveal the dynamical mechanisms that shape those patterns. Are local neural circuits high-dimensional dynamical reservoirs able to generate arbitrary superpositions of patterns with appropriate excitation? Or might circuit dynamics be shaped in response to behavioral context so as to generate only the low-dimensional patterns needed for the task at hand? Here, we address these questions within primate motor cortex by delivering optogenetic and electrical microstimulation perturbations during reaching behavior. We develop a novel analytic approach that relates measured activity to theoretically tractable, dynamical models of excitatory and inhibitory neurons. This computational model captures the dynamical effects of these perturbations and demonstrates that motor cortical activity during reaching is shaped by a self-contained, low-dimensional dynamical system. The subspace containing task-relevant dynamics proves to be oriented so as to be robust to strong non-normal amplification within cortical circuits. This task dynamics space exhibits a privileged causal relationship with behavior, in that stimulation in motor cortex perturb reach kinematics only to the extent that it alters neural states within this subspace. Our results resolve long-standing questions about the dynamical structure of cortical activity associated with movement, and illuminate the dynamical perturbation experiments needed to understand how neural circuits throughout the brain generate complex behavior.},
	publisher = {{bioRxiv}},
	author = {O’Shea, Daniel J. and Duncker, Lea and Goo, Werapong and Sun, Xulu and Vyas, Saurabh and Trautmann, Eric M. and Diester, Ilka and Ramakrishnan, Charu and Deisseroth, Karl and Sahani, Maneesh and Shenoy, Krishna V.},
	urldate = {2022-12-23},
	date = {2022-12-16},
	langid = {english},
}

@misc{don-yehiya_cold_2022,
	title = {{ColD} Fusion: Collaborative Descent for Distributed Multitask Finetuning},
	url = {http://arxiv.org/abs/2212.01378},
	doi = {10.48550/arXiv.2212.01378},
	shorttitle = {{ColD} Fusion},
	abstract = {Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining. Until now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. In this paper, we propose {ColD} Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, {ColD} Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on. We show that {ColD} Fusion yields comparable benefits to multitask pretraining by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. We find {ColD} Fusion outperforms {RoBERTa} and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, {ColD} Fusion-based model outperforms {RoBERTa} by 2.45 points in average without any changes to the architecture.},
	number = {{arXiv}:2212.01378},
	publisher = {{arXiv}},
	author = {Don-Yehiya, Shachar and Venezian, Elad and Raffel, Colin and Slonim, Noam and Katz, Yoav and Choshen, Leshem},
	urldate = {2022-12-23},
	date = {2022-12-02},
	eprinttype = {arxiv},
	eprint = {2212.01378 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
}

@misc{qiu_persistent_2022,
	title = {Persistent spectral theory-guided protein engineering},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.18.520933v1},
	doi = {10.1101/2022.12.18.520933},
	abstract = {{\textless}p{\textgreater}While protein engineering, which iteratively optimizes protein fitness by screening the gigantic mutational space, is constrained by experimental capacity, various machine learning models have substantially expedited protein engineering. Three-dimensional protein structures promise further advantages, but their intricate geometric complexity hinders their applications in deep mutational screening. Persistent homology, an established algebraic topology tool for protein structural complexity reduction, fails to capture the homotopic shape evolution during the filtration of a given data. This work introduces a Topology-offered protein Fitness ({TopFit}) framework to complement protein sequence and structure embeddings. Equipped with an ensemble regression strategy, {TopFit} integrates the persistent spectral theory, a new topological Laplacian, and two auxiliary sequence embeddings to capture mutation-induced topological invariant, shape evolution, and sequence disparity in the protein fitness landscape. The performance of {TopFit} is assessed by 34 benchmark datasets with 128,634 variants, involving a vast variety of protein structure acquisition modalities and training set size variations.{\textless}/p{\textgreater}},
	publisher = {{bioRxiv}},
	author = {Qiu, Yuchi and Wei, Guo-Wei},
	urldate = {2022-12-23},
	date = {2022-12-19},
	langid = {english},
}

@article{gorin_interpretable_2022,
	title = {Interpretable and tractable models of transcriptional noise for the rational design of single-molecule quantification experiments},
	volume = {13},
	rights = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-34857-7},
	doi = {10.1038/s41467-022-34857-7},
	abstract = {The question of how cell-to-cell differences in transcription rate affect {RNA} count distributions is fundamental for understanding biological processes underlying transcription. Answering this question requires quantitative models that are both interpretable (describing concrete biophysical phenomena) and tractable (amenable to mathematical analysis). This enables the identification of experiments which best discriminate between competing hypotheses. As a proof of principle, we introduce a simple but flexible class of models involving a continuous stochastic transcription rate driving a discrete {RNA} transcription and splicing process, and compare and contrast two biologically plausible hypotheses about transcription rate variation. One assumes variation is due to {DNA} experiencing mechanical strain, while the other assumes it is due to regulator number fluctuations. We introduce a framework for numerically and analytically studying such models, and apply Bayesian model selection to identify candidate genes that show signatures of each model in single-cell transcriptomic data from mouse glutamatergic neurons.},
	pages = {7620},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Gorin, Gennady and Vastola, John J. and Fang, Meichen and Pachter, Lior},
	urldate = {2022-12-23},
	date = {2022-12-09},
	langid = {english},
	keywords = {Computational biophysics, Computational models, Bayesian inference, Quantum physics, Stochastic modelling},
}

@misc{mehta_dsi_2022,
	title = {{DSI}++: Updating Transformer Memory with New Documents},
	url = {http://arxiv.org/abs/2212.09744},
	doi = {10.48550/arXiv.2212.09744},
	shorttitle = {{DSI}++},
	abstract = {Differentiable Search Indices ({DSIs}) encode a corpus of documents in the parameters of a model and use the same model to map queries directly to relevant document identifiers. Despite the strong performance of {DSI} models, deploying them in situations where the corpus changes over time is computationally expensive because reindexing the corpus requires re-training the model. In this work, we introduce {DSI}++, a continual learning challenge for {DSI} to incrementally index new documents while being able to answer queries related to both previously and newly indexed documents. Across different model scales and document identifier representations, we show that continual indexing of new documents leads to considerable forgetting of previously indexed documents. We also hypothesize and verify that the model experiences forgetting events during training, leading to unstable learning. To mitigate these issues, we investigate two approaches. The first focuses on modifying the training dynamics. Flatter minima implicitly alleviate forgetting, so we optimize for flatter loss basins and show that the model stably memorizes more documents (+12{\textbackslash}\%). Next, we introduce a generative memory to sample pseudo-queries for documents and supplement them during continual indexing to prevent forgetting for the retrieval task. Extensive experiments on novel continual indexing benchmarks based on Natural Questions ({NQ}) and {MS} {MARCO} demonstrate that our proposed solution mitigates forgetting by a significant margin. Concretely, it improves the average Hits@10 by \$+21.1{\textbackslash}\%\$ over competitive baselines for {NQ} and requires \$6\$ times fewer model updates compared to re-training the {DSI} model for incrementally indexing five corpora in a sequence.},
	number = {{arXiv}:2212.09744},
	publisher = {{arXiv}},
	author = {Mehta, Sanket Vaibhav and Gupta, Jai and Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Rao, Jinfeng and Najork, Marc and Strubell, Emma and Metzler, Donald},
	urldate = {2022-12-23},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09744 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
}

@misc{colnet_reweighting_2022,
	title = {Reweighting the {RCT} for generalization: finite sample error and variable selection},
	url = {http://arxiv.org/abs/2208.07614},
	doi = {10.48550/arXiv.2208.07614},
	shorttitle = {Reweighting the {RCT} for generalization},
	abstract = {Randomized Controlled Trials ({RCTs}) may suffer from limited scope. In particular, samples may be unrepresentative: some {RCTs} over- or under- sample individuals with certain characteristics compared to the target population, for which one wants conclusions on treatment effectiveness. Re-weighting trial individuals to match the target population can improve the treatment effect estimation. In this work, we establish the exact expressions of the bias and variance of such reweighting procedures -- also called Inverse Propensity of Sampling Weighting ({IPSW}) -- in presence of categorical covariates for any sample size. Such results allow us to compare the theoretical performance of different versions of {IPSW} estimates. Besides, our results show how the performance (bias, variance, and quadratic risk) of {IPSW} estimates depends on the two sample sizes ({RCT} and target population). A by-product of our work is the proof of consistency of {IPSW} estimates. Results also reveal that {IPSW} performances are improved when the trial probability to be treated is estimated (rather than using its oracle counterpart). In addition, we study choice of variables: how including covariates that are not necessary for identifiability of the causal effect may impact the asymptotic variance. Including covariates that are shifted between the two samples but not treatment effect modifiers increases the variance while non-shifted but treatment effect modifiers do not. We illustrate all the takeaways in a didactic example, and on a semi-synthetic simulation inspired from critical care medicine.},
	number = {{arXiv}:2208.07614},
	publisher = {{arXiv}},
	author = {Colnet, Bénédicte and Josse, Julie and Varoquaux, Gaël and Scornet, Erwan},
	urldate = {2022-12-23},
	date = {2022-11-04},
	eprinttype = {arxiv},
	eprint = {2208.07614 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{ali_survey_2022,
	title = {A Survey of Vectorization Methods in Topological Data Analysis},
	url = {http://arxiv.org/abs/2212.09703},
	doi = {10.48550/arXiv.2212.09703},
	abstract = {Attempts to incorporate topological information in supervised learning tasks have resulted in the creation of several techniques for vectorizing persistent homology barcodes. In this paper, we study thirteen such methods. Besides describing an organizational framework for these methods, we comprehensively benchmark them against three well-known classification tasks. Surprisingly, we discover that the best-performing method is a simple vectorization, which consists only of a few elementary summary statistics. Finally, we provide a convenient web application which has been designed to facilitate exploration and experimentation with various vectorization methods.},
	number = {{arXiv}:2212.09703},
	publisher = {{arXiv}},
	author = {Ali, Dashti and Asaad, Aras and Jimenez, Maria-Jose and Nanda, Vidit and Paluzo-Hidalgo, Eduardo and Soriano-Trigueros, Manuel},
	urldate = {2022-12-23},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09703 [math]},
	keywords = {Mathematics - Algebraic Topology},
}

@misc{evans_inference_2021,
	title = {Inference with generalizable classifier predictions},
	url = {http://arxiv.org/abs/2106.07623},
	doi = {10.48550/arXiv.2106.07623},
	abstract = {This paper addresses the problem of making statistical inference about a population that can only be identified through classifier predictions. The problem is motivated by scientific studies in which human labels of a population are replaced by a classifier. For downstream analysis of the population based on classifier predictions to be sound, the predictions must generalize equally across experimental conditions. In this paper, we formalize the task of statistical inference using classifier predictions, and propose bootstrap procedures to allow inference with a generalizable classifier. We demonstrate the performance of our methods through extensive simulations and a case study with live cell imaging data.},
	number = {{arXiv}:2106.07623},
	publisher = {{arXiv}},
	author = {Evans, Ciaran and Weinberg, Zara Y. and Puthenveedu, Manojkumar A. and G'Sell, Max},
	urldate = {2022-12-23},
	date = {2021-06-14},
	eprinttype = {arxiv},
	eprint = {2106.07623 [stat]},
	keywords = {Statistics - Methodology, Statistics - Applications},
}

@misc{sun_length-extrapolatable_2022,
	title = {A Length-Extrapolatable Transformer},
	url = {http://arxiv.org/abs/2212.10554},
	doi = {10.48550/arXiv.2212.10554},
	abstract = {Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define attention resolution as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at https://aka.ms/{LeX}-Transformer.},
	number = {{arXiv}:2212.10554},
	publisher = {{arXiv}},
	author = {Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
	urldate = {2022-12-23},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10554 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{barbour_steins_2014,
	title = {Steins (magic) method},
	url = {http://arxiv.org/abs/1411.1179},
	doi = {10.48550/arXiv.1411.1179},
	abstract = {The paper presents a general introduction to the astonishing method for deriving probability approximations that was invented by Charles Stein around 50 years ago.},
	number = {{arXiv}:1411.1179},
	publisher = {{arXiv}},
	author = {Barbour, Andrew D. and Chen, Louis H. Y.},
	urldate = {2022-12-23},
	date = {2014-11-05},
	eprinttype = {arxiv},
	eprint = {1411.1179 [math]},
	keywords = {Mathematics - Probability, 60-02, 62E17, 60F05},
}

@misc{khan_bayesian_2022,
	title = {The Bayesian Learning Rule},
	url = {http://arxiv.org/abs/2107.04562},
	doi = {10.48550/arXiv.2107.04562},
	abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, {RMSprop}, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
	number = {{arXiv}:2107.04562},
	publisher = {{arXiv}},
	author = {Khan, Mohammad Emtiyaz and Rue, Håvard},
	urldate = {2022-12-23},
	date = {2022-03-18},
	eprinttype = {arxiv},
	eprint = {2107.04562 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xia_training_2022,
	title = {Training Trajectories of Language Models Across Scales},
	url = {http://arxiv.org/abs/2212.09803},
	doi = {10.48550/arXiv.2212.09803},
	abstract = {Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized {OPT} models (Zhang et al.,2022)--from 125M to 175B parameters--on next-token prediction, sequence-level generation, and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from {BIG}-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.},
	number = {{arXiv}:2212.09803},
	publisher = {{arXiv}},
	author = {Xia, Mengzhou and Artetxe, Mikel and Zhou, Chunting and Lin, Xi Victoria and Pasunuru, Ramakanth and Chen, Danqi and Zettlemoyer, Luke and Stoyanov, Ves},
	urldate = {2022-12-23},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09803 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{acosta_quantifying_2022,
	title = {Quantifying Local Extrinsic Curvature in Neural Manifolds},
	url = {http://arxiv.org/abs/2212.10414},
	doi = {10.48550/arXiv.2212.10414},
	abstract = {The neural manifold hypothesis postulates that the activity of a neural population forms a low-dimensional manifold within the larger neural state space, whose structure reflects the structure of the encoded task variables. Many dimensionality reduction techniques have been used to study the structure of neural manifolds, but these methods do not provide an explicit parameterization of the manifold, and fail to capture the global structure of topologically nontrivial manifolds. Topological data analysis methods can reveal the shared topological structure between neural manifolds and the task variables they represent, but fail to capture much of the geometric information including distance, angles, and curvature. In this work, we leverage tools from Riemannian geometry and topologically-aware deep generative models to introduce a novel approach for studying the geometry of neural manifolds. This approach (1) computes an explicit parameterization of the manifolds and (2) estimates their local extrinsic curvature. Our approach correctly estimates the geometry of synthetic neural manifolds generated from smooth deformations of circles, spheres, and tori. We expect this approach to open new avenues of inquiry exploring geometric neural correlates of perception and behavior, and provide a new means to compare representations in biological and artificial neural systems.},
	number = {{arXiv}:2212.10414},
	publisher = {{arXiv}},
	author = {Acosta, Francisco E. and Sanborn, Sophia and Duc, Khanh Dao and Madhav, Manu and Miolane, Nina},
	urldate = {2022-12-23},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10414 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition},
}

@misc{hie_high-level_2022,
	title = {A high-level programming language for generative protein design},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1},
	doi = {10.1101/2022.12.21.521526},
	abstract = {Combining a basic set of building blocks into more complex forms is a universal design principle. Most protein designs have proceeded from a manual bottom-up approach using parts created by nature, but top-down design of proteins is fundamentally hard due to biological complexity. We demonstrate how the modularity and programmability long sought for protein design can be realized through generative artificial intelligence. Advanced protein language models demonstrate emergent learning of atomic resolution structure and protein design principles. We leverage these developments to enable the programmable design of de novo protein sequences and structures of high complexity. First, we describe a high-level programming language based on modular building blocks that allows a designer to easily compose a set of desired properties. We then develop an energy-based generative model, built on atomic resolution structure prediction with a language model, that realizes all-atom structure designs that have the programmed properties. Designing a diverse set of specifications, including constraints on atomic coordinates, secondary structure, symmetry, and multimerization, demonstrates the generality and controllability of the approach. Enumerating constraints at increasing levels of hierarchical complexity shows that the approach can access a combinatorially large design space.},
	publisher = {{bioRxiv}},
	author = {Hie, Brian and Candido, Salvatore and Lin, Zeming and Kabeli, Ori and Rao, Roshan and Smetanin, Nikita and Sercu, Tom and Rives, Alexander},
	urldate = {2022-12-23},
	date = {2022-12-22},
	langid = {english},
}

@misc{hie_high-level_2022-1,
	title = {A high-level programming language for generative protein design},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.21.521526v1},
	doi = {10.1101/2022.12.21.521526},
	abstract = {Combining a basic set of building blocks into more complex forms is a universal design principle. Most protein designs have proceeded from a manual bottom-up approach using parts created by nature, but top-down design of proteins is fundamentally hard due to biological complexity. We demonstrate how the modularity and programmability long sought for protein design can be realized through generative artificial intelligence. Advanced protein language models demonstrate emergent learning of atomic resolution structure and protein design principles. We leverage these developments to enable the programmable design of de novo protein sequences and structures of high complexity. First, we describe a high-level programming language based on modular building blocks that allows a designer to easily compose a set of desired properties. We then develop an energy-based generative model, built on atomic resolution structure prediction with a language model, that realizes all-atom structure designs that have the programmed properties. Designing a diverse set of specifications, including constraints on atomic coordinates, secondary structure, symmetry, and multimerization, demonstrates the generality and controllability of the approach. Enumerating constraints at increasing levels of hierarchical complexity shows that the approach can access a combinatorially large design space.},
	publisher = {{bioRxiv}},
	author = {Hie, Brian and Candido, Salvatore and Lin, Zeming and Kabeli, Ori and Rao, Roshan and Smetanin, Nikita and Sercu, Tom and Rives, Alexander},
	urldate = {2022-12-23},
	date = {2022-12-22},
	langid = {english},
}

@misc{cai_reversible_2022,
	title = {Reversible Column Networks},
	url = {http://arxiv.org/abs/2212.11696},
	doi = {10.48550/arXiv.2212.11696},
	abstract = {We propose a new neural network design paradigm Reversible Column Network ({RevCol}). The main body of {RevCol} is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes {RevCol} very different behavior from conventional networks: during forward propagation, features in {RevCol} are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that {CNN}-style {RevCol} models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after {ImageNet}-22K pre-training, {RevCol}-{XL} obtains 88.2\% {ImageNet}-1K accuracy. Given more pre-training data, our largest model {RevCol}-H reaches 90.0\% on {ImageNet}-1K, 63.8\% {APbox} on {COCO} detection minival set, 61.0\% {mIoU} on {ADE}20k segmentation. To our knowledge, it is the best {COCO} detection and {ADE}20k segmentation result among pure (static) {CNN} models. Moreover, as a general macro architecture fashion, {RevCol} can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and {NLP} tasks. We release code and models at https://github.com/megvii-research/{RevCol}},
	number = {{arXiv}:2212.11696},
	publisher = {{arXiv}},
	author = {Cai, Yuxuan and Zhou, Yizhuang and Han, Qi and Sun, Jianjian and Kong, Xiangwen and Li, Jun and Zhang, Xiangyu},
	urldate = {2022-12-23},
	date = {2022-12-22},
	eprinttype = {arxiv},
	eprint = {2212.11696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{cai_reversible_2022-1,
	title = {Reversible Column Networks},
	url = {http://arxiv.org/abs/2212.11696},
	doi = {10.48550/arXiv.2212.11696},
	abstract = {We propose a new neural network design paradigm Reversible Column Network ({RevCol}). The main body of {RevCol} is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes {RevCol} very different behavior from conventional networks: during forward propagation, features in {RevCol} are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that {CNN}-style {RevCol} models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after {ImageNet}-22K pre-training, {RevCol}-{XL} obtains 88.2\% {ImageNet}-1K accuracy. Given more pre-training data, our largest model {RevCol}-H reaches 90.0\% on {ImageNet}-1K, 63.8\% {APbox} on {COCO} detection minival set, 61.0\% {mIoU} on {ADE}20k segmentation. To our knowledge, it is the best {COCO} detection and {ADE}20k segmentation result among pure (static) {CNN} models. Moreover, as a general macro architecture fashion, {RevCol} can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and {NLP} tasks. We release code and models at https://github.com/megvii-research/{RevCol}},
	number = {{arXiv}:2212.11696},
	publisher = {{arXiv}},
	author = {Cai, Yuxuan and Zhou, Yizhuang and Han, Qi and Sun, Jianjian and Kong, Xiangwen and Li, Jun and Zhang, Xiangyu},
	urldate = {2022-12-23},
	date = {2022-12-22},
	eprinttype = {arxiv},
	eprint = {2212.11696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{chang_spagft_2022,
	title = {{SpaGFT} is a graph Fourier transform for tissue module identification from spatially resolved transcriptomics},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.10.519929v1},
	doi = {10.1101/2022.12.10.519929},
	abstract = {The tissue module ({TM}) was defined as an architectural area containing recurrent cellular communities executing specific biological functions at different tissue sites. However, the computational identification of {TMs} poses challenges owing to their various length scales, convoluted biological processes, not well-defined molecular features, and irregular spatial patterns. Here, we present a hypothesis-free graph Fourier transform model, {SpaGFT}, to characterize {TMs}. For the first time, {SpaGFT} transforms complex gene expression patterns into simple, but informative signals, leading to the accurate identification of spatially variable genes ({SVGs}) at a fast computational speed. Based on clustering the transformed signals of the {SVGs}, {SpaGFT} provides a novel computational framework for {TM} characterization. Three case studies were used to illustrate {TM} identities, the biological processes of convoluted {TMs} in the lymph node, and conserved {TMs} across multiple samples constituting the complex organ. The superior accuracy, scalability, and interpretability of {SpaGFT} indicate that it is a novel and powerful tool for the investigation of {TMs} to gain new insights into a variety of biological questions.},
	publisher = {{bioRxiv}},
	author = {Chang, Yuzhou and Liu, Jixin and Ma, Anjun and Li, Zihai and Liu, Bingqiang and Ma, Qin},
	urldate = {2022-12-18},
	date = {2022-12-13},
	langid = {english},
	note = {Pages: 2022.12.10.519929
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/W3L89V7T/Chang et al. - 2022 - SpaGFT is a graph Fourier transform for tissue mod.pdf:application/pdf},
}

@misc{ghiasi_what_2022,
	title = {What do Vision Transformers Learn? A Visual Exploration},
	url = {http://arxiv.org/abs/2212.06727},
	doi = {10.48550/arXiv.2212.06727},
	shorttitle = {What do Vision Transformers Learn?},
	abstract = {Vision transformers ({ViTs}) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of {ViTs} remains challenging. In this paper, we first address the obstacles to performing visualizations on {ViTs}. Assisted by these solutions, we observe that neurons in {ViTs} trained with language model supervision (e.g., {CLIP}) are activated by semantic concepts rather than visual features. We also explore the underlying differences between {ViTs} and {CNNs}, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that {ViTs} maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of {ViT} variants, including {DeiT}, {CoaT}, {ConViT}, {PiT}, Swin, and Twin, to validate the effectiveness of our method.},
	number = {{arXiv}:2212.06727},
	publisher = {{arXiv}},
	author = {Ghiasi, Amin and Kazemi, Hamid and Borgnia, Eitan and Reich, Steven and Shu, Manli and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
	urldate = {2022-12-18},
	date = {2022-12-13},
	eprinttype = {arxiv},
	eprint = {2212.06727 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/VENHA5JI/Ghiasi et al. - 2022 - What do Vision Transformers Learn A Visual Explor.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZVLL3WLT/2212.html:text/html},
}

@misc{raghu_vision_2022,
	title = {Do Vision Transformers See Like Convolutional Neural Networks?},
	url = {http://arxiv.org/abs/2108.08810},
	doi = {10.48550/arXiv.2108.08810},
	abstract = {Convolutional neural networks ({CNNs}) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models ({ViT}) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of {ViTs} and {CNNs} on image classification benchmarks, we find striking differences between the two architectures, such as {ViT} having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and {ViT} residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating {ViTs} successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the {MLP}-Mixer.},
	number = {{arXiv}:2108.08810},
	publisher = {{arXiv}},
	author = {Raghu, Maithra and Unterthiner, Thomas and Kornblith, Simon and Zhang, Chiyuan and Dosovitskiy, Alexey},
	urldate = {2022-12-18},
	date = {2022-03-03},
	eprinttype = {arxiv},
	eprint = {2108.08810 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/F6ZWU5VU/Raghu et al. - 2022 - Do Vision Transformers See Like Convolutional Neur.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/D956ZIAB/2108.html:text/html},
}

@misc{yang_gpvit_2022,
	title = {{GPViT}: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation},
	url = {http://arxiv.org/abs/2212.06795},
	doi = {10.48550/arXiv.2212.06795},
	shorttitle = {{GPViT}},
	abstract = {We present the Group Propagation Vision Transformer ({GPViT}): a novel nonhierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block ({GP} Block) to exchange global information. In each {GP} Block, features are first grouped together by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped features; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate {GPViT} on a variety of visual recognition tasks including image classification, semantic segmentation, object detection, and instance segmentation. Our method achieves significant performance gains over previous works across all tasks, especially on tasks that require high-resolution outputs, for example, our {GPViT}-L3 outperforms Swin Transformer-B by 2.0 {mIoU} on {ADE}20K semantic segmentation with only half as many parameters. Code and pre-trained models are available at https://github.com/{ChenhongyiYang}/{GPViT} .},
	number = {{arXiv}:2212.06795},
	publisher = {{arXiv}},
	author = {Yang, Chenhongyi and Xu, Jiarui and De Mello, Shalini and Crowley, Elliot J. and Wang, Xiaolong},
	urldate = {2022-12-18},
	date = {2022-12-13},
	eprinttype = {arxiv},
	eprint = {2212.06795 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/98RAAW2J/Yang et al. - 2022 - GPViT A High Resolution Non-Hierarchical Vision T.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/UT5NIFG9/2212.html:text/html},
}

@online{noauthor_molecular_nodate,
	title = {Molecular Computational Anatomy: Unifying the Particle to Tissue Continuum via Measure Representations of the Brain {\textbar} {BME} Frontiers},
	url = {https://spj.science.org/doi/10.34133/2022/9868673?permanently=true},
	urldate = {2022-12-18},
	keywords = {notion},
	file = {Molecular Computational Anatomy\: Unifying the Particle to Tissue Continuum via Measure Representations of the Brain | BME Frontiers:/home/ajl/Zotero/storage/ICKT8TJG/9868673.html:text/html},
}

@article{wayment-steele_deep_2022,
	title = {Deep learning models for predicting {RNA} degradation via dual crowdsourcing},
	rights = {2022 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00571-8},
	doi = {10.1038/s42256-022-00571-8},
	abstract = {Medicines based on messenger {RNA} ({mRNA}) hold immense potential, as evidenced by their rapid deployment as {COVID}-19 vaccines. However, worldwide distribution of {mRNA} molecules has been limited by their thermostability, which is fundamentally limited by the intrinsic instability of {RNA} molecules to a chemical degradation reaction called in-line hydrolysis. Predicting the degradation of an {RNA} molecule is a key task in designing more stable {RNA}-based therapeutics. Here, we describe a crowdsourced machine learning competition (‘Stanford {OpenVaccine}’) on Kaggle, involving single-nucleotide resolution measurements on 6,043 diverse 102–130-nucleotide {RNA} constructs that were themselves solicited through crowdsourcing on the {RNA} design platform Eterna. The entire experiment was completed in less than 6 months, and 41\% of nucleotide-level predictions from the winning model were within experimental error of the ground truth measurement. Furthermore, these models generalized to blindly predicting orthogonal degradation data on much longer {mRNA} molecules (504–1,588 nucleotides) with improved accuracy compared with previously published models. These results indicate that such models can represent in-line hydrolysis with excellent accuracy, supporting their use for designing stabilized messenger {RNAs}. The integration of two crowdsourcing platforms, one for dataset creation and another for machine learning, may be fruitful for other urgent problems that demand scientific discovery on rapid timescales.},
	pages = {1--11},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Wayment-Steele, Hannah K. and Kladwang, Wipapat and Watkins, Andrew M. and Kim, Do Soon and Tunguz, Bojan and Reade, Walter and Demkin, Maggie and Romano, Jonathan and Wellington-Oguri, Roger and Nicol, John J. and Gao, Jiayang and Onodera, Kazuki and Fujikawa, Kazuki and Mao, Hanfei and Vandewiele, Gilles and Tinti, Michele and Steenwinckel, Bram and Ito, Takuya and Noumi, Taiga and He, Shujun and Ishi, Keiichiro and Lee, Youhan and Öztürk, Fatih and Chiu, King Yuen and Öztürk, Emin and Amer, Karim and Fares, Mohamed and Das, Rhiju},
	urldate = {2022-12-18},
	date = {2022-12-14},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, notion, {RNA}},
	file = {Full Text PDF:/home/ajl/Zotero/storage/FNIMBEEQ/Wayment-Steele et al. - 2022 - Deep learning models for predicting RNA degradatio.pdf:application/pdf},
}

@misc{cherti_reproducible_2022,
	title = {Reproducible scaling laws for contrastive language-image learning},
	url = {http://arxiv.org/abs/2212.07143},
	doi = {10.48550/arXiv.2212.07143},
	abstract = {Scaling up neural networks has led to remarkable performance across a wide range of tasks. Moreover, performance often follows reliable scaling laws as a function of training set size, model size, and compute, which offers valuable guidance as large-scale experiments are becoming increasingly expensive. However, previous work on scaling laws has primarily used private data {\textbackslash}\& models or focused on uni-modal language or vision learning. To address these limitations, we investigate scaling laws for contrastive language-image pre-training ({CLIP}) with the public {LAION} dataset and the open-source {OpenCLIP} repository. Our large-scale experiments involve models trained on up to two billion image-text pairs and identify power law scaling for multiple downstream tasks including zero-shot classification, retrieval, linear probing, and end-to-end fine-tuning. We find that the training distribution plays a key role in scaling laws as the {OpenAI} and {OpenCLIP} models exhibit different scaling behavior despite identical model architectures and similar training recipes. We open-source our evaluation workflow and all models, including the largest public {CLIP} models, to ensure reproducibility and make scaling laws research more accessible. Source code and instructions to reproduce this study will be available at https://github.com/{LAION}-{AI}/scaling-laws-openclip},
	number = {{arXiv}:2212.07143},
	publisher = {{arXiv}},
	author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
	urldate = {2022-12-18},
	date = {2022-12-14},
	eprinttype = {arxiv},
	eprint = {2212.07143 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JS57B9HI/Cherti et al. - 2022 - Reproducible scaling laws for contrastive language.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/5DZSA2F3/2212.html:text/html},
}

@misc{bakarji_dimensionally_2022,
	title = {Dimensionally Consistent Learning with Buckingham Pi},
	url = {http://arxiv.org/abs/2202.04643},
	doi = {10.48550/arXiv.2202.04643},
	abstract = {In the absence of governing equations, dimensional analysis is a robust technique for extracting insights and finding symmetries in physical systems. Given measurement variables and parameters, the Buckingham Pi theorem provides a procedure for finding a set of dimensionless groups that spans the solution space, although this set is not unique. We propose an automated approach using the symmetric and self-similar structure of available measurement data to discover the dimensionless groups that best collapse this data to a lower dimensional space according to an optimal fit. We develop three data-driven techniques that use the Buckingham Pi theorem as a constraint: (i) a constrained optimization problem with a non-parametric input-output fitting function, (ii) a deep learning algorithm ({BuckiNet}) that projects the input parameter space to a lower dimension in the first layer, and (iii) a technique based on sparse identification of nonlinear dynamics ({SINDy}) to discover dimensionless equations whose coefficients parameterize the dynamics. We explore the accuracy, robustness and computational complexity of these methods as applied to three example problems: a bead on a rotating hoop, a laminar boundary layer, and Rayleigh-B{\textbackslash}'enard convection.},
	number = {{arXiv}:2202.04643},
	publisher = {{arXiv}},
	author = {Bakarji, Joseph and Callaham, Jared and Brunton, Steven L. and Kutz, J. Nathan},
	urldate = {2022-12-18},
	date = {2022-02-09},
	eprinttype = {arxiv},
	eprint = {2202.04643 [physics]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computational Engineering, Finance, and Science, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JY9CG5L6/Bakarji et al. - 2022 - Dimensionally Consistent Learning with Buckingham .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/GZKNLVYJ/2202.html:text/html},
}

@misc{aminabadi_deepspeed_2022,
	title = {{DeepSpeed} Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
	url = {http://arxiv.org/abs/2207.00032},
	doi = {10.48550/arXiv.2207.00032},
	shorttitle = {{DeepSpeed} Inference},
	abstract = {The past several years have witnessed the success of transformer-based models, and their scale and application scenarios continue to grow aggressively. The current landscape of transformer models is increasingly diverse: the model size varies drastically with the largest being of hundred-billion parameters; the model characteristics differ due to the sparsity introduced by the Mixture-of-Experts; the target application scenarios can be latency-critical or throughput-oriented; the deployment hardware could be single- or multi-{GPU} systems with different types of memory and storage, etc. With such increasing diversity and the fast-evolving pace of transformer models, designing a highly performant and efficient inference system is extremely challenging. In this paper, we present {DeepSpeed} Inference, a comprehensive system solution for transformer model inference to address the above-mentioned challenges. {DeepSpeed} Inference consists of (1) a multi-{GPU} inference solution to minimize latency while maximizing the throughput of both dense and sparse transformer models when they fit in aggregate {GPU} memory, and (2) a heterogeneous inference solution that leverages {CPU} and {NVMe} memory in addition to the {GPU} memory and compute to enable high inference throughput with large models which do not fit in aggregate {GPU} memory. {DeepSpeed} Inference reduces latency by up to 7.3X over the state-of-the-art for latency-oriented scenarios and increases throughput by over 1.5x for throughput-oriented scenarios. Moreover, it enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of {GPUs}, an unprecedented scale for inference. It can inference 25x larger models than with {GPU}-only solutions, while delivering a high throughput of 84 {TFLOPS} (over \$50{\textbackslash}\%\$ of A6000 peak).},
	number = {{arXiv}:2207.00032},
	publisher = {{arXiv}},
	author = {Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Zhang, Minjia and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Rasley, Jeff and Smith, Shaden and Ruwase, Olatunji and He, Yuxiong},
	urldate = {2022-12-18},
	date = {2022-06-30},
	eprinttype = {arxiv},
	eprint = {2207.00032 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Performance},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/M4RI5KBJ/Aminabadi et al. - 2022 - DeepSpeed Inference Enabling Efficient Inference .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/YNMM8LI5/2207.html:text/html},
}

@misc{wang_adamix_2022,
	title = {{AdaMix}: Mixture-of-Adaptations for Parameter-efficient Model Tuning},
	url = {http://arxiv.org/abs/2205.12410},
	doi = {10.48550/arXiv.2205.12410},
	shorttitle = {{AdaMix}},
	abstract = {Standard fine-tuning of large pre-trained language models ({PLMs}) for downstream tasks requires updating hundreds of millions to billions of parameters, and storing a large copy of the {PLM} weights for every task resulting in increased cost for storing, sharing and serving the models. To address this, parameter-efficient fine-tuning ({PEFT}) techniques were introduced where small trainable components are injected in the {PLM} and updated during fine-tuning. We propose {AdaMix} as a general {PEFT} method that tunes a mixture of adaptation modules -- given the underlying {PEFT} method of choice -- introduced in each Transformer layer while keeping most of the {PLM} weights frozen. For instance, {AdaMix} can leverage a mixture of adapters like Houlsby or a mixture of low rank decomposition matrices like {LoRA} to improve downstream task performance over the corresponding {PEFT} methods for fully supervised and few-shot {NLU} and {NLG} tasks. Further, we design {AdaMix} such that it matches the same computational cost and the number of tunable parameters as the underlying {PEFT} method. By only tuning 0.1-0.2\% of {PLM} parameters, we show that {AdaMix} outperforms {SOTA} parameter-efficient fine-tuning and full model fine-tuning for both {NLU} and {NLG} tasks.},
	number = {{arXiv}:2205.12410},
	publisher = {{arXiv}},
	author = {Wang, Yaqing and Agarwal, Sahaj and Mukherjee, Subhabrata and Liu, Xiaodong and Gao, Jing and Awadallah, Ahmed Hassan and Gao, Jianfeng},
	urldate = {2022-12-18},
	date = {2022-11-01},
	eprinttype = {arxiv},
	eprint = {2205.12410 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/NF2U22LT/Wang et al. - 2022 - AdaMix Mixture-of-Adaptations for Parameter-effic.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/Q44VXKK9/2205.html:text/html},
}

@article{marquet_embeddings_2022,
	title = {Embeddings from protein language models predict conservation and variant effects},
	volume = {141},
	issn = {0340-6717},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8716573/},
	doi = {10.1007/s00439-021-02411-y},
	abstract = {The emergence of {SARS}-{CoV}-2 variants stressed the demand for tools allowing to interpret the effect of single amino acid variants ({SAVs}) on protein function. While Deep Mutational Scanning ({DMS}) sets continue to expand our understanding of the mutational landscape of single proteins, the results continue to challenge analyses. Protein Language Models ({pLMs}) use the latest deep learning ({DL}) algorithms to leverage growing databases of protein sequences. These methods learn to predict missing or masked amino acids from the context of entire sequence regions. Here, we used {pLM} representations (embeddings) to predict sequence conservation and {SAV} effects without multiple sequence alignments ({MSAs}). Embeddings alone predicted residue conservation almost as accurately from single sequences as {ConSeq} using {MSAs} (two-state Matthews Correlation Coefficient—{MCC}—for {ProtT}5 embeddings of 0.596 ± 0.006 vs. 0.608 ± 0.006 for {ConSeq}). Inputting the conservation prediction along with {BLOSUM}62 substitution scores and {pLM} mask reconstruction probabilities into a simplistic logistic regression ({LR}) ensemble for Variant Effect Score Prediction without Alignments ({VESPA}) predicted {SAV} effect magnitude without any optimization on {DMS} data. Comparing predictions for a standard set of 39 {DMS} experiments to other methods (incl. {ESM}-1v, {DeepSequence}, and {GEMME}) revealed our approach as competitive with the state-of-the-art ({SOTA}) methods using {MSA} input. No method outperformed all others, neither consistently nor statistically significantly, independently of the performance measure applied (Spearman and Pearson correlation). Finally, we investigated binary effect predictions on {DMS} experiments for four human proteins. Overall, embedding-based methods have become competitive with methods relying on {MSAs} for {SAV} effect prediction at a fraction of the costs in computing/energy. Our method predicted {SAV} effects for the entire human proteome ({\textasciitilde} 20 k proteins) within 40 min on one Nvidia Quadro {RTX} 8000. All methods and data sets are freely available for local and online execution through bioembeddings.com, https://github.com/Rostlab/{VESPA}, and {PredictProtein}.},
	pages = {1629--1647},
	number = {10},
	journaltitle = {Human Genetics},
	shortjournal = {Hum Genet},
	author = {Marquet, Céline and Heinzinger, Michael and Olenyi, Tobias and Dallago, Christian and Erckert, Kyra and Bernhofer, Michael and Nechaev, Dmitrii and Rost, Burkhard},
	urldate = {2022-12-18},
	date = {2022},
	pmid = {34967936},
	pmcid = {PMC8716573},
	keywords = {notion},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/U6MGDD37/Marquet et al. - 2022 - Embeddings from protein language models predict co.pdf:application/pdf},
}

@misc{vafaii_functional_2022,
	title = {Functional network organization of the mouse cortex determined by wide-field fluorescence imaging shares some —but not all—properties revealed with simultaneous {fMRI}-{BOLD}},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.13.520297v1},
	doi = {10.1101/2022.12.13.520297},
	abstract = {Understanding the organization of large-scale brain networks remains a central problem in neuroscience. Work in both humans and rodents shows that the brain can be decomposed in terms of several networks (e.g., “default network”). Whereas the bulk of what we know is based on the blood-oxygenation level-dependent ({BOLD}) signal, the relationship between {BOLD} and neuronal activity is complex, which poses several challenges to interpreting networks revealed by this technique. To resolve these challenges, here we employed wide-field Ca2+ imaging simultaneously recorded with {fMRI}-{BOLD} in a highly-sampled group of {GCaMP}6f-expressing mice. This allowed us to determine the relationship between networks discovered by {BOLD} and Ca2+ signals both at the group level and in individual animals. Network partition used a mixed-membership stochastic blockmodel algorithm, which allows networks to be overlapping, such that a brain region may be assigned to multiple networks with varying strengths. Here, we tested the hypothesis that functional networks of the mouse brain are organized in an overlapping manner for both {BOLD} and Ca2+ data. Our findings demonstrate that {BOLD} large-scale networks can be detected via Ca2+ signals. In addition: (1) Overlapping networks were reliably estimated at the group level via random-effects statistical analysis. (2) Functional organization was generally similar for {BOLD} and Ca2+; e.g., with seven networks, cosine similarity was very high in five instances (values between 0.82-0.90; max possible value of 1), moderate in one case (0.73) and different in one case (0.26). (3) We discovered overlapping network organization in both data modalities, which was quantified in multiple ways; e.g., the proportion of regions that belonged to multiple networks was 75\% for {BOLD} and 60\% for Ca2+. (4) The large-scale functional organization of the mouse cortex as determined by Ca2+ signals is considerably more similar to that with {BOLD} when both signal modalities are considered in the low temporal frequency range (0.01 to 0.5 Hz). (5) Although many similarities were observed between data modalities, finer characterization uncovered key differences related to functional hubs believed to play important roles in the integration and/or segregation of signals. In particular, the spatial distribution of membership diversity (i.e., the extent to which a region affiliates with multiple networks), differed for the two types of signals. In conclusion, Ca2+ mesoscale signals revealed that the mouse cortex is functionally organized in terms of large-scale networks in a manner that reflects many of the properties observed using {BOLD}. Despite many similarities, important differences were also uncovered, suggesting that mesoscale Ca2+ has a strong potential to uncover additional properties of the large-scale organization of the mouse brain.},
	publisher = {{bioRxiv}},
	author = {Vafaii, Hadi and Mandino, Francesca and Desrosiers-Grégoire, Gabriel and O’Connor, David and Shen, Xilin and Ge, Xinxin and Herman, Peter and Hyder, Fahmeed and Papademetris, Xenophon and Chakravarty, Mallar and Crair, Michael C. and Constable, R. Todd and Lake, Evelyn {MR} and Pessoa, Luiz},
	urldate = {2022-12-18},
	date = {2022-12-15},
	langid = {english},
	note = {Pages: 2022.12.13.520297
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/NXLANAA4/Vafaii et al. - 2022 - Functional network organization of the mouse corte.pdf:application/pdf},
}

@misc{tyler_anti-correlated_2022,
	title = {Anti-correlated Feature Selection Prevents False Discovery of Subpopulations in {scRNAseq}},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.05.519161v1},
	doi = {10.1101/2022.12.05.519161},
	abstract = {While sub-clustering cell-populations has become popular in single cell-omics, negative controls for this process are lacking. Popular feature-selection/clustering algorithms fail the null-dataset problem, allowing erroneous subdivisions of homogenous clusters until nearly each cell is called its own cluster. Using 45,348 {scRNAseq} analyses of real and synthetic datasets, we found that anti-correlated gene selection reduces or eliminates erroneous subdivisions, increases marker-gene selection efficacy, and efficiently scales to 245k cells without the need for high-performance computing.},
	publisher = {{bioRxiv}},
	author = {Tyler, Scott R. and Guccione, Ernesto and Schadt, Eric E.},
	urldate = {2022-12-18},
	date = {2022-12-07},
	langid = {english},
	note = {Pages: 2022.12.05.519161
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3VYJHHCF/Tyler et al. - 2022 - Anti-correlated Feature Selection Prevents False D.pdf:application/pdf},
}

@online{noauthor_anti-correlated_nodate,
	title = {Anti-correlated Feature Selection Prevents False Discovery of Subpopulations in {scRNAseq} {\textbar} {bioRxiv}},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.05.519161v1},
	urldate = {2022-12-18},
}

@misc{kunes_gradient_2022,
	title = {Gradient Estimation for Binary Latent Variables via Gradient Variance Clipping},
	url = {http://arxiv.org/abs/2208.06124},
	doi = {10.48550/arXiv.2208.06124},
	abstract = {Gradient estimation is often necessary for fitting generative models with discrete latent variables, in contexts such as reinforcement learning and variational autoencoder ({VAE}) training. The {DisARM} estimator (Yin et al. 2020; Dong, Mnih, and Tucker 2020) achieves state of the art gradient variance for Bernoulli latent variable models in many contexts. However, {DisARM} and other estimators have potentially exploding variance near the boundary of the parameter space, where solutions tend to lie. To ameliorate this issue, we propose a new gradient estimator {\textbackslash}textit\{bitflip\}-1 that has lower variance at the boundaries of the parameter space. As bitflip-1 has complementary properties to existing estimators, we introduce an aggregated estimator, {\textbackslash}textit\{unbiased gradient variance clipping\} ({UGC}) that uses either a bitflip-1 or a {DisARM} gradient update for each coordinate. We theoretically prove that {UGC} has uniformly lower variance than {DisARM}. Empirically, we observe that {UGC} achieves the optimal value of the optimization objectives in toy experiments, discrete {VAE} training, and in a best subset selection problem.},
	number = {{arXiv}:2208.06124},
	publisher = {{arXiv}},
	author = {Kunes, Russell Z. and Yin, Mingzhang and Land, Max and Haviv, Doron and Pe'er, Dana and Tavaré, Simon},
	urldate = {2023-01-09},
	date = {2022-08-12},
	eprinttype = {arxiv},
	eprint = {2208.06124 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/Q3IU5WUL/Kunes et al. - 2022 - Gradient Estimation for Binary Latent Variables vi.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/NWA42UTV/2208.html:text/html},
}

@misc{geiger_e3nn_2022,
	title = {e3nn: Euclidean Neural Networks},
	url = {http://arxiv.org/abs/2207.09453},
	doi = {10.48550/arXiv.2207.09453},
	shorttitle = {e3nn},
	abstract = {We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the {TensorProduct} class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable {CNNs}, Clebsch-Gordan Networks, {SE}(3) Transformers and other E(3) equivariant networks.},
	number = {{arXiv}:2207.09453},
	publisher = {{arXiv}},
	author = {Geiger, Mario and Smidt, Tess},
	urldate = {2023-01-09},
	date = {2022-07-18},
	eprinttype = {arxiv},
	eprint = {2207.09453 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/9BQUV9AZ/Geiger and Smidt - 2022 - e3nn Euclidean Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/U6PLI86X/2207.html:text/html},
}

@article{lee_exact_2016,
	title = {Exact post-selection inference, with application to the lasso},
	volume = {44},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1311.6238},
	doi = {10.1214/15-AOS1371},
	abstract = {We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.},
	number = {3},
	journaltitle = {The Annals of Statistics},
	shortjournal = {Ann. Statist.},
	author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
	urldate = {2023-01-09},
	date = {2016-06-01},
	eprinttype = {arxiv},
	eprint = {1311.6238 [math, stat]},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DST568SC/Lee et al. - 2016 - Exact post-selection inference, with application t.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/IHCEU2WK/1311.html:text/html},
}

@article{chen_deepvelo_2022,
	title = {{DeepVelo}: Single-cell transcriptomic deep velocity field learning with neural ordinary differential equations},
	volume = {8},
	url = {https://www.science.org/doi/10.1126/sciadv.abq3745},
	doi = {10.1126/sciadv.abq3745},
	shorttitle = {{DeepVelo}},
	abstract = {Recent advances in single-cell sequencing technologies have provided unprecedented opportunities to measure the gene expression profile and {RNA} velocity of individual cells. However, modeling transcriptional dynamics is computationally challenging because of the high-dimensional, sparse nature of the single-cell gene expression measurements and the nonlinear regulatory relationships. Here, we present {DeepVelo}, a neural network–based ordinary differential equation that can model complex transcriptome dynamics by describing continuous-time gene expression changes within individual cells. We apply {DeepVelo} to public datasets from different sequencing platforms to (i) formulate transcriptome dynamics on different time scales, (ii) measure the instability of cell states, and (iii) identify developmental driver genes via perturbation analysis. Benchmarking against the state-of-the-art methods shows that {DeepVelo} can learn a more accurate representation of the velocity field. Furthermore, our perturbation studies reveal that single-cell dynamical systems could exhibit chaotic properties. In summary, {DeepVelo} allows data-driven discoveries of differential equations that delineate single-cell transcriptome dynamics.},
	pages = {eabq3745},
	number = {48},
	journaltitle = {Science Advances},
	author = {Chen, Zhanlin and King, William C. and Hwang, Aheyon and Gerstein, Mark and Zhang, Jing},
	urldate = {2023-01-10},
	date = {2022-11-30},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/home/ajl/Zotero/storage/62DDFI8V/Chen et al. - 2022 - DeepVelo Single-cell transcriptomic deep velocity.pdf:application/pdf},
}

@article{cabrera_sound_2022,
	title = {The sound of silence: Transgene silencing in mammalian cell engineering},
	volume = {13},
	issn = {2405-4712},
	url = {https://www.sciencedirect.com/science/article/pii/S2405471222004641},
	doi = {10.1016/j.cels.2022.11.005},
	shorttitle = {The sound of silence},
	abstract = {To elucidate principles operating in native biological systems and to develop novel biotechnologies, synthetic biology aims to build and integrate synthetic gene circuits within native transcriptional networks. The utility of synthetic gene circuits for cell engineering relies on the ability to control the expression of all constituent transgene components. Transgene silencing, defined as the loss of expression over time, persists as an obstacle for engineering primary cells and stem cells with transgenic cargos. In this review, we highlight the challenge that transgene silencing poses to the robust engineering of mammalian cells, outline potential molecular mechanisms of silencing, and present approaches for preventing transgene silencing. We conclude with a perspective identifying future research directions for improving the performance of synthetic gene circuits.},
	pages = {950--973},
	number = {12},
	journaltitle = {Cell Systems},
	shortjournal = {Cell Systems},
	author = {Cabrera, Alan and Edelstein, Hailey I. and Glykofrydis, Fokion and Love, Kasey S. and Palacios, Sebastian and Tycko, Josh and Zhang, Meng and Lensch, Sarah and Shields, Cara E. and Livingston, Mark and Weiss, Ron and Zhao, Huimin and Haynes, Karmella A. and Morsut, Leonardo and Chen, Yvonne Y. and Khalil, Ahmad S. and Wong, Wilson W. and Collins, James J. and Rosser, Susan J. and Polizzi, Karen and Elowitz, Michael B. and Fussenegger, Martin and Hilton, Isaac B. and Leonard, Joshua N. and Bintu, Lacramioara and Galloway, Kate E. and Deans, Tara L.},
	urldate = {2023-01-10},
	date = {2022-12-21},
	langid = {english},
	keywords = {genome engineering, mammalian synthetic biology, synthetic gene circuit stability, transgene silencing},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/QUM4QK9N/Cabrera et al. - 2022 - The sound of silence Transgene silencing in mamma.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/RBPXUZER/S2405471222004641.html:text/html},
}

@misc{kunes_supervised_2022,
	title = {Supervised discovery of interpretable gene programs from single-cell data},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2022.12.20.521311v1},
	doi = {10.1101/2022.12.20.521311},
	abstract = {Factor analysis can drive biological discovery by decomposing single-cell gene expression data into a minimal set of gene programs that correspond to processes executed by cells in a sample. However, matrix factorization methods are prone to technical artifacts and poor factor interpretability. We have developed Spectra, an algorithm that identifies user-provided gene programs, modifies them to dataset context as needed, and detects novel programs that together best explain expression covariation. Spectra overcomes the dominance of cell-type signals by modeling cell-type-specific programs, and can characterize interpretable cell states along a continuum. We show that it outperforms existing approaches in challenging tumor immune contexts; Spectra finds factors that change under immune checkpoint therapy, disentangles the highly correlated features of {CD}8+ T-cell tumor reactivity and exhaustion, finds a novel program that explains continuous macrophage state changes under therapy, and identifies cell-type-specific immune metabolic programs.},
	publisher = {{bioRxiv}},
	author = {Kunes, Russell Z. and Walle, Thomas and Nawy, Tal and Pe’er, Dana},
	urldate = {2023-01-10},
	date = {2022-12-21},
	langid = {english},
	note = {Pages: 2022.12.20.521311
Section: New Results},
	file = {Full Text PDF:/home/ajl/Zotero/storage/CV3IAMLB/Kunes et al. - 2022 - Supervised discovery of interpretable gene program.pdf:application/pdf},
}

@misc{tu_maxvit_2022,
	title = {{MaxViT}: Multi-Axis Vision Transformer},
	url = {http://arxiv.org/abs/2204.01697},
	doi = {10.48550/arXiv.2204.01697},
	shorttitle = {{MaxViT}},
	abstract = {Transformers have recently gained significant attention in the computer vision community. However, the lack of scalability of self-attention mechanisms with respect to image size has limited their wide adoption in state-of-the-art vision backbones. In this paper we introduce an efficient and scalable attention model we call multi-axis attention, which consists of two aspects: blocked local and dilated global attention. These design choices allow global-local spatial interactions on arbitrary input resolutions with only linear complexity. We also present a new architectural element by effectively blending our proposed attention model with convolutions, and accordingly propose a simple hierarchical vision backbone, dubbed {MaxViT}, by simply repeating the basic building block over multiple stages. Notably, {MaxViT} is able to ''see'' globally throughout the entire network, even in earlier, high-resolution stages. We demonstrate the effectiveness of our model on a broad spectrum of vision tasks. On image classification, {MaxViT} achieves state-of-the-art performance under various settings: without extra data, {MaxViT} attains 86.5\% {ImageNet}-1K top-1 accuracy; with {ImageNet}-21K pre-training, our model achieves 88.7\% top-1 accuracy. For downstream tasks, {MaxViT} as a backbone delivers favorable performance on object detection as well as visual aesthetic assessment. We also show that our proposed model expresses strong generative modeling capability on {ImageNet}, demonstrating the superior potential of {MaxViT} blocks as a universal vision module. The source code and trained models will be available at https://github.com/google-research/maxvit.},
	number = {{arXiv}:2204.01697},
	publisher = {{arXiv}},
	author = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and Yang, Feng and Milanfar, Peyman and Bovik, Alan and Li, Yinxiao},
	urldate = {2023-01-10},
	date = {2022-09-09},
	eprinttype = {arxiv},
	eprint = {2204.01697 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/4XSITFWW/Tu et al. - 2022 - MaxViT Multi-Axis Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/L8LGQ2NW/2204.html:text/html},
}

@misc{schlag_linear_2021,
	title = {Linear Transformers Are Secretly Fast Weight Programmers},
	url = {http://arxiv.org/abs/2102.11174},
	doi = {10.48550/arXiv.2102.11174},
	abstract = {We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow" neural net learns by gradient descent to program the ``fast weights" of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers ({FWPs}) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the {FWP} can more easily learn to correct the current mapping from keys to values. The {FWP} also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.},
	number = {{arXiv}:2102.11174},
	publisher = {{arXiv}},
	author = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, Jürgen},
	urldate = {2023-01-10},
	date = {2021-06-09},
	eprinttype = {arxiv},
	eprint = {2102.11174 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/F8FGUW43/Schlag et al. - 2021 - Linear Transformers Are Secretly Fast Weight Progr.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/6233KQW5/2102.html:text/html},
}

@misc{hatamizadeh_global_2022,
	title = {Global Context Vision Transformers},
	url = {http://arxiv.org/abs/2206.09959},
	doi = {10.48550/arXiv.2206.09959},
	abstract = {We propose global context vision transformer ({GC} {ViT}), a novel architecture that enhances parameter and compute utilization for computer vision tasks. The core of the novel model are global context self-attention modules, joint with standard local self-attention, to effectively yet efficiently model both long and short-range spatial interactions, as an alternative to complex operations such as an attention masks or local windows shifting. While the local self-attention modules are responsible for modeling short-range information, the global query tokens are shared across all global self-attention modules to interact with local key and values. In addition, we address the lack of inductive bias in {ViTs} and improve the modeling of inter-channel dependencies by proposing a novel downsampler which leverages a parameter-efficient fused inverted residual block. The proposed {GC} {ViT} achieves new state-of-the-art performance across image classification, object detection and semantic segmentation tasks. On {ImageNet}-1K dataset for classification, the tiny, small and base variants of {GC} {ViT} with 28M, 51M and 90M parameters achieve 83.4\%, 83.9\% and 84.4\% Top-1 accuracy, respectively, surpassing comparably-sized prior art such as {CNN}-based {ConvNeXt} and {ViT}-based Swin Transformer. Pre-trained {GC} {ViT} backbones in downstream tasks of object detection, instance segmentation, and semantic segmentation on {MS} {COCO} and {ADE}20K datasets outperform prior work consistently, sometimes by large margins. Code and pre-trained models are available at https://github.com/{NVlabs}/{GCViT}.},
	number = {{arXiv}:2206.09959},
	publisher = {{arXiv}},
	author = {Hatamizadeh, Ali and Yin, Hongxu and Kautz, Jan and Molchanov, Pavlo},
	urldate = {2023-01-10},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2206.09959 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/4LUKN6SW/Hatamizadeh et al. - 2022 - Global Context Vision Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/9K9G2CBP/2206.html:text/html},
}

@article{ghosh_noise_2005,
	title = {Noise characteristics of feed forward loops},
	volume = {2},
	issn = {1478-3975},
	doi = {10.1088/1478-3967/2/1/005},
	abstract = {A prominent feature of gene transcription regulatory networks is the presence in large numbers of motifs, i.e., patterns of interconnection, in the networks. One such motif is the feed forward loop ({FFL}) consisting of three genes X, Y and Z. The protein product x of X controls the synthesis of protein product y of Y. Proteins x and y jointly regulate the synthesis of z proteins from the gene Z. The {FFLs}, depending on the nature of the regulating interactions, can be of eight different types which can again be classified into two categories: coherent and incoherent. In this paper, we study the noise characteristics of {FFLs} using the Langevin formalism and the Monte Carlo simulation technique based on the Gillespie algorithm. We calculate the variances around the mean protein levels in the steady states of the {FFLs} and find that, in the case of coherent {FFLs}, the most abundant {FFL}, namely, the type-1 coherent {FFL}, is the least noisy. This is shown to be true for all parameter values when the {FFLs} operate above their thresholds of activation/repression. In the case of incoherent {FFLs}, no such general conclusion can be shown. The results suggest possible relationships between noise, functionality and abundance.},
	pages = {36--45},
	number = {1},
	journaltitle = {Physical Biology},
	shortjournal = {Phys Biol},
	author = {Ghosh, Bhaswar and Karmakar, Rajesh and Bose, Indrani},
	date = {2005-03},
	pmid = {16204855},
	keywords = {Time Factors, Amino Acid Motifs, Bacterial Proteins, Biophysics, Escherichia coli, Escherichia coli Proteins, Feedback, Physiological, Gene Expression Regulation, Bacterial, Gene Expression Regulation, Fungal, Models, Biological, Models, Chemical, Monte Carlo Method, Saccharomyces cerevisiae, Transcription Factors, Transcription, Genetic},
	file = {Submitted Version:/home/ajl/Zotero/storage/TTENG6DL/Ghosh et al. - 2005 - Noise characteristics of feed forward loops.pdf:application/pdf},
}

@article{brigato_image_2022,
	title = {Image Classification with Small Datasets: Overview and Benchmark},
	volume = {10},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/2212.12478},
	doi = {10.1109/ACCESS.2022.3172939},
	shorttitle = {Image Classification with Small Datasets},
	abstract = {Image classification with small datasets has been an active research area in the recent past. However, as research in this scope is still in its infancy, two key ingredients are missing for ensuring reliable and truthful progress: a systematic and extensive overview of the state of the art, and a common benchmark to allow for objective comparisons between published methods. This article addresses both issues. First, we systematically organize and connect past studies to consolidate a community that is currently fragmented and scattered. Second, we propose a common benchmark that allows for an objective comparison of approaches. It consists of five datasets spanning various domains (e.g., natural images, medical imagery, satellite data) and data types ({RGB}, grayscale, multispectral). We use this benchmark to re-evaluate the standard cross-entropy baseline and ten existing methods published between 2017 and 2021 at renowned venues. Surprisingly, we find that thorough hyper-parameter tuning on held-out validation data results in a highly competitive baseline and highlights a stunted growth of performance over the years. Indeed, only a single specialized method dating back to 2019 clearly wins our benchmark and outperforms the baseline classifier.},
	pages = {49233--49250},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Brigato, L. and Barz, B. and Iocchi, L. and Denzler, J.},
	urldate = {2023-01-10},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2212.12478 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DQWA33A9/Brigato et al. - 2022 - Image Classification with Small Datasets Overview.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/2S2S4AU5/2212.html:text/html},
}

@misc{wei_fully_2022,
	title = {Fully Differentiable {RANSAC}},
	url = {http://arxiv.org/abs/2212.13185},
	doi = {10.48550/arXiv.2212.13185},
	abstract = {We propose the fully differentiable \${\textbackslash}nabla\$-{RANSAC}.It predicts the inlier probabilities of the input data points, exploits the predictions in a guided sampler, and estimates the model parameters (e.g., fundamental matrix) and its quality while propagating the gradients through the entire procedure. The random sampler in \${\textbackslash}nabla\$-{RANSAC} is based on a clever re-parametrization strategy, i.e.{\textbackslash} the Gumbel Softmax sampler, that allows propagating the gradients directly into the subsequent differentiable minimal solver. The model quality function marginalizes over the scores from all models estimated within \${\textbackslash}nabla\$-{RANSAC} to guide the network learning accurate and useful probabilities.\${\textbackslash}nabla\$-{RANSAC} is the first to unlock the end-to-end training of geometric estimation pipelines, containing feature detection, matching and {RANSAC}-like randomized robust estimation. As a proof of its potential, we train \${\textbackslash}nabla\$-{RANSAC} together with {LoFTR}, i.e. a recent detector-free feature matcher, to find reliable correspondences in an end-to-end manner. We test \${\textbackslash}nabla\$-{RANSAC} on a number of real-world datasets on fundamental and essential matrix estimation. It is superior to the state-of-the-art in terms of accuracy while being among the fastest methods. The code and trained models will be made public.},
	number = {{arXiv}:2212.13185},
	publisher = {{arXiv}},
	author = {Wei, Tong and Patel, Yash and Matas, Jiri and Barath, Daniel},
	urldate = {2023-01-10},
	date = {2022-12-26},
	eprinttype = {arxiv},
	eprint = {2212.13185 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/MQDWWCEX/Wei et al. - 2022 - Fully Differentiable RANSAC.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/NBJJYS93/2212.html:text/html},
}

@misc{zhang_fixup_2019,
	title = {Fixup Initialization: Residual Learning Without Normalization},
	url = {http://arxiv.org/abs/1901.09321},
	doi = {10.48550/arXiv.1901.09321},
	shorttitle = {Fixup Initialization},
	abstract = {Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.},
	number = {{arXiv}:1901.09321},
	publisher = {{arXiv}},
	author = {Zhang, Hongyi and Dauphin, Yann N. and Ma, Tengyu},
	urldate = {2023-01-10},
	date = {2019-03-11},
	eprinttype = {arxiv},
	eprint = {1901.09321 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/TBS7ZVU2/Zhang et al. - 2019 - Fixup Initialization Residual Learning Without No.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/MWG4A5FY/1901.html:text/html},
}

@misc{ho_large_2022,
	title = {Large Language Models Are Reasoning Teachers},
	url = {http://arxiv.org/abs/2212.10071},
	doi = {10.48550/arXiv.2212.10071},
	abstract = {Language models ({LMs}) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought ({CoT}) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based {CoT} methods is restricted to very large {LMs} such as {GPT}-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller {LMs}, optimized to efficiently perform a specific task. We propose Fine-tune-{CoT}, a method that leverages the capabilities of very large {LMs} to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available {LMs} across a wide range of complex tasks and model sizes. We find that Fine-tune-{CoT} enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on {CoT} and address them in our analysis.},
	number = {{arXiv}:2212.10071},
	publisher = {{arXiv}},
	author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
	urldate = {2023-01-10},
	date = {2022-12-20},
	eprinttype = {arxiv},
	eprint = {2212.10071 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/XAC5S46J/Ho et al. - 2022 - Large Language Models Are Reasoning Teachers.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/ZAHVBXYV/2212.html:text/html},
}

@misc{chen_seeing_2022,
	title = {Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding},
	url = {http://arxiv.org/abs/2211.06956},
	doi = {10.48550/arXiv.2211.06956},
	shorttitle = {Seeing Beyond the Brain},
	abstract = {Decoding visual stimuli from brain recordings aims to deepen our understanding of the human visual system and build a solid foundation for bridging human and computer vision through the Brain-Computer Interface. However, reconstructing high-quality images with correct semantics from brain recordings is a challenging problem due to the complex underlying representations of brain signals and the scarcity of data annotations. In this work, we present {MinD}-Vis: Sparse Masked Brain Modeling with Double-Conditioned Latent Diffusion Model for Human Vision Decoding. Firstly, we learn an effective self-supervised representation of {fMRI} data using mask modeling in a large latent space inspired by the sparse coding of information in the primary visual cortex. Then by augmenting a latent diffusion model with double-conditioning, we show that {MinD}-Vis can reconstruct highly plausible images with semantically matching details from brain recordings using very few paired annotations. We benchmarked our model qualitatively and quantitatively; the experimental results indicate that our method outperformed state-of-the-art in both semantic mapping (100-way semantic classification) and generation quality ({FID}) by 66\% and 41\% respectively. An exhaustive ablation study was also conducted to analyze our framework.},
	number = {{arXiv}:2211.06956},
	publisher = {{arXiv}},
	author = {Chen, Zijiao and Qing, Jiaxin and Xiang, Tiange and Yue, Wan Lin and Zhou, Juan Helen},
	urldate = {2023-01-10},
	date = {2022-11-14},
	eprinttype = {arxiv},
	eprint = {2211.06956 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, {ACM}-class: I.4, I.5, J.3},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AYTDD6UA/Chen et al. - 2022 - Seeing Beyond the Brain Conditional Diffusion Mod.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/JULP7ZGW/2211.html:text/html},
}

@online{noauthor_deepvelo_nodate,
	title = {{DeepVelo}: Single-cell transcriptomic deep velocity field learning with neural ordinary differential equations {\textbar} Science Advances},
	url = {https://www.science.org/doi/10.1126/sciadv.abq3745},
	urldate = {2023-01-10},
	file = {DeepVelo\: Single-cell transcriptomic deep velocity field learning with neural ordinary differential equations | Science Advances:/home/ajl/Zotero/storage/7W66H9KM/sciadv.html:text/html},
}

@misc{doshi_critical_2022,
	title = {Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians: General Theory and Applications},
	url = {http://arxiv.org/abs/2111.12143},
	doi = {10.48550/arXiv.2111.12143},
	shorttitle = {Critical Initialization of Wide and Deep Neural Networks through Partial Jacobians},
	abstract = {Deep neural networks are notorious for defying theoretical treatment. However, when the number of parameters in each layer tends to infinity the network function is a Gaussian process ({GP}) and quantitatively predictive description is possible. Gaussian approximation allows to formulate criteria for selecting hyperparameters, such as variances of weights and biases, as well as the learning rate. These criteria rely on the notion of criticality defined for deep neural networks. In this work we describe a new practical way to diagnose criticality. We introduce {\textbackslash}emph\{partial Jacobians\} of a network, defined as derivatives of preactivations in layer \$l\$ with respect to preactivations in layer \$l\_0{\textbackslash}leq l\$. We derive recurrence relations for the norms of partial Jacobians and utilize these relations to analyze criticality of deep fully connected neural networks with {LayerNorm} and/or residual connections. We derive and implement a simple and cheap numerical test that allows to select optimal initialization for a broad class of deep neural networks. Using these tools we show quantitatively that proper stacking of the {LayerNorm} (applied to preactivations) and residual connections leads to an architecture that is critical for any initialization. Finally, we apply our methods to analyze the {MLP}-Mixer architecture and show that it is everywhere critical.},
	number = {{arXiv}:2111.12143},
	publisher = {{arXiv}},
	author = {Doshi, Darshil and He, Tianyu and Gromov, Andrey},
	urldate = {2023-01-10},
	date = {2022-01-26},
	eprinttype = {arxiv},
	eprint = {2111.12143 [cond-mat, physics:hep-th, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Theory},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/P65ICDQK/Doshi et al. - 2022 - Critical Initialization of Wide and Deep Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/GGZNIZJ3/2111.html:text/html},
}

@misc{peer_improving_2022,
	title = {Improving the Trainability of Deep Neural Networks through Layerwise Batch-Entropy Regularization},
	url = {http://arxiv.org/abs/2208.01134},
	doi = {10.48550/arXiv.2208.01134},
	abstract = {Training deep neural networks is a very demanding task, especially challenging is how to adapt architectures to improve the performance of trained models. We can find that sometimes, shallow networks generalize better than deep networks, and the addition of more layers results in higher training and test errors. The deep residual learning framework addresses this degradation problem by adding skip connections to several neural network layers. It would at first seem counter-intuitive that such skip connections are needed to train deep networks successfully as the expressivity of a network would grow exponentially with depth. In this paper, we first analyze the flow of information through neural networks. We introduce and evaluate the batch-entropy which quantifies the flow of information through each layer of a neural network. We prove empirically and theoretically that a positive batch-entropy is required for gradient descent-based training approaches to optimize a given loss function successfully. Based on those insights, we introduce batch-entropy regularization to enable gradient descent-based training algorithms to optimize the flow of information through each hidden layer individually. With batch-entropy regularization, gradient descent optimizers can transform untrainable networks into trainable networks. We show empirically that we can therefore train a "vanilla" fully connected network and convolutional neural network -- no skip connections, batch normalization, dropout, or any other architectural tweak -- with 500 layers by simply adding the batch-entropy regularization term to the loss function. The effect of batch-entropy regularization is not only evaluated on vanilla neural networks, but also on residual networks, autoencoders, and also transformer models over a wide range of computer vision as well as natural language processing tasks.},
	number = {{arXiv}:2208.01134},
	publisher = {{arXiv}},
	author = {Peer, David and Keulen, Bart and Stabinger, Sebastian and Piater, Justus and Rodríguez-Sánchez, Antonio},
	urldate = {2023-01-10},
	date = {2022-08-01},
	eprinttype = {arxiv},
	eprint = {2208.01134 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/D9K3JXKQ/Peer et al. - 2022 - Improving the Trainability of Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/K2N86U5X/2208.html:text/html},
}

@misc{schubert_stop_2022,
	title = {Stop using the elbow criterion for k-means and how to choose the number of clusters instead},
	url = {http://arxiv.org/abs/2212.12189},
	doi = {10.48550/arXiv.2212.12189},
	abstract = {A major challenge when using k-means clustering often is how to choose the parameter k, the number of clusters. In this letter, we want to point out that it is very easy to draw poor conclusions from a common heuristic, the "elbow method". Better alternatives have been known in literature for a long time, and we want to draw attention to some of these easy to use options, that often perform better. This letter is a call to stop using the elbow method altogether, because it severely lacks theoretic support, and we want to encourage educators to discuss the problems of the method -- if introducing it in class at all -- and teach alternatives instead, while researchers and reviewers should reject conclusions drawn from the elbow method.},
	number = {{arXiv}:2212.12189},
	publisher = {{arXiv}},
	author = {Schubert, Erich},
	urldate = {2023-01-10},
	date = {2022-12-23},
	eprinttype = {arxiv},
	eprint = {2212.12189 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/SXZZTT6J/Schubert - 2022 - Stop using the elbow criterion for k-means and how.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/SJUMG64I/2212.html:text/html},
}

@online{noauthor_sound_nodate,
	title = {The sound of silence: Transgene silencing in mammalian cell engineering - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S2405471222004641},
	urldate = {2023-01-10},
	keywords = {notion},
	file = {The sound of silence\: Transgene silencing in mammalian cell engineering - ScienceDirect:/home/ajl/Zotero/storage/CCDSX6PK/S2405471222004641.html:text/html},
}

@misc{greydanus_scaling_2020,
	title = {Scaling down Deep Learning},
	url = {http://arxiv.org/abs/2011.14439},
	doi = {10.48550/arXiv.2011.14439},
	abstract = {Though deep learning models have taken on commercial and political relevance, many aspects of their training and operation remain poorly understood. This has sparked interest in "science of deep learning" projects, many of which are run at scale and require enormous amounts of time, money, and electricity. But how much of this research really needs to occur at scale? In this paper, we introduce {MNIST}-1D: a minimalist, low-memory, and low-compute alternative to classic deep learning benchmarks. The training examples are 20 times smaller than {MNIST} examples yet they differentiate more clearly between linear, nonlinear, and convolutional models which attain 32, 68, and 94\% accuracy respectively (these models obtain 94, 99+, and 99+\% on {MNIST}). Then we present example use cases which include measuring the spatial inductive biases of lottery tickets, observing deep double descent, and metalearning an activation function.},
	number = {{arXiv}:2011.14439},
	publisher = {{arXiv}},
	author = {Greydanus, Sam},
	urldate = {2023-01-10},
	date = {2020-12-04},
	eprinttype = {arxiv},
	eprint = {2011.14439 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/3XX3H6X7/Greydanus - 2020 - Scaling down Deep Learning.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/BMIY8X8U/2011.html:text/html},
}

@misc{frankle_lottery_2019,
	title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	url = {http://arxiv.org/abs/1803.03635},
	doi = {10.48550/arXiv.1803.03635},
	shorttitle = {The Lottery Ticket Hypothesis},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for {MNIST} and {CIFAR}10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	number = {{arXiv}:1803.03635},
	publisher = {{arXiv}},
	author = {Frankle, Jonathan and Carbin, Michael},
	urldate = {2023-01-10},
	date = {2019-03-04},
	eprinttype = {arxiv},
	eprint = {1803.03635 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/NIKSA46L/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/8WDSHYCH/1803.html:text/html},
}

@misc{baevski_efficient_2022,
	title = {Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language},
	url = {http://arxiv.org/abs/2212.07525},
	doi = {10.48550/arXiv.2212.07525},
	abstract = {Current self-supervised learning algorithms are often modality-specific and require large amounts of computational resources. To address these issues, we increase the training efficiency of data2vec, a learning objective that generalizes across several modalities. We do not encode masked tokens, use a fast convolutional decoder and amortize the effort to build teacher representations. data2vec 2.0 benefits from the rich contextualized target representations introduced in data2vec which enable a fast self-supervised learner. Experiments on {ImageNet}-1K image classification show that data2vec 2.0 matches the accuracy of Masked Autoencoders in 16.4x lower pre-training time, on Librispeech speech recognition it performs as well as wav2vec 2.0 in 10.6x less time, and on {GLUE} natural language understanding it matches a retrained {RoBERTa} model in half the time. Trading some speed for accuracy results in {ImageNet}-1K top-1 accuracy of 86.8{\textbackslash}\% with a {ViT}-L model trained for 150 epochs.},
	number = {{arXiv}:2212.07525},
	publisher = {{arXiv}},
	author = {Baevski, Alexei and Babu, Arun and Hsu, Wei-Ning and Auli, Michael},
	urldate = {2023-01-10},
	date = {2022-12-14},
	eprinttype = {arxiv},
	eprint = {2212.07525 [cs, eess]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ZJG4TQAI/Baevski et al. - 2022 - Efficient Self-supervised Learning with Contextual.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/EGHWH3JU/2212.html:text/html},
}

@misc{ibarz_generalist_2022,
	title = {A Generalist Neural Algorithmic Learner},
	url = {http://arxiv.org/abs/2209.11142},
	doi = {10.48550/arXiv.2209.11142},
	abstract = {The cornerstone of neural algorithmic reasoning is the ability to solve algorithmic tasks, especially in a way that generalises out of distribution. While recent years have seen a surge in methodological improvements in this area, they mostly focused on building specialist models. Specialist models are capable of learning to neurally execute either only one algorithm or a collection of algorithms with identical control-flow backbone. Here, instead, we focus on constructing a generalist neural algorithmic learner -- a single graph neural network processor capable of learning to execute a wide range of algorithms, such as sorting, searching, dynamic programming, path-finding and geometry. We leverage the {CLRS} benchmark to empirically show that, much like recent successes in the domain of perception, generalist algorithmic learners can be built by "incorporating" knowledge. That is, it is possible to effectively learn algorithms in a multi-task manner, so long as we can learn to execute them well in a single-task regime. Motivated by this, we present a series of improvements to the input representation, training regime and processor architecture over {CLRS}, improving average single-task performance by over 20\% from prior art. We then conduct a thorough ablation of multi-task learners leveraging these improvements. Our results demonstrate a generalist learner that effectively incorporates knowledge captured by specialist models.},
	number = {{arXiv}:2209.11142},
	publisher = {{arXiv}},
	author = {Ibarz, Borja and Kurin, Vitaly and Papamakarios, George and Nikiforou, Kyriacos and Bennani, Mehdi and Csordás, Róbert and Dudzik, Andrew and Bošnjak, Matko and Vitvitskyi, Alex and Rubanova, Yulia and Deac, Andreea and Bevilacqua, Beatrice and Ganin, Yaroslav and Blundell, Charles and Veličković, Petar},
	urldate = {2023-01-10},
	date = {2022-12-03},
	eprinttype = {arxiv},
	eprint = {2209.11142 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/Q28MJNNP/Ibarz et al. - 2022 - A Generalist Neural Algorithmic Learner.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/3Z7VT5DM/2209.html:text/html},
}

@article{benjamin_efficient_2022,
	title = {Efficient neural codes naturally emerge through gradient descent learning},
	volume = {13},
	rights = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-35659-7},
	doi = {10.1038/s41467-022-35659-7},
	abstract = {Human sensory systems are more sensitive to common features in the environment than uncommon features. For example, small deviations from the more frequently encountered horizontal orientations can be more easily detected than small deviations from the less frequent diagonal ones. Here we find that artificial neural networks trained to recognize objects also have patterns of sensitivity that match the statistics of features in images. To interpret these findings, we show mathematically that learning with gradient descent in neural networks preferentially creates representations that are more sensitive to common features, a hallmark of efficient coding. This effect occurs in systems with otherwise unconstrained coding resources, and additionally when learning towards both supervised and unsupervised objectives. This result demonstrates that efficient codes can naturally emerge from gradient-like learning.},
	pages = {7972},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Benjamin, Ari S. and Zhang, Ling-Qi and Qiu, Cheng and Stocker, Alan A. and Kording, Konrad P.},
	urldate = {2023-01-10},
	date = {2022-12-29},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Network models, Human behaviour, Pattern vision},
	file = {Full Text PDF:/home/ajl/Zotero/storage/HFD6J35U/Benjamin et al. - 2022 - Efficient neural codes naturally emerge through gr.pdf:application/pdf},
}

@misc{geiping_cramming_2022,
	title = {Cramming: Training a Language Model on a Single {GPU} in One Day},
	url = {http://arxiv.org/abs/2212.14034},
	doi = {10.48550/arXiv.2212.14034},
	shorttitle = {Cramming},
	abstract = {Recent trends in language modeling have focused on increasing performance through scaling, and have resulted in an environment where training language models is out of reach for most researchers and practitioners. While most in the community are asking how to push the limits of extreme computation, we ask the opposite question: How far can we get with a single {GPU} in just one day? We investigate the downstream performance achievable with a transformer-based language model trained completely from scratch with masked language modeling for a single day on a single consumer {GPU}. Aside from re-analyzing nearly all components of the pretraining pipeline for this scenario and providing a modified pipeline with performance close to {BERT}, we investigate why scaling down is hard, and which modifications actually improve performance in this scenario. We provide evidence that even in this constrained setting, performance closely follows scaling laws observed in large-compute settings. Through the lens of scaling laws, we categorize a range of recent improvements to training and architecture and discuss their merit and practical applicability (or lack thereof) for the limited compute setting.},
	number = {{arXiv}:2212.14034},
	publisher = {{arXiv}},
	author = {Geiping, Jonas and Goldstein, Tom},
	urldate = {2023-01-10},
	date = {2022-12-28},
	eprinttype = {arxiv},
	eprint = {2212.14034 [cs]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ITE86U2R/Geiping and Goldstein - 2022 - Cramming Training a Language Model on a Single GP.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/5BV8RXV3/2212.html:text/html},
}

@article{rapisardi_percolation_2022,
	title = {Percolation in networks with local homeostatic plasticity},
	volume = {13},
	rights = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-27736-0},
	doi = {10.1038/s41467-021-27736-0},
	abstract = {Percolation is a process that impairs network connectedness by deactivating links or nodes. This process features a phase transition that resembles paradigmatic critical transitions in epidemic spreading, biological networks, traffic and transportation systems. Some biological systems, such as networks of neural cells, actively respond to percolation-like damage, which enables these structures to maintain their function after degradation and aging. Here we study percolation in networks that actively respond to link damage by adopting a mechanism resembling synaptic scaling in neurons. We explain critical transitions in such active networks and show that these structures are more resilient to damage as they are able to maintain a stronger connectedness and ability to spread information. Moreover, we uncover the role of local rescaling strategies in biological networks and indicate a possibility of designing smart infrastructures with improved robustness to perturbations.},
	pages = {122},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Rapisardi, Giacomo and Kryven, Ivan and Arenas, Alex},
	urldate = {2023-01-20},
	date = {2022-01-10},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Complex networks, Stress and resilience},
	file = {Full Text PDF:/home/ajl/Zotero/storage/PVZ9AV86/Rapisardi et al. - 2022 - Percolation in networks with local homeostatic pla.pdf:application/pdf},
}

@misc{bergstra_statistical_2011,
	title = {The Statistical Inefficiency of Sparse Coding for Images (or, One Gabor to Rule them All)},
	url = {http://arxiv.org/abs/1109.6638},
	doi = {10.48550/arXiv.1109.6638},
	abstract = {Sparse coding is a proven principle for learning compact representations of images. However, sparse coding by itself often leads to very redundant dictionaries. With images, this often takes the form of similar edge detectors which are replicated many times at various positions, scales and orientations. An immediate consequence of this observation is that the estimation of the dictionary components is not statistically efficient. We propose a factored model in which factors of variation (e.g. position, scale and orientation) are untangled from the underlying Gabor-like filters. There is so much redundancy in sparse codes for natural images that our model requires only a single dictionary element (a Gabor-like edge detector) to outperform standard sparse coding. Our model scales naturally to arbitrary-sized images while achieving much greater statistical efficiency during learning. We validate this claim with a number of experiments showing, in part, superior compression of out-of-sample data using a sparse coding dictionary learned with only a single image.},
	number = {{arXiv}:1109.6638},
	publisher = {{arXiv}},
	author = {Bergstra, James and Courville, Aaron and Bengio, Yoshua},
	urldate = {2023-01-21},
	date = {2011-09-30},
	eprinttype = {arxiv},
	eprint = {1109.6638 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/UDRGL78H/Bergstra et al. - 2011 - The Statistical Inefficiency of Sparse Coding for .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/N8DWEW2S/1109.html:text/html},
}

@article{rao_exploring_2021,
	title = {Exploring tissue architecture using spatial transcriptomics},
	volume = {596},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8475179/},
	doi = {10.1038/s41586-021-03634-9},
	abstract = {Deciphering the principles and mechanisms by which gene activity orchestrates complex cellular arrangements in multicellular organisms has far-reaching implications for research in the life sciences. Recent technological advancements in next-generation sequencing-based and imaging-based approaches have established the potential of spatial transcriptomics to measure expression levels of all or most genes systematically throughout tissue space, and have been adopted to generate biological insight in neuroscience, development, plant biology, and a range of diseases including cancer. Similar to datasets made possible by genomic sequencing and population health surveys, the large-scale atlases generated by this technology lend themselves to exploratory data analysis for hypothesis generation. Here, we review spatial transcriptomic technologies and describe the repertoire of operations available for paths of analysis of the resulting data. Spatial transcriptomics can also be deployed for hypothesis testing using experimental designs comparing timepoints or conditions - including genetic or environmental perturbations. Finally, spatial transcriptomic data is naturally amenable to integration with other data modalities providing an expandable framework for insight into tissue organization.},
	pages = {211--220},
	number = {7871},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Rao, Anjali and Barkley, Dalia and França, Gustavo S. and Yanai, Itai},
	urldate = {2023-01-27},
	date = {2021-08},
	pmid = {34381231},
	pmcid = {PMC8475179},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/9I62JQLP/Rao et al. - 2021 - Exploring tissue architecture using spatial transc.pdf:application/pdf},
}

@misc{tian_designing_2023,
	title = {Designing {BERT} for Convolutional Networks: Sparse and Hierarchical Masked Modeling},
	url = {http://arxiv.org/abs/2301.03580},
	doi = {10.48550/arXiv.2301.03580},
	shorttitle = {Designing {BERT} for Convolutional Networks},
	abstract = {We identify and overcome two key obstacles in extending the success of {BERT}-style pre-training, or the masked image modeling, to convolutional networks (convnets): (i) convolution operation cannot handle irregular, random-masked input images; (ii) the single-scale nature of {BERT} pre-training is inconsistent with convnet's hierarchical structure. For (i), we treat unmasked pixels as sparse voxels of 3D point clouds and use sparse convolution to encode. This is the first use of sparse convolution for 2D masked modeling. For (ii), we develop a hierarchical decoder to reconstruct images from multi-scale encoded features. Our method called Sparse {masKed} modeling ({SparK}) is general: it can be used directly on any convolutional model without backbone modifications. We validate it on both classical ({ResNet}) and modern ({ConvNeXt}) models: on three downstream tasks, it surpasses both state-of-the-art contrastive learning and transformer-based masked modeling by similarly large margins (around +1.0\%). Improvements on object detection and instance segmentation are more substantial (up to +3.5\%), verifying the strong transferability of features learned. We also find its favorable scaling behavior by observing more gains on larger models. All this evidence reveals a promising future of generative pre-training on convnets. Codes and models are released at https://github.com/keyu-tian/{SparK}.},
	number = {{arXiv}:2301.03580},
	publisher = {{arXiv}},
	author = {Tian, Keyu and Jiang, Yi and Diao, Qishuai and Lin, Chen and Wang, Liwei and Yuan, Zehuan},
	urldate = {2023-01-29},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2301.03580 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/8NXCZKQ7/Tian et al. - 2023 - Designing BERT for Convolutional Networks Sparse .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/GCDIC3LN/2301.html:text/html},
}

@misc{xie_simmim_2022,
	title = {{SimMIM}: A Simple Framework for Masked Image Modeling},
	url = {http://arxiv.org/abs/2111.09886},
	doi = {10.48550/arXiv.2111.09886},
	shorttitle = {{SimMIM}},
	abstract = {This paper presents {SimMIM}, a simple framework for masked image modeling. We simplify recently proposed related approaches without special designs such as block-wise masking and tokenization via discrete {VAE} or clustering. To study what let the masked image modeling task learn good representations, we systematically study the major components in our framework, and find that simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting raw pixels of {RGB} values by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using {ViT}-B, our approach achieves 83.8\% top-1 fine-tuning accuracy on {ImageNet}-1K by pre-training also on this dataset, surpassing previous best approach by +0.6\%. When applied on a larger model of about 650 million parameters, {SwinV}2-H, it achieves 87.1\% top-1 accuracy on {ImageNet}-1K using only {ImageNet}-1K data. We also leverage this approach to facilitate the training of a 3B model ({SwinV}2-G), that by \$40{\textbackslash}times\$ less data than that in previous practice, we achieve the state-of-the-art on four representative vision benchmarks. The code and models will be publicly available at https://github.com/microsoft/{SimMIM}.},
	number = {{arXiv}:2111.09886},
	publisher = {{arXiv}},
	author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
	urldate = {2023-01-29},
	date = {2022-04-17},
	eprinttype = {arxiv},
	eprint = {2111.09886 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/YGCLHYRQ/Xie et al. - 2022 - SimMIM A Simple Framework for Masked Image Modeli.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/7JM2NFBN/2111.html:text/html},
}

@misc{bao_beit_2022,
	title = {{BEiT}: {BERT} Pre-Training of Image Transformers},
	url = {http://arxiv.org/abs/2106.08254},
	doi = {10.48550/arXiv.2106.08254},
	shorttitle = {{BEiT}},
	abstract = {We introduce a self-supervised vision representation model {BEiT}, which stands for Bidirectional Encoder representation from Image Transformers. Following {BERT} developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training {BEiT}, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size {BEiT} achieves 83.2\% top-1 accuracy on {ImageNet}-1K, significantly outperforming from-scratch {DeiT} training (81.8\%) with the same setup. Moreover, large-size {BEiT} obtains 86.3\% only using {ImageNet}-1K, even outperforming {ViT}-L with supervised pre-training on {ImageNet}-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
	number = {{arXiv}:2106.08254},
	publisher = {{arXiv}},
	author = {Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
	urldate = {2023-01-29},
	date = {2022-09-03},
	eprinttype = {arxiv},
	eprint = {2106.08254 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JH9G56R5/Bao et al. - 2022 - BEiT BERT Pre-Training of Image Transformers.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/482Q2C8F/2106.html:text/html},
}

@misc{graham_submanifold_2017,
	title = {Submanifold Sparse Convolutional Networks},
	url = {http://arxiv.org/abs/1706.01307},
	doi = {10.48550/arXiv.1706.01307},
	abstract = {Convolutional network are the de-facto standard for analysing spatio-temporal data such as images, videos, 3D shapes, etc. Whilst some of this data is naturally dense (for instance, photos), many other data sources are inherently sparse. Examples include pen-strokes forming on a piece of paper, or (colored) 3D point clouds that were obtained using a {LiDAR} scanner or {RGB}-D camera. Standard "dense" implementations of convolutional networks are very inefficient when applied on such sparse data. We introduce a sparse convolutional operation tailored to processing sparse data that differs from prior work on sparse convolutional networks in that it operates strictly on submanifolds, rather than "dilating" the observation with every layer in the network. Our empirical analysis of the resulting submanifold sparse convolutional networks shows that they perform on par with state-of-the-art methods whilst requiring substantially less computation.},
	number = {{arXiv}:1706.01307},
	publisher = {{arXiv}},
	author = {Graham, Benjamin and van der Maaten, Laurens},
	urldate = {2023-01-29},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01307 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DPHEK7SD/Graham and van der Maaten - 2017 - Submanifold Sparse Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/MUYHYBD2/1706.html:text/html},
}

@misc{woo_convnext_2023-1,
	title = {{ConvNeXt} V2: Co-designing and Scaling {ConvNets} with Masked Autoencoders},
	url = {http://arxiv.org/abs/2301.00808},
	doi = {10.48550/arXiv.2301.00808},
	shorttitle = {{ConvNeXt} V2},
	abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern {ConvNets}, represented by {ConvNeXt}, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with {ImageNet} labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders ({MAE}). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization ({GRN}) layer that can be added to the {ConvNeXt} architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called {ConvNeXt} V2, which significantly improves the performance of pure {ConvNets} on various recognition benchmarks, including {ImageNet} classification, {COCO} detection, and {ADE}20K segmentation. We also provide pre-trained {ConvNeXt} V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on {ImageNet}, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
	number = {{arXiv}:2301.00808},
	publisher = {{arXiv}},
	author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
	urldate = {2023-01-29},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2301.00808 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/EESNXMH3/Woo et al. - 2023 - ConvNeXt V2 Co-designing and Scaling ConvNets wit.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/7AXTKHIJ/2301.html:text/html},
}

@online{noauthor_papers_nodate,
	title = {Papers with Code - {InternImage}: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions},
	url = {https://paperswithcode.com/paper/internimage-exploring-large-scale-vision},
	shorttitle = {Papers with Code - {InternImage}},
	abstract = {🏆 {SOTA} for Object Detection on {COCO} test-dev (box {AP} metric)},
	urldate = {2023-02-03},
	langid = {english},
}

@misc{wang_internimage_2022,
	title = {{InternImage}: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions},
	url = {http://arxiv.org/abs/2211.05778},
	doi = {10.48550/arXiv.2211.05778},
	shorttitle = {{InternImage}},
	abstract = {Compared to the great progress of large-scale vision transformers ({ViTs}) in recent years, large-scale models based on convolutional neural networks ({CNNs}) are still in an early state. This work presents a new large-scale {CNN}-based foundation model, termed {InternImage}, which can obtain the gain from increasing parameters and training data like {ViTs}. Different from the recent {CNNs} that focus on large dense kernels, {InternImage} takes deformable convolution as the core operator, so that our model not only has the large effective receptive field required for downstream tasks such as detection and segmentation, but also has the adaptive spatial aggregation conditioned by input and task information. As a result, the proposed {InternImage} reduces the strict inductive bias of traditional {CNNs} and makes it possible to learn stronger and more robust patterns with large-scale parameters from massive data like {ViTs}. The effectiveness of our model is proven on challenging benchmarks including {ImageNet}, {COCO}, and {ADE}20K. It is worth mentioning that {InternImage}-H achieved a new record 65.4 {mAP} on {COCO} test-dev and 62.9 {mIoU} on {ADE}20K, outperforming current leading {CNNs} and {ViTs}. The code will be released at https://github.com/{OpenGVLab}/{InternImage}.},
	number = {{arXiv}:2211.05778},
	publisher = {{arXiv}},
	author = {Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and Wang, Xiaogang and Qiao, Yu},
	urldate = {2023-02-03},
	date = {2022-11-13},
	eprinttype = {arxiv},
	eprint = {2211.05778 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/566MHHJW/Wang et al. - 2022 - InternImage Exploring Large-Scale Vision Foundati.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/3U9SSUYN/2211.html:text/html},
}

@misc{woo_convnext_2023-2,
	title = {{ConvNeXt} V2: Co-designing and Scaling {ConvNets} with Masked Autoencoders},
	url = {http://arxiv.org/abs/2301.00808},
	doi = {10.48550/arXiv.2301.00808},
	shorttitle = {{ConvNeXt} V2},
	abstract = {Driven by improved architectures and better representation learning frameworks, the field of visual recognition has enjoyed rapid modernization and performance boost in the early 2020s. For example, modern {ConvNets}, represented by {ConvNeXt}, have demonstrated strong performance in various scenarios. While these models were originally designed for supervised learning with {ImageNet} labels, they can also potentially benefit from self-supervised learning techniques such as masked autoencoders ({MAE}). However, we found that simply combining these two approaches leads to subpar performance. In this paper, we propose a fully convolutional masked autoencoder framework and a new Global Response Normalization ({GRN}) layer that can be added to the {ConvNeXt} architecture to enhance inter-channel feature competition. This co-design of self-supervised learning techniques and architectural improvement results in a new model family called {ConvNeXt} V2, which significantly improves the performance of pure {ConvNets} on various recognition benchmarks, including {ImageNet} classification, {COCO} detection, and {ADE}20K segmentation. We also provide pre-trained {ConvNeXt} V2 models of various sizes, ranging from an efficient 3.7M-parameter Atto model with 76.7\% top-1 accuracy on {ImageNet}, to a 650M Huge model that achieves a state-of-the-art 88.9\% accuracy using only public training data.},
	number = {{arXiv}:2301.00808},
	publisher = {{arXiv}},
	author = {Woo, Sanghyun and Debnath, Shoubhik and Hu, Ronghang and Chen, Xinlei and Liu, Zhuang and Kweon, In So and Xie, Saining},
	urldate = {2023-02-03},
	date = {2023-01-02},
	eprinttype = {arxiv},
	eprint = {2301.00808 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/JHGE2IC4/Woo et al. - 2023 - ConvNeXt V2 Co-designing and Scaling ConvNets wit.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/HCG882UL/2301.html:text/html},
}

@misc{gupta_patch_2023,
	title = {Patch Gradient Descent: Training Neural Networks on Very Large Images},
	url = {http://arxiv.org/abs/2301.13817},
	doi = {10.48550/arXiv.2301.13817},
	shorttitle = {Patch Gradient Descent},
	abstract = {Traditional {CNN} models are trained and tested on relatively low resolution images ({\textless}300 px), and cannot be directly operated on large-scale images due to compute and memory constraints. We propose Patch Gradient Descent ({PatchGD}), an effective learning strategy that allows to train the existing {CNN} architectures on large-scale images in an end-to-end manner. {PatchGD} is based on the hypothesis that instead of performing gradient-based updates on an entire image at once, it should be possible to achieve a good solution by performing model updates on only small parts of the image at a time, ensuring that the majority of it is covered over the course of iterations. {PatchGD} thus extensively enjoys better memory and compute efficiency when training models on large scale images. {PatchGD} is thoroughly evaluated on two datasets - {PANDA} and {UltraMNIST} with {ResNet}50 and {MobileNetV}2 models under different memory constraints. Our evaluation clearly shows that {PatchGD} is much more stable and efficient than the standard gradient-descent method in handling large images, and especially when the compute memory is limited.},
	number = {{arXiv}:2301.13817},
	publisher = {{arXiv}},
	author = {Gupta, Deepak K. and Mago, Gowreesh and Chavan, Arnav and Prasad, Dilip K.},
	urldate = {2023-02-03},
	date = {2023-01-31},
	eprinttype = {arxiv},
	eprint = {2301.13817 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AS8TS998/Gupta et al. - 2023 - Patch Gradient Descent Training Neural Networks o.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/NYM2CCT3/2301.html:text/html},
}

@misc{dai_coatnet_2021,
	title = {{CoAtNet}: Marrying Convolution and Attention for All Data Sizes},
	url = {http://arxiv.org/abs/2106.04803},
	doi = {10.48550/arXiv.2106.04803},
	shorttitle = {{CoAtNet}},
	abstract = {Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present {CoAtNets}(pronounced "coat" nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our {CoAtNets} achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, {CoAtNet} achieves 86.0\% {ImageNet} top-1 accuracy; When pre-trained with 13M images from {ImageNet}-21K, our {CoAtNet} achieves 88.56\% top-1 accuracy, matching {ViT}-huge pre-trained with 300M images from {JFT}-300M while using 23x less data; Notably, when we further scale up {CoAtNet} with {JFT}-3B, it achieves 90.88\% top-1 accuracy on {ImageNet}, establishing a new state-of-the-art result.},
	number = {{arXiv}:2106.04803},
	publisher = {{arXiv}},
	author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V. and Tan, Mingxing},
	urldate = {2023-02-03},
	date = {2021-09-15},
	eprinttype = {arxiv},
	eprint = {2106.04803 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/BCF7BM97/Dai et al. - 2021 - CoAtNet Marrying Convolution and Attention for Al.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/XD536KYX/2106.html:text/html},
}

@misc{fang_eva_2022,
	title = {{EVA}: Exploring the Limits of Masked Visual Representation Learning at Scale},
	url = {http://arxiv.org/abs/2211.07636},
	doi = {10.48550/arXiv.2211.07636},
	shorttitle = {{EVA}},
	abstract = {We launch {EVA}, a vision-centric foundation model to explore the limits of visual representation at scale using only publicly accessible data. {EVA} is a vanilla {ViT} pre-trained to reconstruct the masked out image-text aligned vision features conditioned on visible image patches. Via this pretext task, we can efficiently scale up {EVA} to one billion parameters, and sets new records on a broad range of representative vision downstream tasks, such as image recognition, video action recognition, object detection, instance segmentation and semantic segmentation without heavy supervised training. Moreover, we observe quantitative changes in scaling {EVA} result in qualitative changes in transfer learning performance that are not present in other models. For instance, {EVA} takes a great leap in the challenging large vocabulary instance segmentation task: our model achieves almost the same state-of-the-art performance on {LVISv}1.0 dataset with over a thousand categories and {COCO} dataset with only eighty categories. Beyond a pure vision encoder, {EVA} can also serve as a vision-centric, multi-modal pivot to connect images and text. We find initializing the vision tower of a giant {CLIP} from {EVA} can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models. To facilitate future research, we release all the code and models at https://github.com/baaivision/{EVA}.},
	number = {{arXiv}:2211.07636},
	publisher = {{arXiv}},
	author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
	urldate = {2023-02-07},
	date = {2022-12-05},
	eprinttype = {arxiv},
	eprint = {2211.07636 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/K98I67ED/Fang et al. - 2022 - EVA Exploring the Limits of Masked Visual Represe.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/S8XUCMTV/2211.html:text/html},
}

@book{y_cajal_texture_1999,
	location = {Vienna},
	title = {Texture of the Nervous System of Man and the Vertebrates},
	isbn = {978-3-7091-7323-7 978-3-7091-6435-8},
	url = {http://link.springer.com/10.1007/978-3-7091-6435-8},
	publisher = {Springer},
	author = {y Cajal, Santiago Ramón},
	urldate = {2023-02-15},
	date = {1999},
	langid = {english},
	doi = {10.1007/978-3-7091-6435-8},
	keywords = {brain, cell, cells, Golgi, histology, morphology, nerv cell, Nervous System, neurons, spinal cord, tissue, vertebrates},
	file = {Full Text:/home/ajl/Zotero/storage/KYI5VZPT/y Cajal - 1999 - Texture of the Nervous System of Man and the Verte.pdf:application/pdf},
}

@book{y_cajal_texture_1999-1,
	location = {Vienna},
	title = {Texture of the Nervous System of Man and the Vertebrates},
	isbn = {978-3-7091-7323-7 978-3-7091-6435-8},
	url = {http://link.springer.com/10.1007/978-3-7091-6435-8},
	publisher = {Springer},
	author = {y Cajal, Santiago Ramón},
	urldate = {2023-02-15},
	date = {1999},
	langid = {english},
	doi = {10.1007/978-3-7091-6435-8},
	keywords = {brain, cell, cells, Golgi, histology, morphology, nerv cell, Nervous System, neurons, spinal cord, tissue, vertebrates},
	file = {Full Text:/home/ajl/Zotero/storage/XDIGQMJR/y Cajal - 1999 - Texture of the Nervous System of Man and the Verte.pdf:application/pdf},
}

@online{noauthor_nih_2022,
	title = {{NIH} {BRAIN} Initiative Launches Projects to Develop Cell Atlases and Molecular Tools for Cell Access},
	url = {https://www.nimh.nih.gov/news/science-news/2022/nih-brain-initiative-launches-projects-to-develop-cell-atlases-and-molecular-tools-for-cell-access},
	abstract = {The National Institutes of Health has launched two transformative projects supported by the Brain Research Through Advancing Innovative Neurotechnologies® ({BRAIN}) Initiative: The {BRAIN} Initiative® Cell Atlas Network and the Armamentarium for Precision Brain Cell Access.},
	titleaddon = {National Institute of Mental Health ({NIMH})},
	urldate = {2023-02-15},
	date = {2022-09-22},
	langid = {english},
	file = {Snapshot:/home/ajl/Zotero/storage/P8Z9L7ZB/nih-brain-initiative-launches-projects-to-develop-cell-atlases-and-molecular-tools-for-cell-acc.html:text/html},
}

@article{tasic_shared_2018,
	title = {Shared and distinct transcriptomic cell types across neocortical areas},
	volume = {563},
	rights = {2018 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0654-5},
	doi = {10.1038/s41586-018-0654-5},
	abstract = {The neocortex contains a multitude of cell types that are segregated into layers and functionally distinct areas. To investigate the diversity of cell types across the mouse neocortex, here we analysed 23,822 cells from two areas at distant poles of the mouse neocortex: the primary visual cortex and the anterior lateral motor cortex. We define 133 transcriptomic cell types by deep, single-cell {RNA} sequencing. Nearly all types of {GABA} (γ-aminobutyric acid)-containing neurons are shared across both areas, whereas most types of glutamatergic neurons were found in one of the two areas. By combining single-cell {RNA} sequencing and retrograde labelling, we match transcriptomic types of glutamatergic neurons to their long-range projection specificity. Our study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct areas of the adult mouse cortex.},
	pages = {72--78},
	number = {7729},
	journaltitle = {Nature},
	author = {Tasic, Bosiljka and Yao, Zizhen and Graybuck, Lucas T. and Smith, Kimberly A. and Nguyen, Thuc Nghi and Bertagnolli, Darren and Goldy, Jeff and Garren, Emma and Economo, Michael N. and Viswanathan, Sarada and Penn, Osnat and Bakken, Trygve and Menon, Vilas and Miller, Jeremy and Fong, Olivia and Hirokawa, Karla E. and Lathia, Kanan and Rimorin, Christine and Tieu, Michael and Larsen, Rachael and Casper, Tamara and Barkan, Eliza and Kroll, Matthew and Parry, Sheana and Shapovalova, Nadiya V. and Hirschstein, Daniel and Pendergraft, Julie and Sullivan, Heather A. and Kim, Tae Kyung and Szafer, Aaron and Dee, Nick and Groblewski, Peter and Wickersham, Ian and Cetin, Ali and Harris, Julie A. and Levi, Boaz P. and Sunkin, Susan M. and Madisen, Linda and Daigle, Tanya L. and Looger, Loren and Bernard, Amy and Phillips, John and Lein, Ed and Hawrylycz, Michael and Svoboda, Karel and Jones, Allan R. and Koch, Christof and Zeng, Hongkui},
	urldate = {2023-02-15},
	date = {2018-11},
	langid = {english},
	note = {Number: 7729
Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Gene expression, Transcriptomics, Visual system},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3NDKRE4C/Tasic et al. - 2018 - Shared and distinct transcriptomic cell types acro.pdf:application/pdf},
}

@article{oh_mesoscale_2014,
	title = {A mesoscale connectome of the mouse brain},
	volume = {508},
	issn = {1476-4687},
	doi = {10.1038/nature13186},
	abstract = {Comprehensive knowledge of the brain's wiring diagram is fundamental for understanding how the nervous system processes information at both local and global scales. However, with the singular exception of the C. elegans microscale connectome, there are no complete connectivity data sets in other species. Here we report a brain-wide, cellular-level, mesoscale connectome for the mouse. The Allen Mouse Brain Connectivity Atlas uses enhanced green fluorescent protein ({EGFP})-expressing adeno-associated viral vectors to trace axonal projections from defined regions and cell types, and high-throughput serial two-photon tomography to image the {EGFP}-labelled axons throughout the brain. This systematic and standardized approach allows spatial registration of individual experiments into a common three dimensional (3D) reference space, resulting in a whole-brain connectivity matrix. A computational model yields insights into connectional strength distribution, symmetry and other network properties. Virtual tractography illustrates 3D topography among interconnected regions. Cortico-thalamic pathway analysis demonstrates segregation and integration of parallel pathways. The Allen Mouse Brain Connectivity Atlas is a freely available, foundational resource for structural and functional investigations into the neural circuits that support behavioural and cognitive processes in health and disease.},
	pages = {207--214},
	number = {7495},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Oh, Seung Wook and Harris, Julie A. and Ng, Lydia and Winslow, Brent and Cain, Nicholas and Mihalas, Stefan and Wang, Quanxin and Lau, Chris and Kuan, Leonard and Henry, Alex M. and Mortrud, Marty T. and Ouellette, Benjamin and Nguyen, Thuc Nghi and Sorensen, Staci A. and Slaughterbeck, Clifford R. and Wakeman, Wayne and Li, Yang and Feng, David and Ho, Anh and Nicholas, Eric and Hirokawa, Karla E. and Bohn, Phillip and Joines, Kevin M. and Peng, Hanchuan and Hawrylycz, Michael J. and Phillips, John W. and Hohmann, John G. and Wohnoutka, Paul and Gerfen, Charles R. and Koch, Christof and Bernard, Amy and Dang, Chinh and Jones, Allan R. and Zeng, Hongkui},
	date = {2014-04-10},
	pmid = {24695228},
	pmcid = {PMC5102064},
	keywords = {Male, Animals, Brain, Mice, Atlases as Topic, Axons, Cerebral Cortex, Connectome, Corpus Striatum, Mice, Inbred C57BL, Models, Neurological, Neuroanatomical Tract-Tracing Techniques, Thalamus},
	file = {Accepted Version:/home/ajl/Zotero/storage/XMDEWIDA/Oh et al. - 2014 - A mesoscale connectome of the mouse brain.pdf:application/pdf},
}

@article{sun_integrating_2021,
	title = {Integrating barcoded neuroanatomy with spatial transcriptional profiling enables identification of gene correlates of projections},
	volume = {24},
	issn = {1546-1726},
	doi = {10.1038/s41593-021-00842-4},
	abstract = {Functional circuits consist of neurons with diverse axonal projections and gene expression. Understanding the molecular signature of projections requires high-throughput interrogation of both gene expression and projections to multiple targets in the same cells at cellular resolution, which is difficult to achieve using current technology. Here, we introduce {BARseq}2, a technique that simultaneously maps projections and detects multiplexed gene expression by in situ sequencing. We determined the expression of cadherins and cell-type markers in 29,933 cells and the projections of 3,164 cells in both the mouse motor cortex and auditory cortex. Associating gene expression and projections in 1,349 neurons revealed shared cadherin signatures of homologous projections across the two cortical areas. These cadherins were enriched across multiple branches of the transcriptomic taxonomy. By correlating multigene expression and projections to many targets in single neurons with high throughput, {BARseq}2 provides a potential path to uncovering the molecular logic underlying neuronal circuits.},
	pages = {873--885},
	number = {6},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Sun, Yu-Chi and Chen, Xiaoyin and Fischer, Stephan and Lu, Shaina and Zhan, Huiqing and Gillis, Jesse and Zador, Anthony M.},
	date = {2021-06},
	pmid = {33972801},
	pmcid = {PMC8178227},
	keywords = {Male, Animals, Neural Pathways, Mice, Mice, Inbred C57BL, Auditory Cortex, Brain Mapping, Electronic Data Processing, Gene Regulatory Networks, Motor Cortex},
	file = {Accepted Version:/home/ajl/Zotero/storage/CD42ZBCP/Sun et al. - 2021 - Integrating barcoded neuroanatomy with spatial tra.pdf:application/pdf},
}

@article{zeisel_molecular_2018,
	title = {Molecular Architecture of the Mouse Nervous System},
	volume = {174},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S009286741830789X},
	doi = {10.1016/j.cell.2018.06.021},
	abstract = {The mammalian nervous system executes complex behaviors controlled by specialized, precisely positioned, and interacting cell types. Here, we used {RNA} sequencing of half a million single cells to create a detailed census of cell types in the mouse nervous system. We mapped cell types spatially and derived a hierarchical, data-driven taxonomy. Neurons were the most diverse and were grouped by developmental anatomical units and by the expression of neurotransmitters and neuropeptides. Neuronal diversity was driven by genes encoding cell identity, synaptic connectivity, neurotransmission, and membrane conductance. We discovered seven distinct, regionally restricted astrocyte types that obeyed developmental boundaries and correlated with the spatial distribution of key glutamate and glycine neurotransmitters. In contrast, oligodendrocytes showed a loss of regional identity followed by a secondary diversification. The resource presented here lays a solid foundation for understanding the molecular architecture of the mammalian nervous system and enables genetic manipulation of specific cell types.},
	pages = {999--1014.e22},
	number = {4},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Zeisel, Amit and Hochgerner, Hannah and Lönnerberg, Peter and Johnsson, Anna and Memic, Fatima and van der Zwan, Job and Häring, Martin and Braun, Emelie and Borm, Lars E. and La Manno, Gioele and Codeluppi, Simone and Furlan, Alessandro and Lee, Kawai and Skene, Nathan and Harris, Kenneth D. and Hjerling-Leffler, Jens and Arenas, Ernest and Ernfors, Patrik and Marklund, Ulrika and Linnarsson, Sten},
	urldate = {2023-02-15},
	date = {2018-08-09},
	langid = {english},
	keywords = {{RNA} sequencing, cell type, classification, single-cell transcriptomics, transcriptomics},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/A7IHEMC5/Zeisel et al. - 2018 - Molecular Architecture of the Mouse Nervous System.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/TCBDDSML/S009286741830789X.html:text/html},
}

@article{bandler_single-cell_2022,
	title = {Single-cell delineation of lineage and genetic identity in the mouse brain},
	volume = {601},
	rights = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04237-0},
	doi = {10.1038/s41586-021-04237-0},
	abstract = {During neurogenesis, mitotic progenitor cells lining the ventricles of the embryonic mouse brain undergo their final rounds of cell division, giving rise to a wide spectrum of postmitotic neurons and glia1,2. The link between developmental lineage and cell-type diversity remains an open question. Here we used massively parallel tagging of progenitors to track clonal relationships and transcriptomic signatures during mouse forebrain development. We quantified clonal divergence and convergence across all major cell classes postnatally, and found diverse types of {GABAergic} neuron that share a common lineage. Divergence of {GABAergic} clones occurred during embryogenesis upon cell-cycle exit, suggesting that differentiation into subtypes is initiated as a lineage-dependent process at the progenitor cell level.},
	pages = {404--409},
	number = {7893},
	journaltitle = {Nature},
	author = {Bandler, Rachel C. and Vitali, Ilaria and Delgado, Ryan N. and Ho, May C. and Dvoretskova, Elena and Ibarra Molinas, Josue S. and Frazel, Paul W. and Mohammadkhani, Maesoumeh and Machold, Robert and Maedler, Sophia and Liddelow, Shane A. and Nowakowski, Tomasz J. and Fishell, Gord and Mayer, Christian},
	urldate = {2023-02-15},
	date = {2022-01},
	langid = {english},
	note = {Number: 7893
Publisher: Nature Publishing Group},
	keywords = {Cell fate and cell lineage, Cell type diversity},
	file = {Full Text PDF:/home/ajl/Zotero/storage/VTHBD5MC/Bandler et al. - 2022 - Single-cell delineation of lineage and genetic ide.pdf:application/pdf},
}

@article{lein_genome-wide_2007,
	title = {Genome-wide atlas of gene expression in the adult mouse brain},
	volume = {445},
	issn = {1476-4687},
	doi = {10.1038/nature05453},
	abstract = {Molecular approaches to understanding the functional circuitry of the nervous system promise new insights into the relationship between genes, brain and behaviour. The cellular diversity of the brain necessitates a cellular resolution approach towards understanding the functional genomics of the nervous system. We describe here an anatomically comprehensive digital atlas containing the expression patterns of approximately 20,000 genes in the adult mouse brain. Data were generated using automated high-throughput procedures for in situ hybridization and data acquisition, and are publicly accessible online. Newly developed image-based informatics tools allow global genome-scale structural analysis and cross-correlation, as well as identification of regionally enriched genes. Unbiased fine-resolution analysis has identified highly specific cellular markers as well as extensive evidence of cellular heterogeneity not evident in classical neuroanatomical atlases. This highly standardized atlas provides an open, primary data resource for a wide variety of further studies concerning brain organization and function.},
	pages = {168--176},
	number = {7124},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Lein, Ed S. and Hawrylycz, Michael J. and Ao, Nancy and Ayres, Mikael and Bensinger, Amy and Bernard, Amy and Boe, Andrew F. and Boguski, Mark S. and Brockway, Kevin S. and Byrnes, Emi J. and Chen, Lin and Chen, Li and Chen, Tsuey-Ming and Chin, Mei Chi and Chong, Jimmy and Crook, Brian E. and Czaplinska, Aneta and Dang, Chinh N. and Datta, Suvro and Dee, Nick R. and Desaki, Aimee L. and Desta, Tsega and Diep, Ellen and Dolbeare, Tim A. and Donelan, Matthew J. and Dong, Hong-Wei and Dougherty, Jennifer G. and Duncan, Ben J. and Ebbert, Amanda J. and Eichele, Gregor and Estin, Lili K. and Faber, Casey and Facer, Benjamin A. and Fields, Rick and Fischer, Shanna R. and Fliss, Tim P. and Frensley, Cliff and Gates, Sabrina N. and Glattfelder, Katie J. and Halverson, Kevin R. and Hart, Matthew R. and Hohmann, John G. and Howell, Maureen P. and Jeung, Darren P. and Johnson, Rebecca A. and Karr, Patrick T. and Kawal, Reena and Kidney, Jolene M. and Knapik, Rachel H. and Kuan, Chihchau L. and Lake, James H. and Laramee, Annabel R. and Larsen, Kirk D. and Lau, Christopher and Lemon, Tracy A. and Liang, Agnes J. and Liu, Ying and Luong, Lon T. and Michaels, Jesse and Morgan, Judith J. and Morgan, Rebecca J. and Mortrud, Marty T. and Mosqueda, Nerick F. and Ng, Lydia L. and Ng, Randy and Orta, Geralyn J. and Overly, Caroline C. and Pak, Tu H. and Parry, Sheana E. and Pathak, Sayan D. and Pearson, Owen C. and Puchalski, Ralph B. and Riley, Zackery L. and Rockett, Hannah R. and Rowland, Stephen A. and Royall, Joshua J. and Ruiz, Marcos J. and Sarno, Nadia R. and Schaffnit, Katherine and Shapovalova, Nadiya V. and Sivisay, Taz and Slaughterbeck, Clifford R. and Smith, Simon C. and Smith, Kimberly A. and Smith, Bryan I. and Sodt, Andy J. and Stewart, Nick N. and Stumpf, Kenda-Ruth and Sunkin, Susan M. and Sutram, Madhavi and Tam, Angelene and Teemer, Carey D. and Thaller, Christina and Thompson, Carol L. and Varnam, Lee R. and Visel, Axel and Whitlock, Ray M. and Wohnoutka, Paul E. and Wolkey, Crissa K. and Wong, Victoria Y. and Wood, Matthew and Yaylaoglu, Murat B. and Young, Rob C. and Youngstrom, Brian L. and Yuan, Xu Feng and Zhang, Bin and Zwingman, Theresa A. and Jones, Allan R.},
	date = {2007-01-11},
	pmid = {17151600},
	keywords = {Male, Animals, Brain, Mice, Computational Biology, Mice, Inbred C57BL, Gene Expression Profiling, Gene Expression Regulation, Genome, Genomics, Hippocampus, Organ Specificity, {RNA}, Messenger},
}

@article{knox_high-resolution_2018,
	title = {High-resolution data-driven model of the mouse connectome},
	volume = {3},
	issn = {2472-1751},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6372022/},
	doi = {10.1162/netn_a_00066},
	abstract = {Knowledge of mesoscopic brain connectivity is important for understanding inter- and intraregion information processing. Models of structural connectivity are typically constructed and analyzed with the assumption that regions are homogeneous. We instead use the Allen Mouse Brain Connectivity Atlas to construct a model of whole-brain connectivity at the scale of 100 μm voxels. The data consist of 428 anterograde tracing experiments in wild type C57BL/6J mice, mapping fluorescently labeled neuronal projections brain-wide. Inferring spatial connectivity with this dataset is underdetermined, since the approximately 2 × 105 source voxels outnumber the number of experiments. To address this issue, we assume that connection patterns and strengths vary smoothly across major brain divisions. We model the connectivity at each voxel as a radial basis kernel-weighted average of the projection patterns of nearby injections. The voxel model outperforms a previous regional model in predicting held-out experiments and compared with a human-curated dataset. This voxel-scale model of the mouse connectome permits researchers to extend their previous analyses of structural connectivity to much higher levels of resolution, and it allows for comparison with functional imaging and other datasets., Anatomical tracing experiments can provide a wealth of information regarding connectivities originating from the injection sites. However, it is difficult to integrate all this information into a comprehensive connectivity model. In this study we construct a high-resolution model of the mouse brain connectome using the assumption that connectivity patterns vary smoothly within brain regions, and we present several extensions of this model. We believe that this higher resolution connectome will be of great use to the community, enabling comparisons with other data modalities, such as functional imaging and gene expression, as well as for theoretical studies.},
	pages = {217--236},
	number = {1},
	journaltitle = {Network Neuroscience},
	shortjournal = {Netw Neurosci},
	author = {Knox, Joseph E. and Harris, Kameron Decker and Graddis, Nile and Whitesell, Jennifer D. and Zeng, Hongkui and Harris, Julie A. and Shea-Brown, Eric and Mihalas, Stefan},
	urldate = {2023-02-15},
	date = {2018-12-01},
	pmid = {30793081},
	pmcid = {PMC6372022},
	file = {PubMed Central Full Text PDF:/home/ajl/Zotero/storage/ZSU7QIEZ/Knox et al. - 2018 - High-resolution data-driven model of the mouse con.pdf:application/pdf},
}

@article{wang_allen_2020,
	title = {The Allen Mouse Brain Common Coordinate Framework: A 3D Reference Atlas},
	volume = {181},
	issn = {0092-8674},
	url = {https://www.sciencedirect.com/science/article/pii/S0092867420304025},
	doi = {10.1016/j.cell.2020.04.007},
	shorttitle = {The Allen Mouse Brain Common Coordinate Framework},
	abstract = {Recent large-scale collaborations are generating major surveys of cell types and connections in the mouse brain, collecting large amounts of data across modalities, spatial scales, and brain areas. Successful integration of these data requires a standard 3D reference atlas. Here, we present the Allen Mouse Brain Common Coordinate Framework ({CCFv}3) as such a resource. We constructed an average template brain at 10 μm voxel resolution by interpolating high resolution in-plane serial two-photon tomography images with 100 μm z-sampling from 1,675 young adult C57BL/6J mice. Then, using multimodal reference data, we parcellated the entire brain directly in 3D, labeling every voxel with a brain structure spanning 43 isocortical areas and their layers, 329 subcortical gray matter structures, 81 fiber tracts, and 8 ventricular structures. {CCFv}3 can be used to analyze, visualize, and integrate multimodal and multiscale datasets in 3D and is openly accessible (https://atlas.brain-map.org/).},
	pages = {936--953.e20},
	number = {4},
	journaltitle = {Cell},
	shortjournal = {Cell},
	author = {Wang, Quanxin and Ding, Song-Lin and Li, Yang and Royall, Josh and Feng, David and Lesnar, Phil and Graddis, Nile and Naeemi, Maitham and Facer, Benjamin and Ho, Anh and Dolbeare, Tim and Blanchard, Brandon and Dee, Nick and Wakeman, Wayne and Hirokawa, Karla E. and Szafer, Aaron and Sunkin, Susan M. and Oh, Seung Wook and Bernard, Amy and Phillips, John W. and Hawrylycz, Michael and Koch, Christof and Zeng, Hongkui and Harris, Julie A. and Ng, Lydia},
	urldate = {2023-02-15},
	date = {2020-05-14},
	langid = {english},
	keywords = {3D brain atlas, average mouse brain, brain anatomy, brain parcellation, {CCFv}3, common coordinate framework, fiber tracts, mouse cortex, reference atlas, transgenic mice},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/N485IEVU/Wang et al. - 2020 - The Allen Mouse Brain Common Coordinate Framework.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/PKI3TK4C/S0092867420304025.html:text/html},
}

@online{noauthor_graph_nodate,
	title = {Graph deep learning for the characterization of tumour microenvironments from spatial protein profiles in tissue specimens - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/36357512/},
	urldate = {2023-02-15},
	file = {Graph deep learning for the characterization of tumour microenvironments from spatial protein profiles in tissue specimens - PubMed:/home/ajl/Zotero/storage/8H3X2G56/36357512.html:text/html},
}

@article{townes_nonnegative_2023,
	title = {Nonnegative spatial factorization applied to spatial genomics},
	volume = {20},
	rights = {2022 The Author(s)},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01687-w},
	doi = {10.1038/s41592-022-01687-w},
	abstract = {Nonnegative matrix factorization ({NMF}) is widely used to analyze high-dimensional count data because, in contrast to real-valued alternatives such as factor analysis, it produces an interpretable parts-based representation. However, in applications such as spatial transcriptomics, {NMF} fails to incorporate known structure between observations. Here, we present nonnegative spatial factorization ({NSF}), a spatially-aware probabilistic dimension reduction model based on transformed Gaussian processes that naturally encourages sparsity and scales to tens of thousands of observations. {NSF} recovers ground truth factors more accurately than real-valued alternatives such as {MEFISTO} in simulations, and has lower out-of-sample prediction error than probabilistic {NMF} on three spatial transcriptomics datasets from mouse brain and liver. Since not all patterns of gene expression have spatial correlations, we also propose a hybrid extension of {NSF} that combines spatial and nonspatial components, enabling quantification of spatial importance for both observations and features. A {TensorFlow} implementation of {NSF} is available from https://github.com/willtownes/nsf-paper.},
	pages = {229--238},
	number = {2},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Townes, F. William and Engelhardt, Barbara E.},
	urldate = {2023-02-15},
	date = {2023-02},
	langid = {english},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Gene expression analysis, Software, Statistical methods, Transcriptomics},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3HE53E6H/Townes and Engelhardt - 2023 - Nonnegative spatial factorization applied to spati.pdf:application/pdf},
}

@article{brbic_annotation_2022,
	title = {Annotation of spatially resolved single-cell data with {STELLAR}},
	volume = {19},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-022-01651-8},
	doi = {10.1038/s41592-022-01651-8},
	abstract = {Accurate cell-type annotation from spatially resolved single cells is crucial to understand functional spatial biology that is the basis of tissue organization. However, current computational methods for annotating spatially resolved single-cell data are typically based on techniques established for dissociated single-cell technologies and thus do not take spatial organization into account. Here we present {STELLAR}, a geometric deep learning method for cell-type discovery and identification in spatially resolved single-cell datasets. {STELLAR} automatically assigns cells to cell types present in the annotated reference dataset and discovers novel cell types and cell states. {STELLAR} transfers annotations across different dissection regions, different tissues and different donors, and learns cell representations that capture higher-order tissue structures. We successfully applied {STELLAR} to {CODEX} multiplexed fluorescent microscopy data and multiplexed {RNA} imaging datasets. Within the Human {BioMolecular} Atlas Program, {STELLAR} has annotated 2.6 million spatially resolved single cells with dramatic time savings.},
	pages = {1411--1418},
	number = {11},
	journaltitle = {Nature Methods},
	shortjournal = {Nat Methods},
	author = {Brbić, Maria and Cao, Kaidi and Hickey, John W. and Tan, Yuqi and Snyder, Michael P. and Nolan, Garry P. and Leskovec, Jure},
	urldate = {2023-02-15},
	date = {2022-11},
	langid = {english},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Machine learning, Transcriptomics, Fluorescence imaging, Proteomics},
	file = {Full Text PDF:/home/ajl/Zotero/storage/3VU6DMAG/Brbić et al. - 2022 - Annotation of spatially resolved single-cell data .pdf:application/pdf},
}

@article{wu_stability-driven_2016-1,
	title = {Stability-driven nonnegative matrix factorization to interpret spatial gene expression and build local gene networks},
	volume = {113},
	url = {https://www.pnas.org/doi/10.1073/pnas.1521171113},
	doi = {10.1073/pnas.1521171113},
	abstract = {Spatial gene expression patterns enable the detection of local covariability and are extremely useful for identifying local gene interactions during normal development. The abundance of spatial expression data in recent years has led to the modeling and analysis of regulatory networks. The inherent complexity of such data makes it a challenge to extract biological information. We developed {staNMF}, a method that combines a scalable implementation of nonnegative matrix factorization ({NMF}) with a new stability-driven model selection criterion. When applied to a set of Drosophila early embryonic spatial gene expression images, one of the largest datasets of its kind, {staNMF} identified 21 principal patterns ({PP}). Providing a compact yet biologically interpretable representation of Drosophila expression patterns, {PP} are comparable to a fate map generated experimentally by laser ablation and show exceptional promise as a data-driven alternative to manual annotations. Our analysis mapped genes to cell-fate programs and assigned putative biological roles to uncharacterized genes. Finally, we used the {PP} to generate local transcription factor regulatory networks. Spatially local correlation networks were constructed for six {PP} that span along the embryonic anterior–posterior axis. Using a two-tail 5\% cutoff on correlation, we reproduced 10 of the 11 links in the well-studied gap gene network. The performance of {PP} with the Drosophila data suggests that {staNMF} provides informative decompositions and constitutes a useful computational lens through which to extract biological insight from complex and often noisy gene expression data.},
	pages = {4290--4295},
	number = {16},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Wu, Siqi and Joseph, Antony and Hammonds, Ann S. and Celniker, Susan E. and Yu, Bin and Frise, Erwin},
	urldate = {2023-02-15},
	date = {2016-04-19},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:/home/ajl/Zotero/storage/XX5GUTQA/Wu et al. - 2016 - Stability-driven nonnegative matrix factorization .pdf:application/pdf},
}

@article{walker_deciphering_2022-1,
	title = {Deciphering tissue structure and function using spatial transcriptomics},
	volume = {5},
	rights = {2022 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-03175-5},
	doi = {10.1038/s42003-022-03175-5},
	abstract = {The rapid development of spatial transcriptomics ({ST}) techniques has allowed the measurement of transcriptional levels across many genes together with the spatial positions of cells. This has led to an explosion of interest in computational methods and techniques for harnessing both spatial and transcriptional information in analysis of {ST} datasets. The wide diversity of approaches in aim, methodology and technology for {ST} provides great challenges in dissecting cellular functions in spatial contexts. Here, we synthesize and review the key problems in analysis of {ST} data and methods that are currently applied, while also expanding on open questions and areas of future development.},
	pages = {1--10},
	number = {1},
	journaltitle = {Communications Biology},
	shortjournal = {Commun Biol},
	author = {Walker, Benjamin L. and Cang, Zixuan and Ren, Honglei and Bourgain-Chang, Eric and Nie, Qing},
	urldate = {2023-02-15},
	date = {2022-03-10},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational biology and bioinformatics, Systems biology},
	file = {Full Text PDF:/home/ajl/Zotero/storage/CY2273W2/Walker et al. - 2022 - Deciphering tissue structure and function using sp.pdf:application/pdf},
}

@misc{chen_symbolic_2023,
	title = {Symbolic Discovery of Optimization Algorithms},
	url = {http://arxiv.org/abs/2302.06675},
	doi = {10.48550/arXiv.2302.06675},
	abstract = {We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, \${\textbackslash}textbf\{Lion\}\$ (\${\textbackslash}textit\{Evo\${\textbackslash}textbf\{L\}\$ved S\${\textbackslash}textbf\{i\}\$gn M\${\textbackslash}textbf\{o\}\$me\${\textbackslash}textbf\{n\}\$tum\}\$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of {ViT} by up to 2\% on {ImageNet} and saves up to 5x the pre-training compute on {JFT}. On vision-language contrastive learning, we achieve 88.3\% \${\textbackslash}textit\{zero-shot\}\$ and 91.1\% \${\textbackslash}textit\{fine-tuning\}\$ accuracy on {ImageNet}, surpassing the previous best results by 2\% and 0.1\%, respectively. On diffusion models, Lion outperforms Adam by achieving a better {FID} score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. The implementation of Lion is publicly available.},
	number = {{arXiv}:2302.06675},
	publisher = {{arXiv}},
	author = {Chen, Xiangning and Liang, Chen and Huang, Da and Real, Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu, Yifeng and Le, Quoc V.},
	urldate = {2023-02-19},
	date = {2023-02-13},
	eprinttype = {arxiv},
	eprint = {2302.06675 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
}

@online{noauthor_notitle_nodate-1,
	url = {https://academic.oup.com/g3journal/advance-article/doi/10.1093/g3journal/jkad004/6982776},
	urldate = {2023-02-19},
}

@article{huang_towards_2022,
	title = {Towards a unification of the 2 meanings of “epigenetics”},
	volume = {20},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3001944},
	doi = {10.1371/journal.pbio.3001944},
	abstract = {The notion of epigenetic “marks” used by molecular biologists is conceptually disconnected from the idea of Waddington’s epigenetic “landscape” that is used by systems biologists and biophysicists. Recent advances suggest that these 2 distinct schools of thought could be united.},
	pages = {e3001944},
	number = {12},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Huang, Sui},
	urldate = {2023-02-19},
	date = {2022-12-27},
	langid = {english},
	keywords = {Gene expression, Chromatin, Epigenetics, Gene regulation, Genetic loci, Transcription factors, Transcriptional control, Valleys},
}

@article{thompson_genomic_2008,
	title = {Genomic Anatomy of the Hippocampus},
	volume = {60},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627308010568},
	doi = {10.1016/j.neuron.2008.12.008},
	abstract = {Availability of genome-scale in situ hybridization data allows systematic analysis of genetic neuroanatomical architecture. Within the hippocampus, electrophysiology and lesion and imaging studies demonstrate functional heterogeneity along the septotemporal axis, although precise underlying circuitry and molecular substrates remain uncharacterized. Application of unbiased statistical component analyses to genome-scale hippocampal gene expression data revealed robust septotemporal molecular heterogeneity, leading to the identification of a large cohort of genes with robust regionalized hippocampal expression. Manual mapping of heterogeneous {CA}3 pyramidal neuron expression patterns demonstrates an unexpectedly complex molecular parcellation into a relatively coherent set of nine expression domains in the septal/temporal and proximal/distal axes with reciprocal, nonoverlapping boundaries. Unique combinatorial profiles of adhesion molecules within these domains suggest corresponding differential connectivity, which is demonstrated for {CA}3 projections to the lateral septum using retrograde labeling. This complex, discrete molecular architecture provides a novel paradigm for predicting functional differentiation across the full septotemporal extent of the hippocampus.},
	pages = {1010--1021},
	number = {6},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Thompson, Carol L. and Pathak, Sayan D. and Jeromin, Andreas and Ng, Lydia L. and {MacPherson}, Cameron R. and Mortrud, Marty T. and Cusick, Allison and Riley, Zackery L. and Sunkin, Susan M. and Bernard, Amy and Puchalski, Ralph B. and Gage, Fred H. and Jones, Allan R. and Bajic, Vladimir B. and Hawrylycz, Michael J. and Lein, Ed S.},
	urldate = {2023-02-26},
	date = {2008-12-26},
	langid = {english},
	keywords = {notion, {MOLNEURO}, {SYSBIO}, {SYSNEURO}},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/M4TX5LB8/Thompson et al. - 2008 - Genomic Anatomy of the Hippocampus.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/YRAA5T3F/S0896627308010568.html:text/html},
}

@misc{sahak_denoising_2023,
	title = {Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild},
	url = {http://arxiv.org/abs/2302.07864},
	doi = {10.48550/arXiv.2302.07864},
	abstract = {Diffusion models have shown promising results on single-image super-resolution and other image- to-image translation tasks. Despite this success, they have not outperformed state-of-the-art {GAN} models on the more challenging blind super-resolution task, where the input images are out of distribution, with unknown degradations. This paper introduces {SR}3+, a diffusion-based model for blind super-resolution, establishing a new state-of-the-art. To this end, we advocate self-supervised training with a combination of composite, parameterized degradations for self-supervised training, and noise-conditioing augmentation during training and testing. With these innovations, a large-scale convolutional architecture, and large-scale datasets, {SR}3+ greatly outperforms {SR}3. It outperforms Real-{ESRGAN} when trained on the same data, with a {DRealSR} {FID} score of 36.82 vs. 37.22, which further improves to {FID} of 32.37 with larger models, and further still with larger training sets.},
	number = {{arXiv}:2302.07864},
	publisher = {{arXiv}},
	author = {Sahak, Hshmat and Watson, Daniel and Saharia, Chitwan and Fleet, David},
	urldate = {2023-02-19},
	date = {2023-02-15},
	eprinttype = {arxiv},
	eprint = {2302.07864 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{chen_riemannian_2023,
	title = {Riemannian Flow Matching on General Geometries},
	url = {http://arxiv.org/abs/2302.03660},
	doi = {10.48550/arXiv.2302.03660},
	abstract = {We propose Riemannian Flow Matching ({RFM}), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, inherently cannot scale to high dimensions, or use approximations to limiting quantities that result in biased objectives. Riemannian Flow Matching bypasses these inconveniences and exhibits multiple benefits over prior approaches: It is completely simulation-free on simple geometries, it does not require divergence computation, and its target vector field is computed in closed form even on general geometries. The key ingredient behind {RFM} is the construction of a simple kernel function for defining per-sample vector fields, which subsumes existing Euclidean cases. Extending to general geometries, we rely on the use of spectral decompositions to efficiently compute kernel functions. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we showcase, for the first time, tractable training on general geometries, including on triangular meshes and maze-like manifolds with boundaries.},
	number = {{arXiv}:2302.03660},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Lipman, Yaron},
	urldate = {2023-02-19},
	date = {2023-02-07},
	eprinttype = {arxiv},
	eprint = {2302.03660 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{huang_neural_2020,
	title = {Neural Networks with Recurrent Generative Feedback},
	url = {http://arxiv.org/abs/2007.09200},
	doi = {10.48550/arXiv.2007.09200},
	abstract = {Neural networks are vulnerable to input perturbations such as additive noise and adversarial attacks. In contrast, human perception is much more robust to such perturbations. The Bayesian brain hypothesis states that human brains use an internal generative model to update the posterior beliefs of the sensory input. This mechanism can be interpreted as a form of self-consistency between the maximum a posteriori ({MAP}) estimation of an internal generative model and the external environment. Inspired by such hypothesis, we enforce self-consistency in neural networks by incorporating generative recurrent feedback. We instantiate this design on convolutional neural networks ({CNNs}). The proposed framework, termed Convolutional Neural Networks with Feedback ({CNN}-F), introduces a generative feedback with latent variables to existing {CNN} architectures, where consistent predictions are made through alternating {MAP} inference under a Bayesian framework. In the experiments, {CNN}-F shows considerably improved adversarial robustness over conventional feedforward {CNNs} on standard benchmarks.},
	number = {{arXiv}:2007.09200},
	publisher = {{arXiv}},
	author = {Huang, Yujia and Gornet, James and Dai, Sihui and Yu, Zhiding and Nguyen, Tan and Tsao, Doris Y. and Anandkumar, Anima},
	urldate = {2023-02-19},
	date = {2020-11-10},
	eprinttype = {arxiv},
	eprint = {2007.09200 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{vanderhaeghen_developmental_2023,
	title = {Developmental mechanisms underlying the evolution of human cortical circuits},
	rights = {2023 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-023-00675-z},
	doi = {10.1038/s41583-023-00675-z},
	abstract = {The brain of modern humans has evolved remarkable computational abilities that enable higher cognitive functions. These capacities are tightly linked to an increase in the size and connectivity of the cerebral cortex, which is thought to have resulted from evolutionary changes in the mechanisms of cortical development. Convergent progress in evolutionary genomics, developmental biology and neuroscience has recently enabled the identification of genomic changes that act as human-specific modifiers of cortical development. These modifiers influence most aspects of corticogenesis, from the timing and complexity of cortical neurogenesis to synaptogenesis and the assembly of cortical circuits. Mutations of human-specific genetic modifiers of corticogenesis have started to be linked to neurodevelopmental disorders, providing evidence for their physiological relevance and suggesting potential relationships between the evolution of the human brain and its sensitivity to specific diseases.},
	pages = {1--20},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Vanderhaeghen, Pierre and Polleux, Franck},
	urldate = {2023-02-19},
	date = {2023-02-15},
	langid = {english},
	keywords = {Development of the nervous system, Evolution, Genetics of the nervous system},
}

@misc{xiao_synergy_2022,
	title = {Synergy and Symmetry in Deep Learning: Interactions between the Data, Model, and Inference Algorithm},
	url = {http://arxiv.org/abs/2207.04612},
	doi = {10.48550/arXiv.2207.04612},
	shorttitle = {Synergy and Symmetry in Deep Learning},
	abstract = {Although learning in high dimensions is commonly believed to suffer from the curse of dimensionality, modern machine learning methods often exhibit an astonishing power to tackle a wide range of challenging real-world learning problems without using abundant amounts of data. How exactly these methods break this curse remains a fundamental open question in the theory of deep learning. While previous efforts have investigated this question by studying the data (D), model (M), and inference algorithm (I) as independent modules, in this paper, we analyze the triplet (D, M, I) as an integrated system and identify important synergies that help mitigate the curse of dimensionality. We first study the basic symmetries associated with various learning algorithms (M, I), focusing on four prototypical architectures in deep learning: fully-connected networks ({FCN}), locally-connected networks ({LCN}), and convolutional networks with and without pooling ({GAP}/{VEC}). We find that learning is most efficient when these symmetries are compatible with those of the data distribution and that performance significantly deteriorates when any member of the (D, M, I) triplet is inconsistent or suboptimal.},
	number = {{arXiv}:2207.04612},
	publisher = {{arXiv}},
	author = {Xiao, Lechao and Pennington, Jeffrey},
	urldate = {2023-02-19},
	date = {2022-07-11},
	eprinttype = {arxiv},
	eprint = {2207.04612 [cs]},
	keywords = {Computer Science - Machine Learning, 68T07},
}

@misc{chughtai_toy_2023,
	title = {A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations},
	url = {http://arxiv.org/abs/2302.03025},
	doi = {10.48550/arXiv.2302.03025},
	shorttitle = {A Toy Model of Universality},
	abstract = {Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.},
	number = {{arXiv}:2302.03025},
	publisher = {{arXiv}},
	author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
	urldate = {2023-02-19},
	date = {2023-02-06},
	eprinttype = {arxiv},
	eprint = {2302.03025 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Representation Theory},
}

@article{kolchinsky_semantic_2018,
	title = {Semantic information, autonomous agency, and nonequilibrium statistical physics},
	volume = {8},
	issn = {2042-8898, 2042-8901},
	url = {http://arxiv.org/abs/1806.08053},
	doi = {10.1098/rsfs.2018.0041},
	abstract = {Shannon information theory provides various measures of so-called "syntactic information", which reflect the amount of statistical correlation between systems. In contrast, the concept of "semantic information" refers to those correlations which carry significance or "meaning" for a given system. Semantic information plays an important role in many fields, including biology, cognitive science, and philosophy, and there has been a long-standing interest in formulating a broadly applicable and formal theory of semantic information. In this paper we introduce such a theory. We define semantic information as the syntactic information that a physical system has about its environment which is causally necessary for the system to maintain its own existence. "Causal necessity" is defined in terms of counter-factual interventions which scramble correlations between the system and its environment, while "maintaining existence" is defined in terms of the system's ability to keep itself in a low entropy state. We also use recent results in nonequilibrium statistical physics to analyze semantic information from a thermodynamic point of view. Our framework is grounded in the intrinsic dynamics of a system coupled to an environment, and is applicable to any physical system, living or otherwise. It leads to formal definitions of several concepts that have been intuitively understood to be related to semantic information, including "value of information", "semantic content", and "agency".},
	pages = {20180041},
	number = {6},
	journaltitle = {Interface Focus},
	shortjournal = {Interface Focus.},
	author = {Kolchinsky, Artemy and Wolpert, David H.},
	urldate = {2023-02-19},
	date = {2018-12-06},
	eprinttype = {arxiv},
	eprint = {1806.08053 [cond-mat]},
	keywords = {Condensed Matter - Statistical Mechanics},
}

@article{pineda_geometric_2023,
	title = {Geometric deep learning reveals the spatiotemporal features of microscopic motion},
	volume = {5},
	rights = {2023 The Author(s)},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00595-0},
	doi = {10.1038/s42256-022-00595-0},
	abstract = {The characterization of dynamical processes in living systems provides important clues for their mechanistic interpretation and link to biological functions. Owing to recent advances in microscopy techniques, it is now possible to routinely record the motion of cells, organelles and individual molecules at multiple spatiotemporal scales in physiological conditions. However, the automated analysis of dynamics occurring in crowded and complex environments still lags behind the acquisition of microscopic image sequences. Here we present a framework based on geometric deep learning that achieves the accurate estimation of dynamical properties in various biologically relevant scenarios. This deep-learning approach relies on a graph neural network enhanced by attention-based components. By processing object features with geometric priors, the network is capable of performing multiple tasks, from linking coordinates into trajectories to inferring local and global dynamic properties. We demonstrate the flexibility and reliability of this approach by applying it to real and simulated data corresponding to a broad range of biological experiments.},
	pages = {71--82},
	number = {1},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Pineda, Jesús and Midtvedt, Benjamin and Bachimanchi, Harshith and Noé, Sergio and Midtvedt, Daniel and Volpe, Giovanni and Manzo, Carlo},
	urldate = {2023-02-19},
	date = {2023-01},
	langid = {english},
	keywords = {Cellular imaging, Cell migration, Single-molecule biophysics, Super-resolution microscopy},
}

@misc{shen_cross-modal_2023,
	title = {Cross-Modal Fine-Tuning: Align then Refine},
	url = {http://arxiv.org/abs/2302.05738},
	doi = {10.48550/arXiv.2302.05738},
	shorttitle = {Cross-Modal Fine-Tuning},
	abstract = {Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and {NLP}. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose {ORCA}, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. {ORCA} adapts to a target task via an align-then-refine workflow: given the target input, {ORCA} first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that {ORCA} obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, {AutoML}, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate {ORCA}'s utility in data-limited regimes.},
	number = {{arXiv}:2302.05738},
	publisher = {{arXiv}},
	author = {Shen, Junhong and Li, Liam and Dery, Lucio M. and Staten, Corey and Khodak, Mikhail and Neubig, Graham and Talwalkar, Ameet},
	urldate = {2023-02-19},
	date = {2023-02-11},
	eprinttype = {arxiv},
	eprint = {2302.05738 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{patro_efficiency_2023,
	title = {Efficiency 360: Efficient Vision Transformers},
	url = {http://arxiv.org/abs/2302.08374},
	doi = {10.48550/arXiv.2302.08374},
	shorttitle = {Efficiency 360},
	abstract = {Transformers are widely used for solving tasks in natural language processing, computer vision, speech, and music domains. In this paper, we talk about the efficiency of transformers in terms of memory (the number of parameters), computation cost (number of floating points operations), and performance of models, including accuracy, the robustness of the model, and fair {\textbackslash}\& bias-free features. We mainly discuss the vision transformer for the image classification task. Our contribution is to introduce an efficient 360 framework, which includes various aspects of the vision transformer, to make it more efficient for industrial applications. By considering those applications, we categorize them into multiple dimensions such as privacy, robustness, transparency, fairness, inclusiveness, continual learning, probabilistic models, approximation, computational complexity, and spectral complexity. We compare various vision transformer models based on their performance, the number of parameters, and the number of floating point operations ({FLOPs}) on multiple datasets.},
	number = {{arXiv}:2302.08374},
	publisher = {{arXiv}},
	author = {Patro, Badri N. and Agneeswaran, Vijay},
	urldate = {2023-02-19},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2302.08374 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@article{soelistyo_learning_2022,
	title = {Learning biophysical determinants of cell fate with deep neural networks},
	volume = {4},
	rights = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-022-00503-6},
	doi = {10.1038/s42256-022-00503-6},
	abstract = {Deep learning is now a powerful tool in microscopy data analysis, and is routinely used for image processing applications such as segmentation and denoising. However, it has rarely been used to directly learn mechanistic models of a biological system, owing to the complexity of the internal representations. Here, we develop an end-to-end machine learning approach capable of learning an explainable model of a complex biological phenomenon, cell competition, directly from a large corpus of time-lapse microscopy data. Cell competition is a quality control mechanism that eliminates unfit cells from a tissue, during which cell fate is thought to be determined by the local cellular neighbourhood over time. To investigate this, we developed a new approach (τ-{VAE}) by coupling a probabilistic encoder to a temporal convolution network to predict the fate of each cell in an epithelium. Using the τ-{VAE}’s latent representation of the local tissue organization and the flow of information in the network, we decode the physical parameters responsible for correct prediction of fate in cell competition. Remarkably, the model autonomously learns that cell density is the single most important factor in predicting cell fate—a conclusion that is in agreement with our current understanding from over a decade of scientific research. Finally, to test the learned internal representation, we challenge the network with experiments performed in the presence of drugs that block signalling pathways involved in competition. We present a novel discriminator network, which using the predictions of the τ-{VAE} can identify conditions that deviate from the normal behaviour, paving the way for automated, mechanism-aware drug screening.},
	pages = {636--644},
	number = {7},
	journaltitle = {Nature Machine Intelligence},
	shortjournal = {Nat Mach Intell},
	author = {Soelistyo, Christopher J. and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},
	urldate = {2023-02-19},
	date = {2022-07},
	langid = {english},
	keywords = {Machine learning, Cellular imaging, Computer science, Drug screening},
}

@misc{ferreira_label-free_2022,
	title = {Label-free segmentation from cardiac ultrasound using self-supervised learning},
	url = {http://arxiv.org/abs/2210.04979},
	doi = {10.48550/arXiv.2210.04979},
	abstract = {Background: Segmentation and measurement of cardiac chambers is critical in echocardiography but is also laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations, while unsupervised approaches have fared poorly in ultrasound to date. Objectives: We built a pipeline for self-supervised (no manual labels required) segmentation of cardiac chambers, combining computer vision, clinical domain knowledge, and deep learning. Methods: We trained on 450 echocardiograms (145,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean age 61 years, 51\% female), using the resulting segmentations to calculate structural and functional measurements. We also tested our pipeline against external images from an additional 10,030 patients (20,060 images) with available manual tracings of the left ventricle. Results: r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation for {LVESV} and {LVEDV} (pipeline vs. clinical r2= 0.74 and r2=0.65, respectively), {LVEF} and {LV} mass (r2= 0.46 and r2=0.54), left and right atrium volumes (r2=0.7 and r2=0.6), and right ventricle area (r2=0.47). When binarized into normal vs. abnormal categories, average accuracy was 0.81 (range 0.71-0.95). A subset of the test echocardiograms (n=553) had corresponding cardiac {MRI}; correlation between pipeline and {CMR} measurements was similar to that between clinical echocardiogram and {CMR}. Finally, in the external dataset, our pipeline accurately segments the left ventricle with an average Dice score of 0.83 (95\% {CI} 0.83). Conclusions: Our results demonstrate a human-label-free, valid, and scalable method for segmentation from ultrasound, a noisy but globally important imaging modality.},
	number = {{arXiv}:2210.04979},
	publisher = {{arXiv}},
	author = {Ferreira, Danielle L. and Salaymang, Zaynaf and Arnaout, Rima},
	urldate = {2023-02-19},
	date = {2022-10-10},
	eprinttype = {arxiv},
	eprint = {2210.04979 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{pinto_tuning_2023,
	title = {Tuning computer vision models with task rewards},
	url = {http://arxiv.org/abs/2302.08242},
	doi = {10.48550/arXiv.2302.08242},
	abstract = {Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.},
	number = {{arXiv}:2302.08242},
	publisher = {{arXiv}},
	author = {Pinto, André Susano and Kolesnikov, Alexander and Shi, Yuge and Beyer, Lucas and Zhai, Xiaohua},
	urldate = {2023-02-19},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2302.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{sandmann_evolutionary_2023,
	title = {Evolutionary origins and interactomes of human, young microproteins and small peptides translated from short open reading frames},
	volume = {0},
	issn = {1097-2765},
	url = {https://www.cell.com/molecular-cell/abstract/S1097-2765(23)00075-8},
	doi = {10.1016/j.molcel.2023.01.023},
	number = {0},
	journaltitle = {Molecular Cell},
	shortjournal = {Molecular Cell},
	author = {Sandmann, Clara-L. and Schulz, Jana F. and Ruiz-Orera, Jorge and Kirchner, Marieluise and Ziehm, Matthias and Adami, Eleonora and Marczenke, Maike and Christ, Annabel and Liebe, Nina and Greiner, Johannes and Schoenenberger, Aaron and Muecke, Michael B. and Liang, Ning and Moritz, Robert L. and Sun, Zhi and Deutsch, Eric W. and Gotthardt, Michael and Mudge, Jonathan M. and Prensner, John R. and Willnow, Thomas E. and Mertins, Philipp and Heesch, Sebastiaan van and Hubner, Norbert},
	urldate = {2023-02-19},
	date = {2023-02-17},
	keywords = {de novo genes, microproteins, primate-specific proteins, {PRISMA}, protein evolution, protein interactome, ribosome profiling, short linear motifs, short {ORFs}, short peptides, {SLiMs}},
}

@article{zhang_hippocampal_2023,
	title = {Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience},
	volume = {26},
	rights = {2022 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01212-4},
	doi = {10.1038/s41593-022-01212-4},
	abstract = {Daily experience suggests that we perceive distances near us linearly. However, the actual geometry of spatial representation in the brain is unknown. Here we report that neurons in the {CA}1 region of rat hippocampus that mediate spatial perception represent space according to a non-linear hyperbolic geometry. This geometry uses an exponential scale and yields greater positional information than a linear scale. We found that the size of the representation matches the optimal predictions for the number of {CA}1 neurons. The representations also dynamically expanded proportional to the logarithm of time that the animal spent exploring the environment, in correspondence with the maximal mutual information that can be received. The dynamic changes tracked even small variations due to changes in the running speed of the animal. These results demonstrate how neural circuits achieve efficient representations using dynamic hyperbolic geometry.},
	pages = {131--139},
	number = {1},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Zhang, Huanqiu and Rich, P. Dylan and Lee, Albert K. and Sharpee, Tatyana O.},
	urldate = {2023-02-19},
	date = {2023-01},
	langid = {english},
	keywords = {Learning and memory, Neural encoding},
}

@article{gunnarsdottir_multimodal_2022,
	title = {Multimodal gradient mapping of rodent hippocampus},
	volume = {253},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811922002117},
	doi = {10.1016/j.neuroimage.2022.119082},
	abstract = {The hippocampus plays a central role in supporting our coherent and enduring sense of self and our place in the world. Understanding its functional organisation is central to understanding this complex role. Previous studies suggest function varies along a long hippocampal axis, but there is disagreement about the presence of sharp discontinuities or gradual change along that axis. Other open questions relate to the underlying drivers of this variation and the conservation of organisational principles across species. Here, we delineate the primary organisational principles underlying patterns of hippocampal functional connectivity ({FC}) in the mouse using gradient analysis on resting state {fMRI} data. We further applied gradient analysis to mouse gene co-expression data to examine the relationship between variation in genomic anatomy and functional organisation. Two principal {FC} gradients along a hippocampal axis were revealed. The principal gradient exhibited a sharp discontinuity that divided the hippocampus into dorsal and ventral compartments. The second, more continuous, gradient followed the long axis of the ventral compartment. Dorsal regions were more strongly connected to areas involved in spatial navigation while ventral regions were more strongly connected to areas involved in emotion, recapitulating patterns seen in humans. In contrast, gene co-expression gradients showed a more segregated and discrete organisation. Our findings suggest that hippocampal functional organisation exhibits both sharp and gradual transitions and that hippocampal genomic anatomy exerts only a subtle influence on this organisation.},
	pages = {119082},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Gunnarsdóttir, Brynja and Zerbi, Valerio and Kelly, Clare},
	urldate = {2023-02-26},
	date = {2022-06-01},
	langid = {english},
	keywords = {Functional connectivity, Gradients, notion, Gene expression, Hippocampus, {fMRI}, Rodent},
	file = {ScienceDirect Full Text PDF:/home/ajl/Zotero/storage/MYZ7VJ5P/Gunnarsdóttir et al. - 2022 - Multimodal gradient mapping of rodent hippocampus.pdf:application/pdf;ScienceDirect Snapshot:/home/ajl/Zotero/storage/B7I46WIH/S1053811922002117.html:text/html},
}

@misc{chien_maeeg_2022,
	title = {{MAEEG}: Masked Auto-encoder for {EEG} Representation Learning},
	url = {http://arxiv.org/abs/2211.02625},
	doi = {10.48550/arXiv.2211.02625},
	shorttitle = {{MAEEG}},
	abstract = {Decoding information from bio-signals such as {EEG}, using machine learning has been a challenge due to the small data-sets and difficulty to obtain labels. We propose a reconstruction-based self-supervised learning model, the masked auto-encoder for {EEG} ({MAEEG}), for learning {EEG} representations by learning to reconstruct the masked {EEG} features using a transformer architecture. We found that {MAEEG} can learn representations that significantly improve sleep stage classification ({\textasciitilde}5\% accuracy increase) when only a small number of labels are given. We also found that input sample lengths and different ways of masking during reconstruction-based {SSL} pretraining have a huge effect on downstream model performance. Specifically, learning to reconstruct a larger proportion and more concentrated masked signal results in better performance on sleep classification. Our findings provide insight into how reconstruction-based {SSL} could help representation learning for {EEG}.},
	number = {{arXiv}:2211.02625},
	publisher = {{arXiv}},
	author = {Chien, Hsiang-Yun Sherry and Goh, Hanlin and Sandino, Christopher M. and Cheng, Joseph Y.},
	urldate = {2023-02-26},
	date = {2022-10-27},
	eprinttype = {arxiv},
	eprint = {2211.02625 [cs, eess]},
	keywords = {Computer Science - Machine Learning, notion, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/ZYQ99H3Z/Chien et al. - 2022 - MAEEG Masked Auto-encoder for EEG Representation .pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/YQTEU729/2211.html:text/html},
}

@misc{pang_masked_2022,
	title = {Masked Autoencoders for Point Cloud Self-supervised Learning},
	url = {http://arxiv.org/abs/2203.06604},
	doi = {10.48550/arXiv.2203.06604},
	abstract = {As a promising scheme of self-supervised learning, masked autoencoding has significantly advanced natural language processing and computer vision. Inspired by this, we propose a neat scheme of masked autoencoders for point cloud self-supervised learning, addressing the challenges posed by point cloud's properties, including leakage of location information and uneven information density. Concretely, we divide the input point cloud into irregular point patches and randomly mask them at a high ratio. Then, a standard Transformer based autoencoder, with an asymmetric design and a shifting mask tokens operation, learns high-level latent features from unmasked point patches, aiming to reconstruct the masked point patches. Extensive experiments show that our approach is efficient during pre-training and generalizes well on various downstream tasks. Specifically, our pre-trained models achieve 85.18\% accuracy on {ScanObjectNN} and 94.04\% accuracy on {ModelNet}40, outperforming all the other self-supervised learning methods. We show with our scheme, a simple architecture entirely based on standard Transformers can surpass dedicated Transformer models from supervised learning. Our approach also advances state-of-the-art accuracies by 1.5\%-2.3\% in the few-shot object classification. Furthermore, our work inspires the feasibility of applying unified architectures from languages and images to the point cloud.},
	number = {{arXiv}:2203.06604},
	publisher = {{arXiv}},
	author = {Pang, Yatian and Wang, Wenxiao and Tay, Francis E. H. and Liu, Wei and Tian, Yonghong and Yuan, Li},
	urldate = {2023-02-26},
	date = {2022-03-28},
	eprinttype = {arxiv},
	eprint = {2203.06604 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/FUMLT8Q4/Pang et al. - 2022 - Masked Autoencoders for Point Cloud Self-supervise.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/CJQS8ZFJ/2203.html:text/html},
}

@article{sotelo_viewing_2003,
	title = {Viewing the brain through the master hand of Ramon y Cajal},
	volume = {4},
	rights = {2003 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1010},
	doi = {10.1038/nrn1010},
	abstract = {For most neuroscientists, the roots of our discipline stem from Santiago Ramón y Cajal, the Spanish scientist who, during almost half a century of patient work, showed that the nervous system is made up of independent nerve cells. His studies on the anatomical organization of the brain are still a source of inspiration for many of us. His monumental body of work fully justifies that Ramón y Cajal be singled out as the founder of modern neuroscience.},
	pages = {71--77},
	number = {1},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Sotelo, Constantino},
	urldate = {2023-02-26},
	date = {2003-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {notion, Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	file = {Full Text PDF:/home/ajl/Zotero/storage/YQ79U9CS/Sotelo - 2003 - Viewing the brain through the master hand of Ramon.pdf:application/pdf},
}

@misc{mcallester_mathematics_2023,
	title = {On the Mathematics of Diffusion Models},
	url = {http://arxiv.org/abs/2301.11108},
	doi = {10.48550/arXiv.2301.11108},
	abstract = {This paper presents the stochastic differential equations of diffusion models assuming only familiarity with Gaussian distributions. This treatment of the diffusion model {SDE} and the associated reverse-time {SDEs} unifies the {VAE} and score-matching treatments. It also yields the contribution of this paper -- a novel likelihood formula derived from a non-variational {VAE} analysis (equations (10) and (12) in the text). The paper presents the mathematics directly with attributions saved for a final section.},
	number = {{arXiv}:2301.11108},
	publisher = {{arXiv}},
	author = {{McAllester}, David},
	urldate = {2023-02-26},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2301.11108 [cs, math]},
	keywords = {Computer Science - Machine Learning, notion, Computer Science - Artificial Intelligence, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/NMBJ2MKX/McAllester - 2023 - On the Mathematics of Diffusion Models.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/NUTPRG98/2301.html:text/html},
}

@article{cembrowski_heterogeneity_2019,
	title = {Heterogeneity within classical cell types is the rule: lessons from hippocampal pyramidal neurons},
	volume = {20},
	rights = {2019 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-019-0125-5},
	doi = {10.1038/s41583-019-0125-5},
	shorttitle = {Heterogeneity within classical cell types is the rule},
	abstract = {The mechanistic operation of brain regions is often interpreted by partitioning constituent neurons into ‘cell types’. Historically, such cell types were broadly defined by their correspondence to gross features of the nervous system (such as cytoarchitecture). Modern-day neuroscientific techniques, enabling a more nuanced examination of neuronal properties, have illustrated a wealth of heterogeneity within these classical cell types. Here, we review the extent of this within-cell-type heterogeneity in one of the simplest cortical regions of the mammalian brain, the rodent hippocampus. We focus on the mounting evidence that the classical {CA}3, {CA}1 and subiculum pyramidal cell types all exhibit prominent and spatially patterned within-cell-type heterogeneity, and suggest these cell types provide a model system for exploring the organization and function of such heterogeneity. Given that the hippocampus is structurally simple and evolutionarily ancient, within-cell-type heterogeneity is likely to be a general and crucial feature of the mammalian brain.},
	pages = {193--204},
	number = {4},
	journaltitle = {Nature Reviews Neuroscience},
	shortjournal = {Nat Rev Neurosci},
	author = {Cembrowski, Mark S. and Spruston, Nelson},
	urldate = {2023-02-27},
	date = {2019-04},
	langid = {english},
	note = {Number: 4
Publisher: Nature Publishing Group},
	keywords = {notion, Hippocampus, Cellular neuroscience, Molecular neuroscience},
	file = {Full Text PDF:/home/ajl/Zotero/storage/S7KCXLDU/Cembrowski and Spruston - 2019 - Heterogeneity within classical cell types is the r.pdf:application/pdf},
}

@misc{tseng_graphguide_2023,
	title = {{GraphGUIDE}: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion},
	url = {http://arxiv.org/abs/2302.03790},
	doi = {10.48550/arXiv.2302.03790},
	shorttitle = {{GraphGUIDE}},
	abstract = {Diffusion models achieve state-of-the-art performance in generating realistic objects and have been successfully applied to images, text, and videos. Recent work has shown that diffusion can also be defined on graphs, including graph representations of drug-like molecules. Unfortunately, it remains difficult to perform conditional generation on graphs in a way which is interpretable and controllable. In this work, we propose {GraphGUIDE}, a novel framework for graph generation using diffusion models, where edges in the graph are flipped or set at each discrete time step. We demonstrate {GraphGUIDE} on several graph datasets, and show that it enables full control over the conditional generation of arbitrary structural properties without relying on predefined labels. Our framework for graph diffusion can have a large impact on the interpretable conditional generation of graphs, including the generation of drug-like molecules with desired properties in a way which is informed by experimental evidence.},
	number = {{arXiv}:2302.03790},
	publisher = {{arXiv}},
	author = {Tseng, Alex M. and Diamant, Nathaniel and Biancalani, Tommaso and Scalia, Gabriele},
	urldate = {2023-02-27},
	date = {2023-02-07},
	eprinttype = {arxiv},
	eprint = {2302.03790 [cs]},
	keywords = {Computer Science - Machine Learning, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/XQ4IVR43/Tseng et al. - 2023 - GraphGUIDE interpretable and controllable conditi.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/C4E9N4GJ/2302.html:text/html},
}

@misc{reddi_convergence_2019,
	title = {On the Convergence of Adam and Beyond},
	url = {http://arxiv.org/abs/1904.09237},
	doi = {10.48550/arXiv.1904.09237},
	abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as {RMSProp}, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
	number = {{arXiv}:1904.09237},
	publisher = {{arXiv}},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	urldate = {2023-02-28},
	date = {2019-04-19},
	eprinttype = {arxiv},
	eprint = {1904.09237 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/DC4YEWD8/Reddi et al. - 2019 - On the Convergence of Adam and Beyond.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/KATQMLI8/1904.html:text/html},
}

@misc{kramer_training_2020,
	title = {Training Invertible Linear Layers through Rank-One Perturbations},
	url = {http://arxiv.org/abs/2010.07033},
	doi = {10.48550/arXiv.2010.07033},
	abstract = {Many types of neural network layers rely on matrix properties such as invertibility or orthogonality. Retaining such properties during optimization with gradient-based stochastic optimizers is a challenging task, which is usually addressed by either reparameterization of the affected parameters or by directly optimizing on the manifold. This work presents a novel approach for training invertible linear layers. In lieu of directly optimizing the network parameters, we train rank-one perturbations and add them to the actual weight matrices infrequently. This P\${\textasciicircum}\{4\}\$Inv update allows keeping track of inverses and determinants without ever explicitly computing them. We show how such invertible blocks improve the mixing and thus the mode separation of the resulting normalizing flows. Furthermore, we outline how the P\${\textasciicircum}4\$ concept can be utilized to retain properties other than invertibility.},
	number = {{arXiv}:2010.07033},
	publisher = {{arXiv}},
	author = {Krämer, Andreas and Köhler, Jonas and Noé, Frank},
	urldate = {2023-03-01},
	date = {2020-11-30},
	eprinttype = {arxiv},
	eprint = {2010.07033 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion, Physics - Data Analysis, Statistics and Probability, Mathematics - Optimization and Control, 68T07, 82-10, Physics - Chemical Physics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/BBLAN54Y/Krämer et al. - 2020 - Training Invertible Linear Layers through Rank-One.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/U2I2I3RM/2010.html:text/html},
}

@misc{hirsch_patchperpix_2022,
	title = {{PatchPerPix} for Instance Segmentation},
	url = {http://arxiv.org/abs/2001.07626},
	doi = {10.48550/arXiv.2001.07626},
	abstract = {We present a novel method for proposal free instance segmentation that can handle sophisticated object shapes which span large parts of an image and form dense object clusters with crossovers. Our method is based on predicting dense local shape descriptors, which we assemble to form instances. All instances are assembled simultaneously in one go. To our knowledge, our method is the first non-iterative method that yields instances that are composed of learnt shape patches. We evaluate our method on a diverse range of data domains, where it defines the new state of the art on four benchmarks, namely the {ISBI} 2012 {EM} segmentation benchmark, the {BBBC}010 C. elegans dataset, and 2d as well as 3d fluorescence microscopy data of cell nuclei. We show furthermore that our method also applies to 3d light microscopy data of Drosophila neurons, which exhibit extreme cases of complex shape clusters},
	number = {{arXiv}:2001.07626},
	publisher = {{arXiv}},
	author = {Hirsch, Peter and Mais, Lisa and Kainmueller, Dagmar},
	urldate = {2023-03-02},
	date = {2022-12-08},
	eprinttype = {arxiv},
	eprint = {2001.07626 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/AQAJGX6Q/Hirsch et al. - 2022 - PatchPerPix for Instance Segmentation.pdf:application/pdf;arXiv.org Snapshot:/home/ajl/Zotero/storage/CHCBXH5X/2001.html:text/html},
}

@article{malin-mayor_automated_2023,
	title = {Automated reconstruction of whole-embryo cell lineages by learning from sparse annotations},
	volume = {41},
	rights = {2022 The Author(s)},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01427-7},
	doi = {10.1038/s41587-022-01427-7},
	abstract = {We present a method to automatically identify and track nuclei in time-lapse microscopy recordings of entire developing embryos. The method combines deep learning and global optimization. On a mouse dataset, it reconstructs 75.8\% of cell lineages spanning 1 h, as compared to 31.8\% for the competing method. Our approach improves understanding of where and when cell fate decisions are made in developing embryos, tissues, and organs.},
	pages = {44--49},
	number = {1},
	journaltitle = {Nature Biotechnology},
	shortjournal = {Nat Biotechnol},
	author = {Malin-Mayor, Caroline and Hirsch, Peter and Guignard, Leo and {McDole}, Katie and Wan, Yinan and Lemon, William C. and Kainmueller, Dagmar and Keller, Philipp J. and Preibisch, Stephan and Funke, Jan},
	urldate = {2023-03-02},
	date = {2023-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Image processing, Machine learning, notion, Cell lineage},
	file = {Full Text PDF:/home/ajl/Zotero/storage/PKJ6RI3Y/Malin-Mayor et al. - 2023 - Automated reconstruction of whole-embryo cell line.pdf:application/pdf},
}

@misc{wellawatte_perspective_2022,
	title = {A Perspective on Explanations of Molecular Prediction Models},
	url = {https://chemrxiv.org/engage/chemrxiv/article-details/639222a114d92d7cd6a65e90},
	doi = {10.26434/chemrxiv-2022-qfv02},
	abstract = {Chemists can be skeptical in using deep learning ({DL}) in decision making, due to the lack of interpretability in "black-box" models. Explainable artificial intelligence ({XAI}) is a branch of {AI} which addresses this drawback by providing tools to interpret {DL} models and their predictions. We review the principles of {XAI} in the domain of chemistry and emerging methods for creating and evaluating explanations. Then we focus methods developed by our group and their application to predicting solubility, blood-brain barrier permeability, and the scent of molecules. We show that {XAI} methods like chemical counterfactuals and descriptor explanations can both explain {DL} predictions and give insight into structure-property relationships. Finally, we discuss how a two step process of highly accurate black-box modeling and then creating explanations gives both highly accurate predictions and clear structure-property relationships.},
	publisher = {{ChemRxiv}},
	author = {Wellawatte, Geemi P. and Gandhi, Heta A. and Seshadri, Aditi and White, Andrew D.},
	urldate = {2023-03-05},
	date = {2022-12-09},
	langid = {english},
	keywords = {{XAI}},
}

@misc{cao_choose_2021,
	title = {Choose a Transformer: Fourier or Galerkin},
	url = {http://arxiv.org/abs/2105.14995},
	doi = {10.48550/arXiv.2105.14995},
	shorttitle = {Choose a Transformer},
	abstract = {In this paper, we apply the self-attention from the state-of-the-art Transformer in Attention Is All You Need for the first time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efficacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the first time that the softmax normalization in the scaled dot-product attention is sufficient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projection is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnormalized data. Finally, we present three operator learning experiments, including the viscid Burgers' equation, an interface Darcy flow, and an inverse interface coefficient identification problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows significant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts.},
	number = {{arXiv}:2105.14995},
	publisher = {{arXiv}},
	author = {Cao, Shuhao},
	urldate = {2023-03-05},
	date = {2021-11-01},
	eprinttype = {arxiv},
	eprint = {2105.14995 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, 68T99, 65D15, 65M99, 65N99},
}

@misc{suk_se3_2023,
	title = {{SE}(3) symmetry lets graph neural networks learn arterial velocity estimation from small datasets},
	url = {http://arxiv.org/abs/2302.08780},
	doi = {10.48550/arXiv.2302.08780},
	abstract = {Hemodynamic velocity fields in coronary arteries could be the basis of valuable biomarkers for diagnosis, prognosis and treatment planning in cardiovascular disease. Velocity fields are typically obtained from patient-specific 3D artery models via computational fluid dynamics ({CFD}). However, {CFD} simulation requires meticulous setup by experts and is time-intensive, which hinders large-scale acceptance in clinical practice. To address this, we propose graph neural networks ({GNN}) as an efficient black-box surrogate method to estimate 3D velocity fields mapped to the vertices of tetrahedral meshes of the artery lumen. We train these {GNNs} on synthetic artery models and {CFD}-based ground truth velocity fields. Once the {GNN} is trained, velocity estimates in a new and unseen artery can be obtained with 36-fold speed-up compared to {CFD}. We demonstrate how to construct an {SE}(3)-equivariant {GNN} that is independent of the spatial orientation of the input mesh and show how this reduces the necessary amount of training data compared to a baseline neural network.},
	number = {{arXiv}:2302.08780},
	publisher = {{arXiv}},
	author = {Suk, Julian and Brune, Christoph and Wolterink, Jelmer M.},
	urldate = {2023-03-05},
	date = {2023-02-17},
	eprinttype = {arxiv},
	eprint = {2302.08780 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Fluid Dynamics, Mathematics - Group Theory},
}

@article{qiu_persistent_2023,
	title = {Persistent spectral theory-guided protein engineering},
	volume = {3},
	rights = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-022-00394-y},
	doi = {10.1038/s43588-022-00394-y},
	abstract = {Although protein engineering, which iteratively optimizes protein fitness by screening the gigantic mutational space, is constrained by experimental capacity, various machine learning models have substantially expedited protein engineering. Three-dimensional protein structures promise further advantages, but their intricate geometric complexity hinders their applications in deep mutational screening. Persistent homology, an established algebraic topology tool for protein structural complexity reduction, fails to capture the homotopic shape evolution during filtration of given data. Here we introduce a Topology-offered Protein Fitness ({TopFit}) framework to complement protein sequence and structure embeddings. Equipped with an ensemble regression strategy, {TopFit} integrates the persistent spectral theory, which is a new topological Laplacian, and two auxiliary sequence embeddings to capture mutation-induced topological invariant, shape evolution and sequence disparity in the protein fitness landscape. The performance of {TopFit} is assessed by 34 benchmark datasets with 128,634 variants, involving a vast variety of protein structure acquisition modalities and training set size variations.},
	pages = {149--163},
	number = {2},
	journaltitle = {Nature Computational Science},
	shortjournal = {Nat Comput Sci},
	author = {Qiu, Yuchi and Wei, Guo-Wei},
	urldate = {2023-03-05},
	date = {2023-02},
	langid = {english},
	keywords = {Machine learning, Applied mathematics, Protein design},
}

@misc{yim_se3_2023,
	title = {{SE}(3) diffusion model with application to protein backbone generation},
	url = {http://arxiv.org/abs/2302.02277},
	doi = {10.48550/arXiv.2302.02277},
	abstract = {The design of novel protein structures remains a challenge in protein engineering for applications across biomedicine and chemistry. In this line of work, a diffusion model over rigid bodies in 3D (referred to as frames) has shown success in generating novel, functional protein backbones that have not been observed in nature. However, there exists no principled methodological framework for diffusion on {SE}(3), the space of orientation preserving rigid motions in R3, that operates on frames and confers the group invariance. We address these shortcomings by developing theoretical foundations of {SE}(3) invariant diffusion models on multiple frames followed by a novel framework, {FrameDiff}, for learning the {SE}(3) equivariant score over multiple frames. We apply {FrameDiff} on monomer backbone generation and find it can generate designable monomers up to 500 amino acids without relying on a pretrained protein structure prediction network that has been integral to previous methods. We find our samples are capable of generalizing beyond any known protein structure.},
	number = {{arXiv}:2302.02277},
	publisher = {{arXiv}},
	author = {Yim, Jason and Trippe, Brian L. and De Bortoli, Valentin and Mathieu, Emile and Doucet, Arnaud and Barzilay, Regina and Jaakkola, Tommi},
	urldate = {2023-03-05},
	date = {2023-02-11},
	eprinttype = {arxiv},
	eprint = {2302.02277 [cs, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Quantitative Methods},
}

@misc{tseng_graphguide_2023-1,
	title = {{GraphGUIDE}: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion},
	url = {http://arxiv.org/abs/2302.03790},
	doi = {10.48550/arXiv.2302.03790},
	shorttitle = {{GraphGUIDE}},
	abstract = {Diffusion models achieve state-of-the-art performance in generating realistic objects and have been successfully applied to images, text, and videos. Recent work has shown that diffusion can also be defined on graphs, including graph representations of drug-like molecules. Unfortunately, it remains difficult to perform conditional generation on graphs in a way which is interpretable and controllable. In this work, we propose {GraphGUIDE}, a novel framework for graph generation using diffusion models, where edges in the graph are flipped or set at each discrete time step. We demonstrate {GraphGUIDE} on several graph datasets, and show that it enables full control over the conditional generation of arbitrary structural properties without relying on predefined labels. Our framework for graph diffusion can have a large impact on the interpretable conditional generation of graphs, including the generation of drug-like molecules with desired properties in a way which is informed by experimental evidence.},
	number = {{arXiv}:2302.03790},
	publisher = {{arXiv}},
	author = {Tseng, Alex M. and Diamant, Nathaniel and Biancalani, Tommaso and Scalia, Gabriele},
	urldate = {2023-03-05},
	date = {2023-02-07},
	eprinttype = {arxiv},
	eprint = {2302.03790 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kratsios_generative_2023,
	title = {Generative Ornstein-Uhlenbeck Markets via Geometric Deep Learning},
	url = {http://arxiv.org/abs/2302.09176},
	doi = {10.48550/arXiv.2302.09176},
	abstract = {We consider the problem of simultaneously approximating the conditional distribution of market prices and their log returns with a single machine learning model. We show that an instance of the {GDN} model of Kratsios and Papon (2022) solves this problem without having prior assumptions on the market's "clipped" log returns, other than that they follow a generalized Ornstein-Uhlenbeck process with a priori unknown dynamics. We provide universal approximation guarantees for these conditional distributions and contingent claims with a Lipschitz payoff function.},
	number = {{arXiv}:2302.09176},
	publisher = {{arXiv}},
	author = {Kratsios, Anastasis and Hyndman, Cody},
	urldate = {2023-03-05},
	date = {2023-02-17},
	eprinttype = {arxiv},
	eprint = {2302.09176 [cs, q-fin]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, 68T07, 62M45, 91G60, 91G20, Quantitative Finance - Computational Finance},
}

@misc{peidli_scperturb_2023,
	title = {{scPerturb}: Harmonized Single-Cell Perturbation Data},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2022.08.20.504663v3},
	doi = {10.1101/2022.08.20.504663},
	shorttitle = {{scPerturb}},
	abstract = {Recent biotechnological advances led to growing numbers of single-cell perturbation studies, which reveal molecular and phenotypic responses to large numbers of perturbations. However, analysis across diverse datasets is typically hampered by differences in format, naming conventions, and data filtering. In order to facilitate development and benchmarking of computational methods in systems biology, we collect a set of 44 publicly available single-cell perturbation-response datasets with molecular readouts, including transcriptomics, proteomics and epigenomics. We apply uniform pre-processing and quality control pipelines and harmonize feature annotations. The resulting information resource enables efficient development and testing of computational analysis methods, and facilitates direct comparison and integration across datasets. In addition, we introduce E-statistics for perturbation effect quantification and significance testing, and demonstrate E-distance as a general distance measure for single cell data. Using these datasets, we illustrate the application of E-statistics for quantifying perturbation similarity and efficacy. The data and a package for computing E-statistics is publicly available at scperturb.org. This work provides an information resource and guide for researchers working with single-cell perturbation data, highlights conceptual considerations for new experiments, and makes concrete recommendations for optimal cell counts and read depth.},
	publisher = {{bioRxiv}},
	author = {Peidli, Stefan and Green, Tessa D. and Shen, Ciyue and Gross, Torsten and Min, Joseph and Garda, Samuele and Yuan, Bo and Schumacher, Linus J. and Taylor-King, Jake P. and Marks, Debora S. and Luna, Augustin and Blüthgen, Nils and Sander, Chris},
	urldate = {2023-03-05},
	date = {2023-01-25},
	langid = {english},
}

@inproceedings{wang_global_2020,
	title = {Global Context Enhanced Graph Neural Networks for Session-based Recommendation},
	url = {http://arxiv.org/abs/2106.05081},
	doi = {10.1145/3397271.3401142},
	abstract = {Session-based recommendation ({SBR}) is a challenging task, which aims at recommending items based on anonymous behavior sequences. Almost all the existing solutions for {SBR} model user preference only based on the current session without exploiting the other sessions, which may contain both relevant and irrelevant item-transitions to the current session. This paper proposes a novel approach, called Global Context Enhanced Graph Neural Networks ({GCE}-{GNN}) to exploit item transitions over all sessions in a more subtle manner for better inferring the user preference of the current session. Specifically, {GCE}-{GNN} learns two levels of item embeddings from session graph and global graph, respectively: (i) Session graph, which is to learn the session-level item embedding by modeling pairwise item-transitions within the current session; and (ii) Global graph, which is to learn the global-level item embedding by modeling pairwise item-transitions over all sessions. In {GCE}-{GNN}, we propose a novel global-level item representation learning layer, which employs a session-aware attention mechanism to recursively incorporate the neighbors' embeddings of each node on the global graph. We also design a session-level item representation learning layer, which employs a {GNN} on the session graph to learn session-level item embeddings within the current session. Moreover, {GCE}-{GNN} aggregates the learnt item representations in the two levels with a soft attention mechanism. Experiments on three benchmark datasets demonstrate that {GCE}-{GNN} outperforms the state-of-the-art methods consistently.},
	pages = {169--178},
	booktitle = {Proceedings of the 43rd International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	author = {Wang, Ziyang and Wei, Wei and Cong, Gao and Li, Xiao-Li and Mao, Xian-Ling and Qiu, Minghui},
	urldate = {2023-03-05},
	date = {2020-07-25},
	eprinttype = {arxiv},
	eprint = {2106.05081 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{dong_attention_2021,
	title = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
	url = {http://arxiv.org/abs/2103.03404},
	doi = {10.48550/arXiv.2103.03404},
	shorttitle = {Attention is Not All You Need},
	abstract = {Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons ({MLPs}), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and {MLPs} stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.},
	number = {{arXiv}:2103.03404},
	publisher = {{arXiv}},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	urldate = {2023-03-05},
	date = {2021-03-04},
	eprinttype = {arxiv},
	eprint = {2103.03404 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{he_deep_2023,
	title = {Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation},
	url = {http://arxiv.org/abs/2302.10322},
	doi = {10.48550/arXiv.2302.10322},
	shorttitle = {Deep Transformers without Shortcuts},
	abstract = {Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks ({DNNs}), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide {NN} kernel theory to improve signal propagation in vanilla {DNNs} (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control. And so the question remains: is it possible to train deep vanilla transformers? We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on {WikiText}-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.},
	number = {{arXiv}:2302.10322},
	publisher = {{arXiv}},
	author = {He, Bobby and Martens, James and Zhang, Guodong and Botev, Aleksandar and Brock, Andrew and Smith, Samuel L. and Teh, Yee Whye},
	urldate = {2023-03-05},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.10322 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{he_deep_2023-1,
	title = {Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation},
	url = {http://arxiv.org/abs/2302.10322},
	doi = {10.48550/arXiv.2302.10322},
	shorttitle = {Deep Transformers without Shortcuts},
	abstract = {Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks ({DNNs}), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide {NN} kernel theory to improve signal propagation in vanilla {DNNs} (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control. And so the question remains: is it possible to train deep vanilla transformers? We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on {WikiText}-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.},
	number = {{arXiv}:2302.10322},
	publisher = {{arXiv}},
	author = {He, Bobby and Martens, James and Zhang, Guodong and Botev, Aleksandar and Brock, Andrew and Smith, Samuel L. and Teh, Yee Whye},
	urldate = {2023-03-05},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.10322 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{zhang_deep_2022,
	title = {Deep Learning without Shortcuts: Shaping the Kernel with Tailored Rectifiers},
	url = {http://arxiv.org/abs/2203.08120},
	doi = {10.48550/arXiv.2203.08120},
	shorttitle = {Deep Learning without Shortcuts},
	abstract = {Training very deep neural networks is still an extremely challenging task. The common solution is to use shortcut connections and normalization layers, which are both crucial ingredients in the popular {ResNet} architecture. However, there is strong evidence to suggest that {ResNets} behave more like ensembles of shallower networks than truly deep ones. Recently, it was shown that deep vanilla networks (i.e. networks without normalization layers or shortcut connections) can be trained as fast as {ResNets} by applying certain transformations to their activation functions. However, this method (called Deep Kernel Shaping) isn't fully compatible with {ReLUs}, and produces networks that overfit significantly more than {ResNets} on {ImageNet}. In this work, we rectify this situation by developing a new type of transformation that is fully compatible with a variant of {ReLUs} -- Leaky {ReLUs}. We show in experiments that our method, which introduces negligible extra computational cost, achieves validation accuracies with deep vanilla networks that are competitive with {ResNets} (of the same width/depth), and significantly higher than those obtained with the Edge of Chaos ({EOC}) method. And unlike with {EOC}, the validation accuracies we obtain do not get worse with depth.},
	number = {{arXiv}:2203.08120},
	publisher = {{arXiv}},
	author = {Zhang, Guodong and Botev, Aleksandar and Martens, James},
	urldate = {2023-03-05},
	date = {2022-03-15},
	eprinttype = {arxiv},
	eprint = {2203.08120 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{tenachi_deep_2023,
	title = {Deep symbolic regression for physics guided by units constraints: toward the automated discovery of physical laws},
	url = {http://arxiv.org/abs/2303.03192},
	doi = {10.48550/arXiv.2303.03192},
	shorttitle = {Deep symbolic regression for physics guided by units constraints},
	abstract = {Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, efforts have not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present \${\textbackslash}Phi\$-{SO}, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because it restricts enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful for instance when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations to noisy data. We showcase our machinery on a panel of examples from astrophysics.},
	number = {{arXiv}:2303.03192},
	publisher = {{arXiv}},
	author = {Tenachi, Wassim and Ibata, Rodrigo and Diakogiannis, Foivos I.},
	urldate = {2023-03-09},
	date = {2023-03-06},
	eprinttype = {arxiv},
	eprint = {2303.03192 [astro-ph, physics:physics]},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, notion, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/ajl/Zotero/storage/9BFSA8TX/Tenachi et al. - 2023 - Deep symbolic regression for physics guided by uni.pdf:application/pdf},
}

@misc{yuan_massive_2023,
	title = {Massive Multiplexing of Spatially Resolved Single Neuron Projections with Axonal {BARseq}},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.18.528865v1},
	doi = {10.1101/2023.02.18.528865},
	abstract = {Neurons in the cortex are heterogenous, sending diverse axonal projections to multiple brain regions. Unraveling the logic of these projections requires single-neuron resolution. Although a growing number of techniques have enabled high-throughput reconstruction, these techniques are typically limited to dozens or at most hundreds of neurons per brain, requiring that statistical analyses combine data from different specimens. Here we present axonal {BARseq}, a high-throughput approach based on reading out nucleic acid barcodes using in situ {RNA} sequencing, which enables analysis of even densely labeled neurons. As a proof of principle, we have mapped the long-range projections of {\textgreater}8000 mouse primary auditory cortex neurons from a single brain. We identified major cell types based on projection targets and axonal trajectory. The large sample size enabled us to systematically quantify the projections of intratelencephalic ({IT}) neurons, and revealed that individual {IT} neurons project to different layers in an area-dependent fashion. Axonal {BARseq} is a powerful technique for studying the heterogeneity of single neuronal projections at high throughput within individual brains.},
	publisher = {{bioRxiv}},
	author = {Yuan, Li and Chen, Xiaoyin and Zhan, Huiqing and Gilbert, Henry L. and Zador, Anthony M.},
	urldate = {2023-03-05},
	date = {2023-02-18},
	langid = {english},
}

@article{bowe_acute_2022,
	title = {Acute and postacute sequelae associated with {SARS}-{CoV}-2 reinfection},
	volume = {28},
	rights = {2022 This is a U.S. Government work and not under copyright protection in the {US}; foreign copyright protection may apply},
	issn = {1546-170X},
	url = {https://www.nature.com/articles/s41591-022-02051-3},
	doi = {10.1038/s41591-022-02051-3},
	abstract = {First infection with severe acute respiratory syndrome coronavirus 2 ({SARS}-{CoV}-2) is associated with increased risk of acute and postacute death and sequelae in various organ systems. Whether reinfection adds to risks incurred after first infection is unclear. Here we used the {US} Department of Veterans Affairs’ national healthcare database to build a cohort of individuals with one {SARS}-{CoV}-2 infection (n = 443,588), reinfection (two or more infections, n = 40,947) and a noninfected control (n = 5,334,729). We used inverse probability-weighted survival models to estimate risks and 6-month burdens of death, hospitalization and incident sequelae. Compared to no reinfection, reinfection contributed additional risks of death (hazard ratio ({HR}) = 2.17, 95\% confidence intervals ({CI}) 1.93–2.45), hospitalization ({HR} = 3.32, 95\% {CI} 3.13–3.51) and sequelae including pulmonary, cardiovascular, hematological, diabetes, gastrointestinal, kidney, mental health, musculoskeletal and neurological disorders. The risks were evident regardless of vaccination status. The risks were most pronounced in the acute phase but persisted in the postacute phase at 6 months. Compared to noninfected controls, cumulative risks and burdens of repeat infection increased according to the number of infections. Limitations included a cohort of mostly white males. The evidence shows that reinfection further increases risks of death, hospitalization and sequelae in multiple organ systems in the acute and postacute phase. Reducing overall burden of death and disease due to {SARS}-{CoV}-2 will require strategies for reinfection prevention.},
	pages = {2398--2405},
	number = {11},
	journaltitle = {Nature Medicine},
	shortjournal = {Nat Med},
	author = {Bowe, Benjamin and Xie, Yan and Al-Aly, Ziyad},
	urldate = {2023-03-05},
	date = {2022-11},
	langid = {english},
	keywords = {{SARS}-{CoV}-2, Viral infection},
}

@article{devries_one-year_2023,
	title = {One-Year Adverse Outcomes Among {US} Adults With Post–{COVID}-19 Condition vs Those Without {COVID}-19 in a Large Commercial Insurance Database},
	volume = {4},
	issn = {2689-0186},
	url = {https://doi.org/10.1001/jamahealthforum.2023.0010},
	doi = {10.1001/jamahealthforum.2023.0010},
	abstract = {Many individuals experience ongoing symptoms following the onset of {COVID}-19, characterized as postacute sequelae of {SARS}-{CoV}-2 or post–{COVID}-19 condition ({PCC}). Less is known about the long-term outcomes for these individuals.To quantify 1-year outcomes among individuals meeting a {PCC} definition compared with a control group of individuals without {COVID}-19.This case-control study with a propensity score–matched control group included members of commercial health plans and used national insurance claims data enhanced with laboratory results and mortality data from the Social Security Administration’s Death Master File and Datavant Flatiron data. The study sample consisted of adults meeting a claims-based definition for {PCC} with a 2:1 matched control cohort of individuals with no evidence of {COVID}-19 during the time period of April 1, 2020, to July 31, 2021.Individuals experiencing postacute sequelae of {SARS}-{CoV}-2 using a Centers for Disease Control and Prevention–based definition.Adverse outcomes, including cardiovascular and respiratory outcomes and mortality, for individuals with {PCC} and controls assessed over a 12-month period.The study population included 13 435 individuals with {PCC} and 26 870 individuals with no evidence of {COVID}-19 (mean [{SD}] age, 51 [15.1] years; 58.4\% female). During follow-up, the {PCC} cohort experienced increased health care utilization for a wide range of adverse outcomes: cardiac arrhythmias (relative risk [{RR}], 2.35; 95\% {CI}, 2.26-2.45), pulmonary embolism ({RR}, 3.64; 95\% {CI}, 3.23-3.92), ischemic stroke ({RR}, 2.17; 95\% {CI}, 1.98-2.52), coronary artery disease ({RR}, 1.78; 95\% {CI}, 1.70-1.88), heart failure ({RR}, 1.97; 95\% {CI}, 1.84-2.10), chronic obstructive pulmonary disease ({RR}, 1.94; 95\% {CI}, 1.88-2.00), and asthma ({RR}, 1.95; 95\% {CI}, 1.86-2.03). The {PCC} cohort also experienced increased mortality, as 2.8\% of individuals with {PCC} vs 1.2\% of controls died, implying an excess death rate of 16.4 per 1000 individuals.This case-control study leveraged a large commercial insurance database and found increased rates of adverse outcomes over a 1-year period for a {PCC} cohort surviving the acute phase of illness. The results indicate a need for continued monitoring for at-risk individuals, particularly in the area of cardiovascular and pulmonary management.},
	pages = {e230010},
	number = {3},
	journaltitle = {{JAMA} Health Forum},
	shortjournal = {{JAMA} Health Forum},
	author = {{DeVries}, Andrea and Shambhu, Sonali and Sloop, Sue and Overhage, J. Marc},
	urldate = {2023-03-05},
	date = {2023-03-03},
}

@misc{liang_multiviz_2023,
	title = {{MultiViz}: Towards Visualizing and Understanding Multimodal Models},
	url = {http://arxiv.org/abs/2207.00056},
	doi = {10.48550/arXiv.2207.00056},
	shorttitle = {{MultiViz}},
	abstract = {The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing {MultiViz}, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. {MultiViz} is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in {MultiViz} together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. {MultiViz} is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.},
	number = {{arXiv}:2207.00056},
	publisher = {{arXiv}},
	author = {Liang, Paul Pu and Lyu, Yiwei and Chhablani, Gunjan and Jain, Nihal and Deng, Zihao and Wang, Xingbo and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	urldate = {2023-03-05},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2207.00056 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Multimedia},
}

@misc{wang_flexible_2023,
	title = {A Flexible Nadaraya-Watson Head Can Offer Explainable and Calibrated Classification},
	url = {http://arxiv.org/abs/2212.03411},
	doi = {10.48550/arXiv.2212.03411},
	abstract = {In this paper, we empirically analyze a simple, non-learnable, and nonparametric Nadaraya-Watson ({NW}) prediction head that can be used with any neural network architecture. In the {NW} head, the prediction is a weighted average of labels from a support set. The weights are computed from distances between the query feature and support features. This is in contrast to the dominant approach of using a learnable classification head (e.g., a fully-connected layer) on the features, which can be challenging to interpret and can yield poorly calibrated predictions. Our empirical results on an array of computer vision tasks demonstrate that the {NW} head can yield better calibration with comparable accuracy compared to its parametric counterpart, particularly in data-limited settings. To further increase inference-time efficiency, we propose a simple approach that involves a clustering step run on the training set to create a relatively small distilled support set. Furthermore, we explore two means of interpretability/explainability that fall naturally from the {NW} head. The first is the label weights, and the second is our novel concept of the ``support influence function,'' which is an easy-to-compute metric that quantifies the influence of a support element on the prediction for a given query. As we demonstrate in our experiments, the influence function can allow the user to debug a trained model. We believe that the {NW} head is a flexible, interpretable, and highly useful building block that can be used in a range of applications.},
	number = {{arXiv}:2212.03411},
	publisher = {{arXiv}},
	author = {Wang, Alan Q. and Sabuncu, Mert R.},
	urldate = {2023-03-05},
	date = {2023-02-22},
	eprinttype = {arxiv},
	eprint = {2212.03411 [cs]},
	note = {version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{huuki-myers_integrated_2023,
	title = {Integrated single cell and unsupervised spatial transcriptomic analysis defines molecular anatomy of the human dorsolateral prefrontal cortex},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.15.528722v1},
	doi = {10.1101/2023.02.15.528722},
	abstract = {The molecular organization of the human neocortex has been historically studied in the context of its histological layers. However, emerging spatial transcriptomic technologies have enabled unbiased identification of transcriptionally-defined spatial domains that move beyond classic cytoarchitecture. Here we used the Visium spatial gene expression platform to generate a data-driven molecular neuroanatomical atlas across the anterior-posterior axis of the human dorsolateral prefrontal cortex ({DLPFC}). Integration with paired single nucleus {RNA}-sequencing data revealed distinct cell type compositions and cell-cell interactions across spatial domains. Using {PsychENCODE} and publicly available data, we map the enrichment of cell types and genes associated with neuropsychiatric disorders to discrete spatial domains. Finally, we provide resources for the scientific community to explore these integrated spatial and single cell datasets at research.libd.org/{spatialDLPFC}/.
Summary Generation of a molecular neuroanatomical map of the human prefrontal cortex reveals novel spatial domains and cell-cell interactions relevant for psychiatric disease.},
	publisher = {{bioRxiv}},
	author = {Huuki-Myers, Louise and Spangler, Abby and Eagles, Nick and Montgomery, Kelsey D. and Kwon, Sang Ho and Guo, Boyi and Grant-Peters, Melissa and Divecha, Heena R. and Tippani, Madhavi and Sriworarat, Chaichontat and Nguyen, Annie B. and Ravichandran, Prashanthi and Tran, Matthew N. and Seyedian, Arta and Consortium, {PsychENCODE} and Hyde, Thomas M. and Kleinman, Joel E. and Battle, Alexis and Page, Stephanie C. and Ryten, Mina and Hicks, Stephanie C. and Martinowich, Keri and Collado-Torres, Leonardo and Maynard, Kristen R.},
	urldate = {2023-03-05},
	date = {2023-02-15},
	langid = {english},
}

@article{fajardo-fontiveros_fundamental_2023,
	title = {Fundamental limits to learning closed-form mathematical models from data},
	volume = {14},
	rights = {2023 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-36657-z},
	doi = {10.1038/s41467-023-36657-z},
	abstract = {Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.},
	pages = {1043},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Fajardo-Fontiveros, Oscar and Reichardt, Ignasi and De Los Ríos, Harry R. and Duch, Jordi and Sales-Pardo, Marta and Guimerà, Roger},
	urldate = {2023-03-05},
	date = {2023-02-24},
	langid = {english},
	keywords = {Statistical physics, thermodynamics and nonlinear dynamics, Computational science},
}

@misc{wang_yolov7_2022,
	title = {{YOLOv}7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
	url = {http://arxiv.org/abs/2207.02696},
	doi = {10.48550/arXiv.2207.02696},
	shorttitle = {{YOLOv}7},
	abstract = {{YOLOv}7 surpasses all known object detectors in both speed and accuracy in the range from 5 {FPS} to 160 {FPS} and has the highest accuracy 56.8\% {AP} among all known real-time object detectors with 30 {FPS} or higher on {GPU} V100. {YOLOv}7-E6 object detector (56 {FPS} V100, 55.9\% {AP}) outperforms both transformer-based detector {SWIN}-L Cascade-Mask R-{CNN} (9.2 {FPS} A100, 53.9\% {AP}) by 509\% in speed and 2\% in accuracy, and convolutional-based detector {ConvNeXt}-{XL} Cascade-Mask R-{CNN} (8.6 {FPS} A100, 55.2\% {AP}) by 551\% in speed and 0.7\% {AP} in accuracy, as well as {YOLOv}7 outperforms: {YOLOR}, {YOLOX}, Scaled-{YOLOv}4, {YOLOv}5, {DETR}, Deformable {DETR}, {DINO}-5scale-R50, {ViT}-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train {YOLOv}7 only on {MS} {COCO} dataset from scratch without using any other datasets or pre-trained weights. Source code is released in https://github.com/{WongKinYiu}/yolov7.},
	number = {{arXiv}:2207.02696},
	publisher = {{arXiv}},
	author = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
	urldate = {2023-03-05},
	date = {2022-07-06},
	eprinttype = {arxiv},
	eprint = {2207.02696 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{tan_cerebellar_2023,
	title = {Cerebellar Granule Cells Develop Non-neuronal 3D Genome Architecture over the Lifespan},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.25.530020v1},
	doi = {10.1101/2023.02.25.530020},
	abstract = {The cerebellum contains most of the neurons in the human brain, and exhibits unique modes of development, malformation, and aging. For example, granule cells—the most abundant neuron type—develop unusually late and exhibit unique nuclear morphology. Here, by developing our high-resolution single-cell 3D genome assay Dip-C into population-scale (Pop-C) and virus-enriched ({vDip}-C) modes, we were able to resolve the first 3D genome structures of single cerebellar cells, create life-spanning 3D genome atlases for both human and mouse, and jointly measure transcriptome and chromatin accessibility during development. We found that while the transcriptome and chromatin accessibility of human granule cells exhibit a characteristic maturation pattern within the first year of postnatal life, 3D genome architecture gradually remodels throughout life into a non-neuronal state with ultra-long-range intra-chromosomal contacts and specific inter-chromosomal contacts. This 3D genome remodeling is conserved in mice, and robust to heterozygous deletion of chromatin remodeling disease-associated genes (Chd8 or Arid1b). Together these results reveal unexpected and evolutionarily-conserved molecular processes underlying the unique development and aging of the mammalian cerebellum.},
	publisher = {{bioRxiv}},
	author = {Tan, Longzhi and Shi, Jenny and Moghadami, Siavash and Wright, Cydney P. and Parasar, Bibudha and Seo, Yunji and Vallejo, Kristen and Cobos, Inma and Duncan, Laramie and Chen, Ritchie and Deisseroth, Karl},
	urldate = {2023-03-05},
	date = {2023-02-25},
	langid = {english},
}

@misc{wittmann_focused_2023,
	title = {Focused Decoding Enables 3D Anatomical Detection by Transformers},
	url = {http://arxiv.org/abs/2207.10774},
	doi = {10.48550/arXiv.2207.10774},
	abstract = {Detection Transformers represent end-to-end object detection approaches based on a Transformer encoder-decoder architecture, exploiting the attention mechanism for global relation modeling. Although Detection Transformers deliver results on par with or even superior to their highly optimized {CNN}-based counterparts operating on 2D natural images, their success is closely coupled to access to a vast amount of training data. This, however, restricts the feasibility of employing Detection Transformers in the medical domain, as access to annotated data is typically limited. To tackle this issue and facilitate the advent of medical Detection Transformers, we propose a novel Detection Transformer for 3D anatomical structure detection, dubbed Focused Decoder. Focused Decoder leverages information from an anatomical region atlas to simultaneously deploy query anchors and restrict the cross-attention's field of view to regions of interest, which allows for a precise focus on relevant anatomical structures. We evaluate our proposed approach on two publicly available {CT} datasets and demonstrate that Focused Decoder not only provides strong detection results and thus alleviates the need for a vast amount of annotated data but also exhibits exceptional and highly intuitive explainability of results via attention weights. Our code is available at https://github.com/bwittmann/transoar.},
	number = {{arXiv}:2207.10774},
	publisher = {{arXiv}},
	author = {Wittmann, Bastian and Navarro, Fernando and Shit, Suprosanna and Menze, Bjoern},
	urldate = {2023-03-05},
	date = {2023-02-26},
	eprinttype = {arxiv},
	eprint = {2207.10774 [cs]},
	note = {version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
}

@misc{zhang_contrastive_2022,
	title = {Contrastive Learning of Medical Visual Representations from Paired Images and Text},
	url = {http://arxiv.org/abs/2010.00747},
	doi = {10.48550/arXiv.2010.00747},
	abstract = {Learning visual representations of medical images (e.g., X-rays) is core to medical image understanding but its progress has been held back by the scarcity of human annotations. Existing work commonly relies on fine-tuning weights transferred from {ImageNet} pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. Meanwhile, several recent studies show exciting results from unsupervised contrastive learning from natural images, but we find these methods help little on medical images because of their high inter-class similarity. We propose {ConVIRT}, an alternative unsupervised strategy to learn medical visual representations by exploiting naturally occurring paired descriptive text. Our new method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test {ConVIRT} by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that it leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10{\textbackslash}\% as much labeled training data as an {ImageNet} initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
	number = {{arXiv}:2010.00747},
	publisher = {{arXiv}},
	author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
	urldate = {2023-03-05},
	date = {2022-09-19},
	eprinttype = {arxiv},
	eprint = {2010.00747 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{rosen_towards_2023,
	title = {Towards Universal Cell Embeddings: Integrating Single-cell {RNA}-seq Datasets across Species with {SATURN}},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.02.03.526939v1},
	doi = {10.1101/2023.02.03.526939},
	shorttitle = {Towards Universal Cell Embeddings},
	abstract = {Analysis of single-cell datasets generated from diverse organisms offers unprecedented opportunities to unravel fundamental evolutionary processes of conservation and diversification of cell types. However, inter-species genomic differences limit the joint analysis of crossspecies datasets to orthologous genes. Here, we present {SATURN}, a deep learning method for learning universal cell embeddings that encodes genes’ biological properties using protein language models. By coupling protein embeddings from language models with {RNA} expression, {SATURN} integrates datasets profiled from different species regardless of their genomic similarity. {SATURN} has a unique ability to detect functionally related genes co-expressed across species, redefining differential expression for cross-species analysis. We apply {SATURN} to three species whole-organism atlases and frog and zebrafish embryogenesis datasets. We show that cell embeddings learnt in {SATURN} can be effectively used to transfer annotations across species and identify both homologous and species-specific cell types, even across evolutionarily remote species. Finally, we use {SATURN} to reannotate the five species Cell Atlas of Human Trabecular Meshwork and Aqueous Outflow Structures and find evidence of potentially divergent functions between glaucoma associated genes in humans and other species.},
	publisher = {{bioRxiv}},
	author = {Rosen, Yanay and Brbić, Maria and Roohani, Yusuf and Swanson, Kyle and Li, Ziang and Leskovec, Jure},
	urldate = {2023-03-05},
	date = {2023-02-03},
	langid = {english},
}

@misc{passaro_reducing_2023,
	title = {Reducing {SO}(3) Convolutions to {SO}(2) for Efficient Equivariant {GNNs}},
	url = {http://arxiv.org/abs/2302.03655},
	doi = {10.48550/arXiv.2302.03655},
	abstract = {Graph neural networks that model 3D data, such as point clouds or atoms, are typically desired to be \${SO}(3)\$ equivariant, i.e., equivariant to 3D rotations. Unfortunately equivariant convolutions, which are a fundamental operation for equivariant networks, increase significantly in computational complexity as higher-order tensors are used. In this paper, we address this issue by reducing the \${SO}(3)\$ convolutions or tensor products to mathematically equivalent convolutions in \${SO}(2)\$ . This is accomplished by aligning the node embeddings' primary axis with the edge vectors, which sparsifies the tensor product and reduces the computational complexity from \$O(L{\textasciicircum}6)\$ to \$O(L{\textasciicircum}3)\$, where \$L\$ is the degree of the representation. We demonstrate the potential implications of this improvement by proposing the Equivariant Spherical Channel Network ({eSCN}), a graph neural network utilizing our novel approach to equivariant convolutions, which achieves state-of-the-art results on the large-scale {OC}-20 dataset.},
	number = {{arXiv}:2302.03655},
	publisher = {{arXiv}},
	author = {Passaro, Saro and Zitnick, C. Lawrence},
	urldate = {2023-03-05},
	date = {2023-02-07},
	eprinttype = {arxiv},
	eprint = {2302.03655 [physics]},
	keywords = {Computer Science - Machine Learning, I.2.6, Physics - Computational Physics, Physics - Chemical Physics, 20C35 (Primary), J.2},
}

@article{huntenburg_gradients_2021,
	title = {Gradients of functional connectivity in the mouse cortex reflect neocortical evolution},
	volume = {225},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811920310132},
	doi = {10.1016/j.neuroimage.2020.117528},
	abstract = {Understanding cortical organization is a fundamental goal of neuroscience that requires comparisons across species and modalities. Large-scale connectivity gradients have recently been introduced as a data-driven representation of the intrinsic organization of the cortex. We studied resting-state functional connectivity gradients in the mouse cortex and found robust spatial patterns across four data sets. The principal gradient of functional connectivity shows a striking overlap with an axis of neocortical evolution from two primordial origins. Additional gradients reflect sensory specialization and aspects of a sensory-to-transmodal hierarchy, and are associated with transcriptomic features. While some of these gradients strongly resemble observations in the human cortex, the overall pattern in the mouse cortex emphasizes the specialization of sensory areas over a global functional hierarchy.},
	pages = {117528},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Huntenburg, Julia M. and Yeow, Ling Yun and Mandino, Francesca and Grandjean, Joanes},
	urldate = {2023-03-05},
	date = {2021-01-15},
	langid = {english},
	keywords = {Cortical gradient, Dual origin, Intrinsic functional organization, Mouse cortex, Resting-state functional connectivity},
}

@misc{villar_passive_2023,
	title = {The passive symmetries of machine learning},
	url = {http://arxiv.org/abs/2301.13724},
	doi = {10.48550/arXiv.2301.13724},
	abstract = {Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry and units covariance, all of which have led to important results in physics. Our goal is to understand the implications of passive symmetries for machine learning: Which passive symmetries play a role (e.g., permutation symmetry in graph neural networks)? What are dos and don'ts in machine learning practice? We assay conditions under which passive symmetries can be implemented as group equivariances. We also discuss links to causal modeling, and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. While this paper is purely conceptual, we believe that it can have a significant impact on helping machine learning make the transition that took place for modern physics in the first half of the Twentieth century.},
	number = {{arXiv}:2301.13724},
	publisher = {{arXiv}},
	author = {Villar, Soledad and Hogg, David W. and Yao, Weichi and Kevrekidis, George A. and Schölkopf, Bernhard},
	urldate = {2023-03-05},
	date = {2023-01-31},
	eprinttype = {arxiv},
	eprint = {2301.13724 [astro-ph, physics:math-ph, physics:physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Data Analysis, Statistics and Probability, Astrophysics - Instrumentation and Methods for Astrophysics, Mathematical Physics},
}

@misc{khaled_better_2020,
	title = {Better Theory for {SGD} in the Nonconvex World},
	url = {http://arxiv.org/abs/2002.03329},
	doi = {10.48550/arXiv.2002.03329},
	abstract = {Large-scale nonconvex optimization problems are ubiquitous in modern machine learning, and among practitioners interested in solving them, Stochastic Gradient Descent ({SGD}) reigns supreme. We revisit the analysis of {SGD} in the nonconvex setting and propose a new variant of the recently introduced expected smoothness assumption which governs the behaviour of the second moment of the stochastic gradient. We show that our assumption is both more general and more reasonable than assumptions made in all prior work. Moreover, our results yield the optimal \${\textbackslash}mathcal\{O\}({\textbackslash}varepsilon{\textasciicircum}\{-4\})\$ rate for finding a stationary point of nonconvex smooth functions, and recover the optimal \${\textbackslash}mathcal\{O\}({\textbackslash}varepsilon{\textasciicircum}\{-1\})\$ rate for finding a global solution if the Polyak-\{{\textbackslash}L\}ojasiewicz condition is satisfied. We compare against convergence rates under convexity and prove a theorem on the convergence of {SGD} under Quadratic Functional Growth and convexity, which might be of independent interest. Moreover, we perform our analysis in a framework which allows for a detailed study of the effects of a wide array of sampling strategies and minibatch sizes for finite-sum optimization problems. We corroborate our theoretical results with experiments on real and synthetic data.},
	number = {{arXiv}:2002.03329},
	publisher = {{arXiv}},
	author = {Khaled, Ahmed and Richtárik, Peter},
	urldate = {2023-03-05},
	date = {2020-07-24},
	eprinttype = {arxiv},
	eprint = {2002.03329 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}

@misc{li_internet_2023,
	title = {Internet Explorer: Targeted Representation Learning on the Open Web},
	url = {http://arxiv.org/abs/2302.14051},
	doi = {10.48550/arXiv.2302.14051},
	shorttitle = {Internet Explorer},
	abstract = {Modern vision models typically rely on fine-tuning general-purpose models pre-trained on large, static datasets. These general-purpose models only capture the knowledge within their pre-training datasets, which are tiny, out-of-date snapshots of the Internet -- where billions of images are uploaded each day. We suggest an alternate approach: rather than hoping our static datasets transfer to our desired tasks after large-scale pre-training, we propose dynamically utilizing the Internet to quickly train a small-scale model that does extremely well on the task at hand. Our approach, called Internet Explorer, explores the web in a self-supervised manner to progressively find relevant examples that improve performance on a desired target dataset. It cycles between searching for images on the Internet with text queries, self-supervised training on downloaded images, determining which images were useful, and prioritizing what to search for next. We evaluate Internet Explorer across several datasets and show that it outperforms or matches {CLIP} oracle performance by using just a single {GPU} desktop to actively query the Internet for 30--40 hours. Results, visualizations, and videos at https://internet-explorer-ssl.github.io/},
	number = {{arXiv}:2302.14051},
	publisher = {{arXiv}},
	author = {Li, Alexander C. and Brown, Ellis and Efros, Alexei A. and Pathak, Deepak},
	urldate = {2023-03-05},
	date = {2023-02-27},
	eprinttype = {arxiv},
	eprint = {2302.14051 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
}

@misc{boutin_diffusion_2023,
	title = {Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?},
	url = {http://arxiv.org/abs/2301.11722},
	doi = {10.48550/arXiv.2301.11722},
	shorttitle = {Diffusion Models as Artists},
	abstract = {An important milestone for {AI} is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the 'diversity vs. recognizability' scoring framework from Boutin et al, 2022 and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines remains -- in part explainable by discrepancies in visual strategies.},
	number = {{arXiv}:2301.11722},
	publisher = {{arXiv}},
	author = {Boutin, Victor and Fel, Thomas and Singhal, Lakshya and Mukherji, Rishav and Nagaraj, Akash and Colin, Julien and Serre, Thomas},
	urldate = {2023-03-05},
	date = {2023-01-27},
	eprinttype = {arxiv},
	eprint = {2301.11722 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@article{shainer_single-cell_2023,
	title = {A single-cell resolution gene expression atlas of the larval zebrafish brain},
	volume = {9},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.ade9909},
	doi = {10.1126/sciadv.ade9909},
	abstract = {The advent of multimodal brain atlases promises to accelerate progress in neuroscience by allowing in silico queries of neuron morphology, connectivity, and gene expression. We used multiplexed fluorescent in situ {RNA} hybridization chain reaction ({HCR}) technology to generate expression maps across the larval zebrafish brain for a growing set of marker genes. The data were registered to the Max Planck Zebrafish Brain (mapzebrain) atlas, thus allowing covisualization of gene expression, single-neuron tracings, and expertly curated anatomical segmentations. Using post hoc {HCR} labeling of the immediate early gene 
              cfos 
              , we mapped responses to prey stimuli and food ingestion across the brain of freely swimming larvae. This unbiased approach revealed, in addition to previously described visual and motor areas, a cluster of neurons in the secondary gustatory nucleus, which express the marker 
              calb2a, 
              as well as a specific neuropeptide Y receptor, and project to the hypothalamus. This discovery exemplifies the power of this new atlas resource for zebrafish neurobiology. 
             
          ,  
            A gene expression atlas links development, structure, and function in the larval zebrafish brain.},
	pages = {eade9909},
	number = {8},
	journaltitle = {Science Advances},
	shortjournal = {Sci. Adv.},
	author = {Shainer, Inbal and Kuehn, Enrico and Laurell, Eva and Al Kassar, Mariam and Mokayes, Nouwar and Sherman, Shachar and Larsch, Johannes and Kunst, Michael and Baier, Herwig},
	urldate = {2023-03-05},
	date = {2023-02-24},
	langid = {english},
}

@misc{zhang_large-scale_2023,
	title = {Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing},
	url = {http://arxiv.org/abs/2303.00915},
	doi = {10.48550/arXiv.2303.00915},
	abstract = {Contrastive pretraining on parallel image-text data has attained great success in vision-language processing ({VLP}), as exemplified by {CLIP} and related methods. However, prior explorations tend to focus on general domains in the web. Biomedical images and text are rather different, but publicly available datasets are small and skew toward chest X-ray, thus severely limiting progress. In this paper, we conducted by far the largest study on biomedical {VLP}, using 15 million figure-caption pairs extracted from biomedical research articles in {PubMed} Central. Our dataset ({PMC}-15M) is two orders of magnitude larger than existing biomedical image-text datasets such as {MIMIC}-{CXR}, and spans a diverse range of biomedical images. The standard {CLIP} method is suboptimal for the biomedical domain. We propose {BiomedCLIP} with domain-specific adaptations tailored to biomedical {VLP}. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering ({VQA}). {BiomedCLIP} established new state of the art in a wide range of standard datasets, substantially outperformed prior {VLP} approaches. Surprisingly, {BiomedCLIP} even outperformed radiology-specific state-of-the-art models such as {BioViL} on radiology-specific tasks such as {RSNA} pneumonia detection, thus highlighting the utility in large-scale pretraining across all biomedical image types. We will release our models at https://aka.ms/biomedclip to facilitate future research in biomedical {VLP}.},
	number = {{arXiv}:2303.00915},
	publisher = {{arXiv}},
	author = {Zhang, Sheng and Xu, Yanbo and Usuyama, Naoto and Bagga, Jaspreet and Tinn, Robert and Preston, Sam and Rao, Rajesh and Wei, Mu and Valluri, Naveen and Wong, Cliff and Lungren, Matthew P. and Naumann, Tristan and Poon, Hoifung},
	urldate = {2023-03-05},
	date = {2023-03-01},
	eprinttype = {arxiv},
	eprint = {2303.00915 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
}

@misc{liu_dropout_2023,
	title = {Dropout Reduces Underfitting},
	url = {http://arxiv.org/abs/2303.01500},
	doi = {10.48550/arXiv.2303.01500},
	abstract = {Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of {SGD} and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations and is only activated later in training. Experiments on {ImageNet} and various vision tasks demonstrate that our methods consistently improve generalization accuracy. Our results encourage more research on understanding regularization in deep learning and our methods can be useful tools for future neural network training, especially in the era of large data. Code is available at https://github.com/facebookresearch/dropout .},
	number = {{arXiv}:2303.01500},
	publisher = {{arXiv}},
	author = {Liu, Zhuang and Xu, Zhiqiu and Jin, Joseph and Shen, Zhiqiang and Darrell, Trevor},
	urldate = {2023-03-05},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2303.01500 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
}

@misc{song_consistency_2023,
	title = {Consistency Models},
	url = {http://arxiv.org/abs/2303.01469},
	doi = {10.48550/arXiv.2303.01469},
	abstract = {Diffusion models have made significant breakthroughs in image, audio, and video generation, but they depend on an iterative generation process that causes slow sampling speed and caps their potential for real-time applications. To overcome this limitation, we propose consistency models, a new family of generative models that achieve high sample quality without adversarial training. They support fast one-step generation by design, while still allowing for few-step sampling to trade compute for sample quality. They also support zero-shot data editing, like image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either as a way to distill pre-trained diffusion models, or as standalone generative models. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step generation. For example, we achieve the new state-of-the-art {FID} of 3.55 on {CIFAR}-10 and 6.20 on {ImageNet} 64x64 for one-step generation. When trained as standalone generative models, consistency models also outperform single-step, non-adversarial generative models on standard benchmarks like {CIFAR}-10, {ImageNet} 64x64 and {LSUN} 256x256.},
	number = {{arXiv}:2303.01469},
	publisher = {{arXiv}},
	author = {Song, Yang and Dhariwal, Prafulla and Chen, Mark and Sutskever, Ilya},
	urldate = {2023-03-05},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2303.01469 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{bramburger_auxiliary_2023,
	title = {Auxiliary Functions as Koopman Observables: Data-Driven Polynomial Optimization for Dynamical Systems},
	url = {http://arxiv.org/abs/2303.01483},
	doi = {10.48550/arXiv.2303.01483},
	shorttitle = {Auxiliary Functions as Koopman Observables},
	abstract = {We present a flexible data-driven method for dynamical system analysis that does not require explicit model discovery. The method is rooted in well-established techniques for approximating the Koopman operator from data and is implemented as a semidefinite program that can be solved numerically. The method is agnostic of whether data is generated through a deterministic or stochastic process, so its implementation requires no prior adjustments by the user to accommodate these different scenarios. Rigorous convergence results justify the applicability of the method, while also extending and uniting similar results from across the literature. Examples on discovering Lyapunov functions and on performing ergodic optimization for both deterministic and stochastic dynamics exemplify these convergence results and demonstrate the performance of the method.},
	number = {{arXiv}:2303.01483},
	publisher = {{arXiv}},
	author = {Bramburger, Jason J. and Fantuzzi, Giovanni},
	urldate = {2023-03-05},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2303.01483 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Dynamical Systems},
}

@misc{poli_hyena_2023,
	title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
	url = {http://arxiv.org/abs/2302.10866},
	doi = {10.48550/arXiv.2302.10866},
	shorttitle = {Hyena Hierarchy},
	abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets ({WikiText}103 and The Pile), reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
	number = {{arXiv}:2302.10866},
	publisher = {{arXiv}},
	author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
	urldate = {2023-03-05},
	date = {2023-02-21},
	eprinttype = {arxiv},
	eprint = {2302.10866 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{zou_generalized_2022,
	title = {Generalized Decoding for Pixel, Image, and Language},
	url = {http://arxiv.org/abs/2212.11270},
	doi = {10.48550/arXiv.2212.11270},
	abstract = {We present X-Decoder, a generalized decoding model that can predict pixel-level segmentation and language tokens seamlessly. X-Decodert takes as input two types of queries: (i) generic non-semantic queries and (ii) semantic queries induced from text inputs, to decode different pixel-level and token-level outputs in the same semantic space. With such a novel design, X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language ({VL}) tasks. Further, our design enables seamless interactions across tasks at different granularities and brings mutual benefits by learning a common and rich pixel-level visual-semantic understanding space, without any pseudo-labeling. After pretraining on a mixed set of a limited amount of segmentation data and millions of image-text pairs, X-Decoder exhibits strong transferability to a wide range of downstream tasks in both zero-shot and finetuning settings. Notably, it achieves (1) state-of-the-art results on open-vocabulary segmentation and referring segmentation on eight datasets; (2) better or competitive finetuned performance to other generalist and specialist models on segmentation and {VL} tasks; and (3) flexibility for efficient finetuning and novel task composition (e.g., referring captioning and image editing). Code, demo, video, and visualization are available at https://x-decoder-vl.github.io.},
	number = {{arXiv}:2212.11270},
	publisher = {{arXiv}},
	author = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
	urldate = {2023-03-05},
	date = {2022-12-21},
	eprinttype = {arxiv},
	eprint = {2212.11270 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
}

@misc{yao_high-resolution_2023,
	title = {A high-resolution transcriptomic and spatial atlas of cell types in the whole mouse brain},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.06.531121v1},
	doi = {10.1101/2023.03.06.531121},
	abstract = {The mammalian brain is composed of millions to billions of cells that are organized into numerous cell types with specific spatial distribution patterns and structural and functional properties. An essential step towards understanding brain function is to obtain a parts list, i.e., a catalog of cell types, of the brain. Here, we report a comprehensive and high-resolution transcriptomic and spatial cell type atlas for the whole adult mouse brain. The cell type atlas was created based on the combination of two single-cell-level, whole-brain-scale datasets: a single- cell {RNA}-sequencing ({scRNA}-seq) dataset of ∼7 million cells profiled, and a spatially resolved transcriptomic dataset of ∼4.3 million cells using {MERFISH}. The atlas is hierarchically organized into five nested levels of classification: 7 divisions, 32 classes, 306 subclasses, 1,045 supertypes and 5,200 clusters. We systematically analyzed the neuronal, non-neuronal, and immature neuronal cell types across the brain and identified a high degree of correspondence between transcriptomic identity and spatial specificity for each cell type. The results reveal unique features of cell type organization in different brain regions, in particular, a dichotomy between the dorsal and ventral parts of the brain: the dorsal part contains relatively fewer yet highly divergent neuronal types, whereas the ventral part contains more numerous neuronal types that are more closely related to each other. We also systematically characterized cell-type specific expression of neurotransmitters, neuropeptides, and transcription factors. The study uncovered extraordinary diversity and heterogeneity in neurotransmitter and neuropeptide expression and co-expression patterns in different cell types across the brain, suggesting they mediate a myriad of modes of intercellular communications. Finally, we found that transcription factors are major determinants of cell type classification in the adult mouse brain and identified a combinatorial transcription factor code that defines cell types across all parts of the brain. The whole-mouse-brain transcriptomic and spatial cell type atlas establishes a benchmark reference atlas and a foundational resource for deep and integrative investigations of cell type and circuit function, development, and evolution of the mammalian brain.},
	publisher = {{bioRxiv}},
	author = {Yao, Zizhen and Velthoven, Cindy T. J. van and Kunst, Michael and Zhang, Meng and {McMillen}, Delissa and Lee, Changkyu and Jung, Won and Goldy, Jeff and Abdelhak, Aliya and Baker, Pamela and Barkan, Eliza and Bertagnolli, Darren and Campos, Jazmin and Carey, Daniel and Casper, Tamara and Chakka, Anish Bhaswanth and Chakrabarty, Rushil and Chavan, Sakshi and Chen, Min and Clark, Michael and Close, Jennie and Crichton, Kirsten and Daniel, Scott and Dolbeare, Tim and Ellingwood, Lauren and Gee, James and Glandon, Alexandra and Gloe, Jessica and Gould, Joshua and Gray, James and Guilford, Nathan and Guzman, Junitta and Hirschstein, Daniel and Ho, Windy and Jin, Kelly and Kroll, Matthew and Lathia, Kanan and Leon, Arielle and Long, Brian and Maltzer, Zoe and Martin, Naomi and {McCue}, Rachel and Meyerdierks, Emma and Nguyen, Thuc Nghi and Pham, Trangthanh and Rimorin, Christine and Ruiz, Augustin and Shapovalova, Nadiya and Slaughterbeck, Cliff and Sulc, Josef and Tieu, Michael and Torkelson, Amy and Tung, Herman and Cuevas, Nasmil Valera and Wadhwani, Katherine and Ward, Katelyn and Levi, Boaz and Farrell, Colin and Thompson, Carol L. and Mufti, Shoaib and Pagan, Chelsea M. and Kruse, Lauren and Dee, Nick and Sunkin, Susan M. and Esposito, Luke and Hawrylycz, Michael J. and Waters, Jack and Ng, Lydia and Smith, Kimberly A. and Tasic, Bosiljka and Zhuang, Xiaowei and Zeng, Hongkui},
	urldate = {2023-03-11},
	date = {2023-03-06},
	langid = {english},
	note = {Pages: 2023.03.06.531121
Section: New Results},
	keywords = {notion},
	file = {Full Text PDF:/home/ajl/Zotero/storage/JAIIAYQT/Yao et al. - 2023 - A high-resolution transcriptomic and spatial atlas.pdf:application/pdf},
}

@misc{langlieb_cell_2023,
	title = {The cell type composition of the adult mouse brain revealed by single cell and spatial genomics},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.06.531307v1},
	doi = {10.1101/2023.03.06.531307},
	abstract = {The function of the mammalian brain relies upon the specification and spatial positioning of diversely specialized cell types. Yet, the molecular identities of the cell types, and their positions within individual anatomical structures, remain incompletely known. To construct a comprehensive atlas of cell types in each brain structure, we paired high-throughput single-nucleus {RNA}-seq with Slide-seq, a recently developed spatial transcriptomics method with near-cellular resolution, across the entire mouse brain. Integration of these datasets revealed the cell type composition of each neuroanatomical structure. Cell type diversity was found to be remarkably high in the midbrain, hindbrain, and hypothalamus, with most clusters requiring a combination of at least three discrete gene expression markers to uniquely define them. Using these data, we developed a framework for genetically accessing each cell type, comprehensively characterized neuropeptide and neurotransmitter signaling, elucidated region-specific specializations in activity-regulated gene expression, and ascertained the heritability enrichment of neurological and psychiatric phenotypes. These data, available as an online resource ({BrainCellData}.org) should find diverse applications across neuroscience, including the construction of new genetic tools, and the prioritization of specific cell types and circuits in the study of brain diseases.},
	publisher = {{bioRxiv}},
	author = {Langlieb, Jonah and Sachdev, Nina and Balderrama, Karol and Nadaf, Naeem and Raj, Mukund and Murray, Evan and Webber, James and Vanderburg, Charles and Gazestani, Vahid and Tward, Daniel and Mezias, Chris and Li, Xu and Norton, Tabitha and Mitra, Partha Pratim and Chen, Fei and Macosko, Evan},
	urldate = {2023-03-11},
	date = {2023-03-08},
	langid = {english},
	note = {Pages: 2023.03.06.531307
Section: New Results},
	file = {Full Text PDF:/home/ajl/Zotero/storage/V77HTJTJ/Langlieb et al. - 2023 - The cell type composition of the adult mouse brain.pdf:application/pdf},
}

@article{chen_spatially_2015,
	title = {Spatially resolved, highly multiplexed {RNA} profiling in single cells},
	volume = {348},
	url = {https://www.science.org/doi/10.1126/science.aaa6090},
	doi = {10.1126/science.aaa6090},
	abstract = {Knowledge of the expression profile and spatial landscape of the transcriptome in individual cells is essential for understanding the rich repertoire of cellular behaviors. Here, we report multiplexed error-robust fluorescence in situ hybridization ({MERFISH}), a single-molecule imaging approach that allows the copy numbers and spatial localizations of thousands of {RNA} species to be determined in single cells. Using error-robust encoding schemes to combat single-molecule labeling and detection errors, we demonstrated the imaging of 100 to 1000 distinct {RNA} species in hundreds of individual cells. Correlation analysis of the {\textasciitilde}104 to 106 pairs of genes allowed us to constrain gene regulatory networks, predict novel functions for many unannotated genes, and identify distinct spatial distribution patterns of {RNAs} that correlate with properties of the encoded proteins.},
	pages = {aaa6090},
	number = {6233},
	journaltitle = {Science},
	author = {Chen, Kok Hao and Boettiger, Alistair N. and Moffitt, Jeffrey R. and Wang, Siyuan and Zhuang, Xiaowei},
	urldate = {2023-03-11},
	date = {2015-04-24},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Full Text PDF:/home/ajl/Zotero/storage/WMHL2JAR/Chen et al. - 2015 - Spatially resolved, highly multiplexed RNA profili.pdf:application/pdf},
}

@article{rodriques_slide-seq_2019,
	title = {Slide-seq: A scalable technology for measuring genome-wide expression at high spatial resolution},
	volume = {363},
	url = {https://www.science.org/doi/10.1126/science.aaw1219},
	doi = {10.1126/science.aaw1219},
	shorttitle = {Slide-seq},
	abstract = {Spatial positions of cells in tissues strongly influence function, yet a high-throughput, genome-wide readout of gene expression with cellular resolution is lacking. We developed Slide-seq, a method for transferring {RNA} from tissue sections onto a surface covered in {DNA}-barcoded beads with known positions, allowing the locations of the {RNA} to be inferred by sequencing. Using Slide-seq, we localized cell types identified by single-cell {RNA} sequencing datasets within the cerebellum and hippocampus, characterized spatial gene expression patterns in the Purkinje layer of mouse cerebellum, and defined the temporal evolution of cell type–specific responses in a mouse model of traumatic brain injury. These studies highlight how Slide-seq provides a scalable method for obtaining spatially resolved gene expression data at resolutions comparable to the sizes of individual cells.},
	pages = {1463--1467},
	number = {6434},
	journaltitle = {Science},
	author = {Rodriques, Samuel G. and Stickels, Robert R. and Goeva, Aleksandrina and Martin, Carly A. and Murray, Evan and Vanderburg, Charles R. and Welch, Joshua and Chen, Linlin M. and Chen, Fei and Macosko, Evan Z.},
	urldate = {2023-03-11},
	date = {2019-03-29},
	note = {Publisher: American Association for the Advancement of Science},
	file = {Accepted Version:/home/ajl/Zotero/storage/29EXEP2W/Rodriques et al. - 2019 - Slide-seq A scalable technology for measuring gen.pdf:application/pdf},
}
